I0402 02:05:47.330528 16384 caffe.cpp:217] Using GPUs 0
I0402 02:05:47.408730 16384 caffe.cpp:222] GPU 0: GeForce GTX 1070
I0402 02:05:48.330663 16384 solver.cpp:60] Initializing solver from parameters: 
train_net: "./Prototxt/rtsd-r1/CoNorm/train.prototxt"
test_net: "./Prototxt/rtsd-r1/CoNorm/test.prototxt"
test_iter: 15
test_interval: 500
base_lr: 0.0001
display: 1
max_iter: 5000
lr_policy: "step"
gamma: 0.5
momentum: 0.9
weight_decay: 0.0005
stepsize: 1000
snapshot: 500
snapshot_prefix: "./snapshots/experiment_1/rtsd-r1/CoNorm/snap"
solver_mode: GPU
device_id: 0
train_state {
  level: 0
  stage: ""
}
iter_size: 1
type: "Adam"
I0402 02:05:48.330819 16384 solver.cpp:93] Creating training net from train_net file: ./Prototxt/rtsd-r1/CoNorm/train.prototxt
I0402 02:05:48.331069 16384 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_1
I0402 02:05:48.331080 16384 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_5
I0402 02:05:48.331178 16384 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: false
    crop_size: 48
    mean_value: 132
    mean_value: 132
    mean_value: 131
  }
  data_param {
    source: "../local_data/lmdb/rtsd-r1/CoNorm/train/lmdb"
    batch_size: 512
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 7
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_relu"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 150
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_relu"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 250
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_relu"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc4_300"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc4_300"
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fc4_relu"
  type: "ReLU"
  bottom: "fc4_300"
  top: "fc4_300"
}
layer {
  name: "drop4"
  type: "Dropout"
  bottom: "fc4_300"
  top: "fc4_300"
  dropout_param {
    dropout_ratio: 0.4
  }
}
layer {
  name: "fc5_67"
  type: "InnerProduct"
  bottom: "fc4_300"
  top: "fc5_classes"
  inner_product_param {
    num_output: 67
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc5_classes"
  bottom: "label"
  top: "loss"
}
I0402 02:05:48.331250 16384 layer_factory.hpp:77] Creating layer data
I0402 02:05:48.332011 16384 net.cpp:100] Creating Layer data
I0402 02:05:48.332027 16384 net.cpp:408] data -> data
I0402 02:05:48.332049 16384 net.cpp:408] data -> label
I0402 02:05:48.333312 16441 db_lmdb.cpp:35] Opened lmdb ../local_data/lmdb/rtsd-r1/CoNorm/train/lmdb
I0402 02:05:48.427832 16384 data_layer.cpp:41] output data size: 512,3,48,48
I0402 02:05:48.479252 16384 net.cpp:150] Setting up data
I0402 02:05:48.479296 16384 net.cpp:157] Top shape: 512 3 48 48 (3538944)
I0402 02:05:48.479301 16384 net.cpp:157] Top shape: 512 (512)
I0402 02:05:48.479305 16384 net.cpp:165] Memory required for data: 14157824
I0402 02:05:48.479346 16384 layer_factory.hpp:77] Creating layer conv1
I0402 02:05:48.479377 16384 net.cpp:100] Creating Layer conv1
I0402 02:05:48.479385 16384 net.cpp:434] conv1 <- data
I0402 02:05:48.479399 16384 net.cpp:408] conv1 -> conv1
I0402 02:05:48.924175 16384 net.cpp:150] Setting up conv1
I0402 02:05:48.924206 16384 net.cpp:157] Top shape: 512 100 42 42 (90316800)
I0402 02:05:48.924209 16384 net.cpp:165] Memory required for data: 375425024
I0402 02:05:48.924230 16384 layer_factory.hpp:77] Creating layer conv1_relu
I0402 02:05:48.924257 16384 net.cpp:100] Creating Layer conv1_relu
I0402 02:05:48.924263 16384 net.cpp:434] conv1_relu <- conv1
I0402 02:05:48.924268 16384 net.cpp:395] conv1_relu -> conv1 (in-place)
I0402 02:05:48.924464 16384 net.cpp:150] Setting up conv1_relu
I0402 02:05:48.924479 16384 net.cpp:157] Top shape: 512 100 42 42 (90316800)
I0402 02:05:48.924481 16384 net.cpp:165] Memory required for data: 736692224
I0402 02:05:48.924484 16384 layer_factory.hpp:77] Creating layer pool1
I0402 02:05:48.924492 16384 net.cpp:100] Creating Layer pool1
I0402 02:05:48.924495 16384 net.cpp:434] pool1 <- conv1
I0402 02:05:48.924500 16384 net.cpp:408] pool1 -> pool1
I0402 02:05:48.924553 16384 net.cpp:150] Setting up pool1
I0402 02:05:48.924562 16384 net.cpp:157] Top shape: 512 100 21 21 (22579200)
I0402 02:05:48.924566 16384 net.cpp:165] Memory required for data: 827009024
I0402 02:05:48.924568 16384 layer_factory.hpp:77] Creating layer conv2
I0402 02:05:48.924579 16384 net.cpp:100] Creating Layer conv2
I0402 02:05:48.924583 16384 net.cpp:434] conv2 <- pool1
I0402 02:05:48.924588 16384 net.cpp:408] conv2 -> conv2
I0402 02:05:48.931659 16384 net.cpp:150] Setting up conv2
I0402 02:05:48.931677 16384 net.cpp:157] Top shape: 512 150 18 18 (24883200)
I0402 02:05:48.931680 16384 net.cpp:165] Memory required for data: 926541824
I0402 02:05:48.931690 16384 layer_factory.hpp:77] Creating layer conv2_relu
I0402 02:05:48.931696 16384 net.cpp:100] Creating Layer conv2_relu
I0402 02:05:48.931699 16384 net.cpp:434] conv2_relu <- conv2
I0402 02:05:48.931704 16384 net.cpp:395] conv2_relu -> conv2 (in-place)
I0402 02:05:48.933260 16384 net.cpp:150] Setting up conv2_relu
I0402 02:05:48.933276 16384 net.cpp:157] Top shape: 512 150 18 18 (24883200)
I0402 02:05:48.933279 16384 net.cpp:165] Memory required for data: 1026074624
I0402 02:05:48.933284 16384 layer_factory.hpp:77] Creating layer pool2
I0402 02:05:48.933290 16384 net.cpp:100] Creating Layer pool2
I0402 02:05:48.933292 16384 net.cpp:434] pool2 <- conv2
I0402 02:05:48.933297 16384 net.cpp:408] pool2 -> pool2
I0402 02:05:48.933339 16384 net.cpp:150] Setting up pool2
I0402 02:05:48.933348 16384 net.cpp:157] Top shape: 512 150 9 9 (6220800)
I0402 02:05:48.933362 16384 net.cpp:165] Memory required for data: 1050957824
I0402 02:05:48.933365 16384 layer_factory.hpp:77] Creating layer conv3
I0402 02:05:48.933373 16384 net.cpp:100] Creating Layer conv3
I0402 02:05:48.933377 16384 net.cpp:434] conv3 <- pool2
I0402 02:05:48.933382 16384 net.cpp:408] conv3 -> conv3
I0402 02:05:48.942178 16384 net.cpp:150] Setting up conv3
I0402 02:05:48.942194 16384 net.cpp:157] Top shape: 512 250 6 6 (4608000)
I0402 02:05:48.942209 16384 net.cpp:165] Memory required for data: 1069389824
I0402 02:05:48.942219 16384 layer_factory.hpp:77] Creating layer conv3_relu
I0402 02:05:48.942227 16384 net.cpp:100] Creating Layer conv3_relu
I0402 02:05:48.942230 16384 net.cpp:434] conv3_relu <- conv3
I0402 02:05:48.942235 16384 net.cpp:395] conv3_relu -> conv3 (in-place)
I0402 02:05:48.946843 16384 net.cpp:150] Setting up conv3_relu
I0402 02:05:48.946862 16384 net.cpp:157] Top shape: 512 250 6 6 (4608000)
I0402 02:05:48.946877 16384 net.cpp:165] Memory required for data: 1087821824
I0402 02:05:48.946882 16384 layer_factory.hpp:77] Creating layer pool3
I0402 02:05:48.946888 16384 net.cpp:100] Creating Layer pool3
I0402 02:05:48.946892 16384 net.cpp:434] pool3 <- conv3
I0402 02:05:48.946898 16384 net.cpp:408] pool3 -> pool3
I0402 02:05:48.946945 16384 net.cpp:150] Setting up pool3
I0402 02:05:48.946974 16384 net.cpp:157] Top shape: 512 250 3 3 (1152000)
I0402 02:05:48.946979 16384 net.cpp:165] Memory required for data: 1092429824
I0402 02:05:48.946982 16384 layer_factory.hpp:77] Creating layer fc4_300
I0402 02:05:48.946988 16384 net.cpp:100] Creating Layer fc4_300
I0402 02:05:48.946992 16384 net.cpp:434] fc4_300 <- pool3
I0402 02:05:48.946997 16384 net.cpp:408] fc4_300 -> fc4_300
I0402 02:05:48.952405 16384 net.cpp:150] Setting up fc4_300
I0402 02:05:48.952424 16384 net.cpp:157] Top shape: 512 300 (153600)
I0402 02:05:48.952426 16384 net.cpp:165] Memory required for data: 1093044224
I0402 02:05:48.952433 16384 layer_factory.hpp:77] Creating layer fc4_relu
I0402 02:05:48.952440 16384 net.cpp:100] Creating Layer fc4_relu
I0402 02:05:48.952442 16384 net.cpp:434] fc4_relu <- fc4_300
I0402 02:05:48.952447 16384 net.cpp:395] fc4_relu -> fc4_300 (in-place)
I0402 02:05:48.952657 16384 net.cpp:150] Setting up fc4_relu
I0402 02:05:48.952672 16384 net.cpp:157] Top shape: 512 300 (153600)
I0402 02:05:48.952675 16384 net.cpp:165] Memory required for data: 1093658624
I0402 02:05:48.952678 16384 layer_factory.hpp:77] Creating layer drop4
I0402 02:05:48.952685 16384 net.cpp:100] Creating Layer drop4
I0402 02:05:48.952688 16384 net.cpp:434] drop4 <- fc4_300
I0402 02:05:48.952693 16384 net.cpp:395] drop4 -> fc4_300 (in-place)
I0402 02:05:48.952726 16384 net.cpp:150] Setting up drop4
I0402 02:05:48.952733 16384 net.cpp:157] Top shape: 512 300 (153600)
I0402 02:05:48.952736 16384 net.cpp:165] Memory required for data: 1094273024
I0402 02:05:48.952739 16384 layer_factory.hpp:77] Creating layer fc5_67
I0402 02:05:48.952745 16384 net.cpp:100] Creating Layer fc5_67
I0402 02:05:48.952749 16384 net.cpp:434] fc5_67 <- fc4_300
I0402 02:05:48.952754 16384 net.cpp:408] fc5_67 -> fc5_classes
I0402 02:05:48.953994 16384 net.cpp:150] Setting up fc5_67
I0402 02:05:48.954010 16384 net.cpp:157] Top shape: 512 67 (34304)
I0402 02:05:48.954013 16384 net.cpp:165] Memory required for data: 1094410240
I0402 02:05:48.954023 16384 layer_factory.hpp:77] Creating layer loss
I0402 02:05:48.954030 16384 net.cpp:100] Creating Layer loss
I0402 02:05:48.954032 16384 net.cpp:434] loss <- fc5_classes
I0402 02:05:48.954037 16384 net.cpp:434] loss <- label
I0402 02:05:48.954042 16384 net.cpp:408] loss -> loss
I0402 02:05:48.954054 16384 layer_factory.hpp:77] Creating layer loss
I0402 02:05:48.954381 16384 net.cpp:150] Setting up loss
I0402 02:05:48.954394 16384 net.cpp:157] Top shape: (1)
I0402 02:05:48.954397 16384 net.cpp:160]     with loss weight 1
I0402 02:05:48.954412 16384 net.cpp:165] Memory required for data: 1094410244
I0402 02:05:48.954416 16384 net.cpp:226] loss needs backward computation.
I0402 02:05:48.954424 16384 net.cpp:226] fc5_67 needs backward computation.
I0402 02:05:48.954427 16384 net.cpp:226] drop4 needs backward computation.
I0402 02:05:48.954430 16384 net.cpp:226] fc4_relu needs backward computation.
I0402 02:05:48.954433 16384 net.cpp:226] fc4_300 needs backward computation.
I0402 02:05:48.954437 16384 net.cpp:226] pool3 needs backward computation.
I0402 02:05:48.954438 16384 net.cpp:226] conv3_relu needs backward computation.
I0402 02:05:48.954442 16384 net.cpp:226] conv3 needs backward computation.
I0402 02:05:48.954444 16384 net.cpp:226] pool2 needs backward computation.
I0402 02:05:48.954447 16384 net.cpp:226] conv2_relu needs backward computation.
I0402 02:05:48.954450 16384 net.cpp:226] conv2 needs backward computation.
I0402 02:05:48.954453 16384 net.cpp:226] pool1 needs backward computation.
I0402 02:05:48.954457 16384 net.cpp:226] conv1_relu needs backward computation.
I0402 02:05:48.954459 16384 net.cpp:226] conv1 needs backward computation.
I0402 02:05:48.954463 16384 net.cpp:228] data does not need backward computation.
I0402 02:05:48.954465 16384 net.cpp:270] This network produces output loss
I0402 02:05:48.954478 16384 net.cpp:283] Network initialization done.
I0402 02:05:48.954677 16384 solver.cpp:193] Creating test net (#0) specified by test_net file: ./Prototxt/rtsd-r1/CoNorm/test.prototxt
I0402 02:05:48.954819 16384 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 48
    mean_value: 133
    mean_value: 133
    mean_value: 132
  }
  data_param {
    source: "../local_data/lmdb/rtsd-r1/CoNorm/test/lmdb"
    batch_size: 512
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 7
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_relu"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 150
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_relu"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 250
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_relu"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc4_300"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc4_300"
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fc4_relu"
  type: "ReLU"
  bottom: "fc4_300"
  top: "fc4_300"
}
layer {
  name: "drop4"
  type: "Dropout"
  bottom: "fc4_300"
  top: "fc4_300"
  dropout_param {
    dropout_ratio: 0.4
  }
}
layer {
  name: "fc5_67"
  type: "InnerProduct"
  bottom: "fc4_300"
  top: "fc5_classes"
  inner_product_param {
    num_output: 67
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc5_classes"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy_1"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_1"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 1
  }
}
layer {
  name: "accuracy_5"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0402 02:05:48.954912 16384 layer_factory.hpp:77] Creating layer data
I0402 02:05:48.955310 16384 net.cpp:100] Creating Layer data
I0402 02:05:48.955338 16384 net.cpp:408] data -> data
I0402 02:05:48.955351 16384 net.cpp:408] data -> label
I0402 02:05:48.956975 16455 db_lmdb.cpp:35] Opened lmdb ../local_data/lmdb/rtsd-r1/CoNorm/test/lmdb
I0402 02:05:48.957154 16384 data_layer.cpp:41] output data size: 512,3,48,48
I0402 02:05:48.982815 16384 net.cpp:150] Setting up data
I0402 02:05:48.982843 16384 net.cpp:157] Top shape: 512 3 48 48 (3538944)
I0402 02:05:48.982848 16384 net.cpp:157] Top shape: 512 (512)
I0402 02:05:48.982851 16384 net.cpp:165] Memory required for data: 14157824
I0402 02:05:48.982857 16384 layer_factory.hpp:77] Creating layer label_data_1_split
I0402 02:05:48.982872 16384 net.cpp:100] Creating Layer label_data_1_split
I0402 02:05:48.982875 16384 net.cpp:434] label_data_1_split <- label
I0402 02:05:48.982884 16384 net.cpp:408] label_data_1_split -> label_data_1_split_0
I0402 02:05:48.982895 16384 net.cpp:408] label_data_1_split -> label_data_1_split_1
I0402 02:05:48.982903 16384 net.cpp:408] label_data_1_split -> label_data_1_split_2
I0402 02:05:48.983031 16384 net.cpp:150] Setting up label_data_1_split
I0402 02:05:48.983058 16384 net.cpp:157] Top shape: 512 (512)
I0402 02:05:48.983062 16384 net.cpp:157] Top shape: 512 (512)
I0402 02:05:48.983067 16384 net.cpp:157] Top shape: 512 (512)
I0402 02:05:48.983068 16384 net.cpp:165] Memory required for data: 14163968
I0402 02:05:48.983072 16384 layer_factory.hpp:77] Creating layer conv1
I0402 02:05:48.983086 16384 net.cpp:100] Creating Layer conv1
I0402 02:05:48.983090 16384 net.cpp:434] conv1 <- data
I0402 02:05:48.983096 16384 net.cpp:408] conv1 -> conv1
I0402 02:05:48.993121 16384 net.cpp:150] Setting up conv1
I0402 02:05:48.993140 16384 net.cpp:157] Top shape: 512 100 42 42 (90316800)
I0402 02:05:48.993144 16384 net.cpp:165] Memory required for data: 375431168
I0402 02:05:48.993156 16384 layer_factory.hpp:77] Creating layer conv1_relu
I0402 02:05:48.993166 16384 net.cpp:100] Creating Layer conv1_relu
I0402 02:05:48.993170 16384 net.cpp:434] conv1_relu <- conv1
I0402 02:05:48.993186 16384 net.cpp:395] conv1_relu -> conv1 (in-place)
I0402 02:05:48.993396 16384 net.cpp:150] Setting up conv1_relu
I0402 02:05:48.993408 16384 net.cpp:157] Top shape: 512 100 42 42 (90316800)
I0402 02:05:48.993412 16384 net.cpp:165] Memory required for data: 736698368
I0402 02:05:48.993415 16384 layer_factory.hpp:77] Creating layer pool1
I0402 02:05:48.993425 16384 net.cpp:100] Creating Layer pool1
I0402 02:05:48.993429 16384 net.cpp:434] pool1 <- conv1
I0402 02:05:48.993434 16384 net.cpp:408] pool1 -> pool1
I0402 02:05:48.993484 16384 net.cpp:150] Setting up pool1
I0402 02:05:48.993492 16384 net.cpp:157] Top shape: 512 100 21 21 (22579200)
I0402 02:05:48.993495 16384 net.cpp:165] Memory required for data: 827015168
I0402 02:05:48.993499 16384 layer_factory.hpp:77] Creating layer conv2
I0402 02:05:48.993507 16384 net.cpp:100] Creating Layer conv2
I0402 02:05:48.993511 16384 net.cpp:434] conv2 <- pool1
I0402 02:05:48.993516 16384 net.cpp:408] conv2 -> conv2
I0402 02:05:48.998499 16384 net.cpp:150] Setting up conv2
I0402 02:05:48.998517 16384 net.cpp:157] Top shape: 512 150 18 18 (24883200)
I0402 02:05:48.998527 16384 net.cpp:165] Memory required for data: 926547968
I0402 02:05:48.998536 16384 layer_factory.hpp:77] Creating layer conv2_relu
I0402 02:05:48.998544 16384 net.cpp:100] Creating Layer conv2_relu
I0402 02:05:48.998548 16384 net.cpp:434] conv2_relu <- conv2
I0402 02:05:48.998553 16384 net.cpp:395] conv2_relu -> conv2 (in-place)
I0402 02:05:49.000619 16384 net.cpp:150] Setting up conv2_relu
I0402 02:05:49.000638 16384 net.cpp:157] Top shape: 512 150 18 18 (24883200)
I0402 02:05:49.000643 16384 net.cpp:165] Memory required for data: 1026080768
I0402 02:05:49.000646 16384 layer_factory.hpp:77] Creating layer pool2
I0402 02:05:49.000653 16384 net.cpp:100] Creating Layer pool2
I0402 02:05:49.000656 16384 net.cpp:434] pool2 <- conv2
I0402 02:05:49.000663 16384 net.cpp:408] pool2 -> pool2
I0402 02:05:49.000715 16384 net.cpp:150] Setting up pool2
I0402 02:05:49.000725 16384 net.cpp:157] Top shape: 512 150 9 9 (6220800)
I0402 02:05:49.000730 16384 net.cpp:165] Memory required for data: 1050963968
I0402 02:05:49.000732 16384 layer_factory.hpp:77] Creating layer conv3
I0402 02:05:49.000742 16384 net.cpp:100] Creating Layer conv3
I0402 02:05:49.000757 16384 net.cpp:434] conv3 <- pool2
I0402 02:05:49.000763 16384 net.cpp:408] conv3 -> conv3
I0402 02:05:49.009531 16384 net.cpp:150] Setting up conv3
I0402 02:05:49.009548 16384 net.cpp:157] Top shape: 512 250 6 6 (4608000)
I0402 02:05:49.009552 16384 net.cpp:165] Memory required for data: 1069395968
I0402 02:05:49.009563 16384 layer_factory.hpp:77] Creating layer conv3_relu
I0402 02:05:49.009569 16384 net.cpp:100] Creating Layer conv3_relu
I0402 02:05:49.009574 16384 net.cpp:434] conv3_relu <- conv3
I0402 02:05:49.009582 16384 net.cpp:395] conv3_relu -> conv3 (in-place)
I0402 02:05:49.011616 16384 net.cpp:150] Setting up conv3_relu
I0402 02:05:49.011632 16384 net.cpp:157] Top shape: 512 250 6 6 (4608000)
I0402 02:05:49.011636 16384 net.cpp:165] Memory required for data: 1087827968
I0402 02:05:49.011639 16384 layer_factory.hpp:77] Creating layer pool3
I0402 02:05:49.011658 16384 net.cpp:100] Creating Layer pool3
I0402 02:05:49.011677 16384 net.cpp:434] pool3 <- conv3
I0402 02:05:49.011684 16384 net.cpp:408] pool3 -> pool3
I0402 02:05:49.011735 16384 net.cpp:150] Setting up pool3
I0402 02:05:49.011744 16384 net.cpp:157] Top shape: 512 250 3 3 (1152000)
I0402 02:05:49.011746 16384 net.cpp:165] Memory required for data: 1092435968
I0402 02:05:49.011749 16384 layer_factory.hpp:77] Creating layer fc4_300
I0402 02:05:49.011756 16384 net.cpp:100] Creating Layer fc4_300
I0402 02:05:49.011759 16384 net.cpp:434] fc4_300 <- pool3
I0402 02:05:49.011766 16384 net.cpp:408] fc4_300 -> fc4_300
I0402 02:05:49.017206 16384 net.cpp:150] Setting up fc4_300
I0402 02:05:49.017222 16384 net.cpp:157] Top shape: 512 300 (153600)
I0402 02:05:49.017225 16384 net.cpp:165] Memory required for data: 1093050368
I0402 02:05:49.017232 16384 layer_factory.hpp:77] Creating layer fc4_relu
I0402 02:05:49.017238 16384 net.cpp:100] Creating Layer fc4_relu
I0402 02:05:49.017242 16384 net.cpp:434] fc4_relu <- fc4_300
I0402 02:05:49.017289 16384 net.cpp:395] fc4_relu -> fc4_300 (in-place)
I0402 02:05:49.017495 16384 net.cpp:150] Setting up fc4_relu
I0402 02:05:49.017508 16384 net.cpp:157] Top shape: 512 300 (153600)
I0402 02:05:49.017510 16384 net.cpp:165] Memory required for data: 1093664768
I0402 02:05:49.017513 16384 layer_factory.hpp:77] Creating layer drop4
I0402 02:05:49.017520 16384 net.cpp:100] Creating Layer drop4
I0402 02:05:49.017524 16384 net.cpp:434] drop4 <- fc4_300
I0402 02:05:49.017531 16384 net.cpp:395] drop4 -> fc4_300 (in-place)
I0402 02:05:49.017562 16384 net.cpp:150] Setting up drop4
I0402 02:05:49.017571 16384 net.cpp:157] Top shape: 512 300 (153600)
I0402 02:05:49.017575 16384 net.cpp:165] Memory required for data: 1094279168
I0402 02:05:49.017578 16384 layer_factory.hpp:77] Creating layer fc5_67
I0402 02:05:49.017588 16384 net.cpp:100] Creating Layer fc5_67
I0402 02:05:49.017591 16384 net.cpp:434] fc5_67 <- fc4_300
I0402 02:05:49.017596 16384 net.cpp:408] fc5_67 -> fc5_classes
I0402 02:05:49.017844 16384 net.cpp:150] Setting up fc5_67
I0402 02:05:49.017853 16384 net.cpp:157] Top shape: 512 67 (34304)
I0402 02:05:49.017856 16384 net.cpp:165] Memory required for data: 1094416384
I0402 02:05:49.017865 16384 layer_factory.hpp:77] Creating layer fc5_classes_fc5_67_0_split
I0402 02:05:49.017874 16384 net.cpp:100] Creating Layer fc5_classes_fc5_67_0_split
I0402 02:05:49.017884 16384 net.cpp:434] fc5_classes_fc5_67_0_split <- fc5_classes
I0402 02:05:49.017890 16384 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_0
I0402 02:05:49.017899 16384 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_1
I0402 02:05:49.017905 16384 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_2
I0402 02:05:49.017957 16384 net.cpp:150] Setting up fc5_classes_fc5_67_0_split
I0402 02:05:49.017966 16384 net.cpp:157] Top shape: 512 67 (34304)
I0402 02:05:49.017968 16384 net.cpp:157] Top shape: 512 67 (34304)
I0402 02:05:49.017971 16384 net.cpp:157] Top shape: 512 67 (34304)
I0402 02:05:49.017974 16384 net.cpp:165] Memory required for data: 1094828032
I0402 02:05:49.017977 16384 layer_factory.hpp:77] Creating layer loss
I0402 02:05:49.017987 16384 net.cpp:100] Creating Layer loss
I0402 02:05:49.018002 16384 net.cpp:434] loss <- fc5_classes_fc5_67_0_split_0
I0402 02:05:49.018007 16384 net.cpp:434] loss <- label_data_1_split_0
I0402 02:05:49.018013 16384 net.cpp:408] loss -> loss
I0402 02:05:49.018023 16384 layer_factory.hpp:77] Creating layer loss
I0402 02:05:49.018347 16384 net.cpp:150] Setting up loss
I0402 02:05:49.018360 16384 net.cpp:157] Top shape: (1)
I0402 02:05:49.018364 16384 net.cpp:160]     with loss weight 1
I0402 02:05:49.018374 16384 net.cpp:165] Memory required for data: 1094828036
I0402 02:05:49.018378 16384 layer_factory.hpp:77] Creating layer accuracy_1
I0402 02:05:49.018386 16384 net.cpp:100] Creating Layer accuracy_1
I0402 02:05:49.018389 16384 net.cpp:434] accuracy_1 <- fc5_classes_fc5_67_0_split_1
I0402 02:05:49.018394 16384 net.cpp:434] accuracy_1 <- label_data_1_split_1
I0402 02:05:49.018412 16384 net.cpp:408] accuracy_1 -> accuracy_1
I0402 02:05:49.018424 16384 net.cpp:150] Setting up accuracy_1
I0402 02:05:49.018431 16384 net.cpp:157] Top shape: (1)
I0402 02:05:49.018435 16384 net.cpp:165] Memory required for data: 1094828040
I0402 02:05:49.018437 16384 layer_factory.hpp:77] Creating layer accuracy_5
I0402 02:05:49.018442 16384 net.cpp:100] Creating Layer accuracy_5
I0402 02:05:49.018445 16384 net.cpp:434] accuracy_5 <- fc5_classes_fc5_67_0_split_2
I0402 02:05:49.018450 16384 net.cpp:434] accuracy_5 <- label_data_1_split_2
I0402 02:05:49.018456 16384 net.cpp:408] accuracy_5 -> accuracy_5
I0402 02:05:49.018462 16384 net.cpp:150] Setting up accuracy_5
I0402 02:05:49.018466 16384 net.cpp:157] Top shape: (1)
I0402 02:05:49.018470 16384 net.cpp:165] Memory required for data: 1094828044
I0402 02:05:49.018472 16384 net.cpp:228] accuracy_5 does not need backward computation.
I0402 02:05:49.018483 16384 net.cpp:228] accuracy_1 does not need backward computation.
I0402 02:05:49.018496 16384 net.cpp:226] loss needs backward computation.
I0402 02:05:49.018499 16384 net.cpp:226] fc5_classes_fc5_67_0_split needs backward computation.
I0402 02:05:49.018502 16384 net.cpp:226] fc5_67 needs backward computation.
I0402 02:05:49.018506 16384 net.cpp:226] drop4 needs backward computation.
I0402 02:05:49.018508 16384 net.cpp:226] fc4_relu needs backward computation.
I0402 02:05:49.018512 16384 net.cpp:226] fc4_300 needs backward computation.
I0402 02:05:49.018514 16384 net.cpp:226] pool3 needs backward computation.
I0402 02:05:49.018517 16384 net.cpp:226] conv3_relu needs backward computation.
I0402 02:05:49.018519 16384 net.cpp:226] conv3 needs backward computation.
I0402 02:05:49.018522 16384 net.cpp:226] pool2 needs backward computation.
I0402 02:05:49.018525 16384 net.cpp:226] conv2_relu needs backward computation.
I0402 02:05:49.018528 16384 net.cpp:226] conv2 needs backward computation.
I0402 02:05:49.018532 16384 net.cpp:226] pool1 needs backward computation.
I0402 02:05:49.018533 16384 net.cpp:226] conv1_relu needs backward computation.
I0402 02:05:49.018537 16384 net.cpp:226] conv1 needs backward computation.
I0402 02:05:49.018540 16384 net.cpp:228] label_data_1_split does not need backward computation.
I0402 02:05:49.018543 16384 net.cpp:228] data does not need backward computation.
I0402 02:05:49.018546 16384 net.cpp:270] This network produces output accuracy_1
I0402 02:05:49.018549 16384 net.cpp:270] This network produces output accuracy_5
I0402 02:05:49.018553 16384 net.cpp:270] This network produces output loss
I0402 02:05:49.018570 16384 net.cpp:283] Network initialization done.
I0402 02:05:49.018626 16384 solver.cpp:72] Solver scaffolding done.
I0402 02:05:49.019140 16384 caffe.cpp:251] Starting Optimization
I0402 02:05:49.019150 16384 solver.cpp:291] Solving 
I0402 02:05:49.019152 16384 solver.cpp:292] Learning Rate Policy: step
I0402 02:05:49.021589 16384 solver.cpp:349] Iteration 0, Testing net (#0)
I0402 02:05:50.631785 16384 solver.cpp:416]     Test net output #0: accuracy_1 = 0.00885417
I0402 02:05:50.631817 16384 solver.cpp:416]     Test net output #1: accuracy_5 = 0.0596354
I0402 02:05:50.631829 16384 solver.cpp:416]     Test net output #2: loss = 73.0288 (* 1 = 73.0288 loss)
I0402 02:05:50.761520 16384 solver.cpp:240] Iteration 0, loss = 75.5098
I0402 02:05:50.761556 16384 solver.cpp:256]     Train net output #0: loss = 75.5098 (* 1 = 75.5098 loss)
I0402 02:05:50.761571 16384 sgd_solver.cpp:106] Iteration 0, lr = 0.0001
I0402 02:05:51.092218 16384 solver.cpp:240] Iteration 1, loss = 66.0083
I0402 02:05:51.092277 16384 solver.cpp:256]     Train net output #0: loss = 66.0083 (* 1 = 66.0083 loss)
I0402 02:05:51.092285 16384 sgd_solver.cpp:106] Iteration 1, lr = 0.0001
I0402 02:05:51.423671 16384 solver.cpp:240] Iteration 2, loss = 53.1694
I0402 02:05:51.423707 16384 solver.cpp:256]     Train net output #0: loss = 53.1694 (* 1 = 53.1694 loss)
I0402 02:05:51.423715 16384 sgd_solver.cpp:106] Iteration 2, lr = 0.0001
I0402 02:05:51.754364 16384 solver.cpp:240] Iteration 3, loss = 45.3708
I0402 02:05:51.754426 16384 solver.cpp:256]     Train net output #0: loss = 45.3708 (* 1 = 45.3708 loss)
I0402 02:05:51.754436 16384 sgd_solver.cpp:106] Iteration 3, lr = 0.0001
I0402 02:05:52.083093 16384 solver.cpp:240] Iteration 4, loss = 40.1562
I0402 02:05:52.083132 16384 solver.cpp:256]     Train net output #0: loss = 40.1562 (* 1 = 40.1562 loss)
I0402 02:05:52.083139 16384 sgd_solver.cpp:106] Iteration 4, lr = 0.0001
I0402 02:05:52.411334 16384 solver.cpp:240] Iteration 5, loss = 33.615
I0402 02:05:52.411371 16384 solver.cpp:256]     Train net output #0: loss = 33.615 (* 1 = 33.615 loss)
I0402 02:05:52.411379 16384 sgd_solver.cpp:106] Iteration 5, lr = 0.0001
I0402 02:05:52.739398 16384 solver.cpp:240] Iteration 6, loss = 26.8898
I0402 02:05:52.739436 16384 solver.cpp:256]     Train net output #0: loss = 26.8898 (* 1 = 26.8898 loss)
I0402 02:05:52.739445 16384 sgd_solver.cpp:106] Iteration 6, lr = 0.0001
I0402 02:05:53.069604 16384 solver.cpp:240] Iteration 7, loss = 23.4647
I0402 02:05:53.069656 16384 solver.cpp:256]     Train net output #0: loss = 23.4647 (* 1 = 23.4647 loss)
I0402 02:05:53.069671 16384 sgd_solver.cpp:106] Iteration 7, lr = 0.0001
I0402 02:05:53.399760 16384 solver.cpp:240] Iteration 8, loss = 19.5688
I0402 02:05:53.399797 16384 solver.cpp:256]     Train net output #0: loss = 19.5688 (* 1 = 19.5688 loss)
I0402 02:05:53.399806 16384 sgd_solver.cpp:106] Iteration 8, lr = 0.0001
I0402 02:05:53.728224 16384 solver.cpp:240] Iteration 9, loss = 16.8417
I0402 02:05:53.728272 16384 solver.cpp:256]     Train net output #0: loss = 16.8417 (* 1 = 16.8417 loss)
I0402 02:05:53.728281 16384 sgd_solver.cpp:106] Iteration 9, lr = 0.0001
I0402 02:05:54.055974 16384 solver.cpp:240] Iteration 10, loss = 14.3325
I0402 02:05:54.056012 16384 solver.cpp:256]     Train net output #0: loss = 14.3325 (* 1 = 14.3325 loss)
I0402 02:05:54.056020 16384 sgd_solver.cpp:106] Iteration 10, lr = 0.0001
I0402 02:05:54.386415 16384 solver.cpp:240] Iteration 11, loss = 11.6068
I0402 02:05:54.386467 16384 solver.cpp:256]     Train net output #0: loss = 11.6068 (* 1 = 11.6068 loss)
I0402 02:05:54.386476 16384 sgd_solver.cpp:106] Iteration 11, lr = 0.0001
I0402 02:05:54.715116 16384 solver.cpp:240] Iteration 12, loss = 10.4542
I0402 02:05:54.715157 16384 solver.cpp:256]     Train net output #0: loss = 10.4542 (* 1 = 10.4542 loss)
I0402 02:05:54.715165 16384 sgd_solver.cpp:106] Iteration 12, lr = 0.0001
I0402 02:05:55.043522 16384 solver.cpp:240] Iteration 13, loss = 9.23772
I0402 02:05:55.043565 16384 solver.cpp:256]     Train net output #0: loss = 9.23772 (* 1 = 9.23772 loss)
I0402 02:05:55.043573 16384 sgd_solver.cpp:106] Iteration 13, lr = 0.0001
I0402 02:05:55.371943 16384 solver.cpp:240] Iteration 14, loss = 8.47533
I0402 02:05:55.371979 16384 solver.cpp:256]     Train net output #0: loss = 8.47533 (* 1 = 8.47533 loss)
I0402 02:05:55.371987 16384 sgd_solver.cpp:106] Iteration 14, lr = 0.0001
I0402 02:05:55.702914 16384 solver.cpp:240] Iteration 15, loss = 7.17834
I0402 02:05:55.702953 16384 solver.cpp:256]     Train net output #0: loss = 7.17834 (* 1 = 7.17834 loss)
I0402 02:05:55.702963 16384 sgd_solver.cpp:106] Iteration 15, lr = 0.0001
I0402 02:05:56.033109 16384 solver.cpp:240] Iteration 16, loss = 6.61292
I0402 02:05:56.033145 16384 solver.cpp:256]     Train net output #0: loss = 6.61292 (* 1 = 6.61292 loss)
I0402 02:05:56.033154 16384 sgd_solver.cpp:106] Iteration 16, lr = 0.0001
I0402 02:05:56.360419 16384 solver.cpp:240] Iteration 17, loss = 6.10845
I0402 02:05:56.360455 16384 solver.cpp:256]     Train net output #0: loss = 6.10845 (* 1 = 6.10845 loss)
I0402 02:05:56.360463 16384 sgd_solver.cpp:106] Iteration 17, lr = 0.0001
I0402 02:05:56.691117 16384 solver.cpp:240] Iteration 18, loss = 5.53507
I0402 02:05:56.691151 16384 solver.cpp:256]     Train net output #0: loss = 5.53507 (* 1 = 5.53507 loss)
I0402 02:05:56.691159 16384 sgd_solver.cpp:106] Iteration 18, lr = 0.0001
I0402 02:05:57.020784 16384 solver.cpp:240] Iteration 19, loss = 5.08719
I0402 02:05:57.020819 16384 solver.cpp:256]     Train net output #0: loss = 5.08719 (* 1 = 5.08719 loss)
I0402 02:05:57.020848 16384 sgd_solver.cpp:106] Iteration 19, lr = 0.0001
I0402 02:05:57.350833 16384 solver.cpp:240] Iteration 20, loss = 4.62041
I0402 02:05:57.350869 16384 solver.cpp:256]     Train net output #0: loss = 4.62041 (* 1 = 4.62041 loss)
I0402 02:05:57.350878 16384 sgd_solver.cpp:106] Iteration 20, lr = 0.0001
I0402 02:05:57.681299 16384 solver.cpp:240] Iteration 21, loss = 4.51076
I0402 02:05:57.681334 16384 solver.cpp:256]     Train net output #0: loss = 4.51076 (* 1 = 4.51076 loss)
I0402 02:05:57.681344 16384 sgd_solver.cpp:106] Iteration 21, lr = 0.0001
I0402 02:05:58.014655 16384 solver.cpp:240] Iteration 22, loss = 4.38038
I0402 02:05:58.014691 16384 solver.cpp:256]     Train net output #0: loss = 4.38038 (* 1 = 4.38038 loss)
I0402 02:05:58.014699 16384 sgd_solver.cpp:106] Iteration 22, lr = 0.0001
I0402 02:05:58.342911 16384 solver.cpp:240] Iteration 23, loss = 4.25354
I0402 02:05:58.342957 16384 solver.cpp:256]     Train net output #0: loss = 4.25354 (* 1 = 4.25354 loss)
I0402 02:05:58.342965 16384 sgd_solver.cpp:106] Iteration 23, lr = 0.0001
I0402 02:05:58.671977 16384 solver.cpp:240] Iteration 24, loss = 4.26852
I0402 02:05:58.672026 16384 solver.cpp:256]     Train net output #0: loss = 4.26852 (* 1 = 4.26852 loss)
I0402 02:05:58.672035 16384 sgd_solver.cpp:106] Iteration 24, lr = 0.0001
I0402 02:05:59.002404 16384 solver.cpp:240] Iteration 25, loss = 4.17972
I0402 02:05:59.002440 16384 solver.cpp:256]     Train net output #0: loss = 4.17972 (* 1 = 4.17972 loss)
I0402 02:05:59.002449 16384 sgd_solver.cpp:106] Iteration 25, lr = 0.0001
I0402 02:05:59.329185 16384 solver.cpp:240] Iteration 26, loss = 4.20037
I0402 02:05:59.329232 16384 solver.cpp:256]     Train net output #0: loss = 4.20037 (* 1 = 4.20037 loss)
I0402 02:05:59.329239 16384 sgd_solver.cpp:106] Iteration 26, lr = 0.0001
I0402 02:05:59.660282 16384 solver.cpp:240] Iteration 27, loss = 4.19713
I0402 02:05:59.660315 16384 solver.cpp:256]     Train net output #0: loss = 4.19713 (* 1 = 4.19713 loss)
I0402 02:05:59.660323 16384 sgd_solver.cpp:106] Iteration 27, lr = 0.0001
I0402 02:05:59.989698 16384 solver.cpp:240] Iteration 28, loss = 4.18646
I0402 02:05:59.989733 16384 solver.cpp:256]     Train net output #0: loss = 4.18646 (* 1 = 4.18646 loss)
I0402 02:05:59.989742 16384 sgd_solver.cpp:106] Iteration 28, lr = 0.0001
I0402 02:06:00.319114 16384 solver.cpp:240] Iteration 29, loss = 4.15438
I0402 02:06:00.319164 16384 solver.cpp:256]     Train net output #0: loss = 4.15438 (* 1 = 4.15438 loss)
I0402 02:06:00.319171 16384 sgd_solver.cpp:106] Iteration 29, lr = 0.0001
I0402 02:06:00.648461 16384 solver.cpp:240] Iteration 30, loss = 4.11479
I0402 02:06:00.648496 16384 solver.cpp:256]     Train net output #0: loss = 4.11479 (* 1 = 4.11479 loss)
I0402 02:06:00.648504 16384 sgd_solver.cpp:106] Iteration 30, lr = 0.0001
I0402 02:06:00.979715 16384 solver.cpp:240] Iteration 31, loss = 4.13426
I0402 02:06:00.979748 16384 solver.cpp:256]     Train net output #0: loss = 4.13426 (* 1 = 4.13426 loss)
I0402 02:06:00.979758 16384 sgd_solver.cpp:106] Iteration 31, lr = 0.0001
I0402 02:06:01.310034 16384 solver.cpp:240] Iteration 32, loss = 4.12722
I0402 02:06:01.310070 16384 solver.cpp:256]     Train net output #0: loss = 4.12722 (* 1 = 4.12722 loss)
I0402 02:06:01.310077 16384 sgd_solver.cpp:106] Iteration 32, lr = 0.0001
I0402 02:06:01.641373 16384 solver.cpp:240] Iteration 33, loss = 4.15682
I0402 02:06:01.641419 16384 solver.cpp:256]     Train net output #0: loss = 4.15682 (* 1 = 4.15682 loss)
I0402 02:06:01.641427 16384 sgd_solver.cpp:106] Iteration 33, lr = 0.0001
I0402 02:06:01.972596 16384 solver.cpp:240] Iteration 34, loss = 4.14961
I0402 02:06:01.972630 16384 solver.cpp:256]     Train net output #0: loss = 4.14961 (* 1 = 4.14961 loss)
I0402 02:06:01.972636 16384 sgd_solver.cpp:106] Iteration 34, lr = 0.0001
I0402 02:06:02.303093 16384 solver.cpp:240] Iteration 35, loss = 4.06494
I0402 02:06:02.303128 16384 solver.cpp:256]     Train net output #0: loss = 4.06494 (* 1 = 4.06494 loss)
I0402 02:06:02.303138 16384 sgd_solver.cpp:106] Iteration 35, lr = 0.0001
I0402 02:06:02.633460 16384 solver.cpp:240] Iteration 36, loss = 4.07177
I0402 02:06:02.633497 16384 solver.cpp:256]     Train net output #0: loss = 4.07177 (* 1 = 4.07177 loss)
I0402 02:06:02.633507 16384 sgd_solver.cpp:106] Iteration 36, lr = 0.0001
I0402 02:06:02.963806 16384 solver.cpp:240] Iteration 37, loss = 4.06307
I0402 02:06:02.963840 16384 solver.cpp:256]     Train net output #0: loss = 4.06307 (* 1 = 4.06307 loss)
I0402 02:06:02.963848 16384 sgd_solver.cpp:106] Iteration 37, lr = 0.0001
I0402 02:06:03.295649 16384 solver.cpp:240] Iteration 38, loss = 4.08481
I0402 02:06:03.295682 16384 solver.cpp:256]     Train net output #0: loss = 4.08481 (* 1 = 4.08481 loss)
I0402 02:06:03.295691 16384 sgd_solver.cpp:106] Iteration 38, lr = 0.0001
I0402 02:06:03.619382 16384 solver.cpp:240] Iteration 39, loss = 4.07746
I0402 02:06:03.619427 16384 solver.cpp:256]     Train net output #0: loss = 4.07746 (* 1 = 4.07746 loss)
I0402 02:06:03.619436 16384 sgd_solver.cpp:106] Iteration 39, lr = 0.0001
I0402 02:06:03.951035 16384 solver.cpp:240] Iteration 40, loss = 4.04893
I0402 02:06:03.951081 16384 solver.cpp:256]     Train net output #0: loss = 4.04893 (* 1 = 4.04893 loss)
I0402 02:06:03.951088 16384 sgd_solver.cpp:106] Iteration 40, lr = 0.0001
I0402 02:06:04.282759 16384 solver.cpp:240] Iteration 41, loss = 3.98569
I0402 02:06:04.282796 16384 solver.cpp:256]     Train net output #0: loss = 3.98569 (* 1 = 3.98569 loss)
I0402 02:06:04.282804 16384 sgd_solver.cpp:106] Iteration 41, lr = 0.0001
I0402 02:06:04.614228 16384 solver.cpp:240] Iteration 42, loss = 4.04513
I0402 02:06:04.614264 16384 solver.cpp:256]     Train net output #0: loss = 4.04513 (* 1 = 4.04513 loss)
I0402 02:06:04.614272 16384 sgd_solver.cpp:106] Iteration 42, lr = 0.0001
I0402 02:06:04.947798 16384 solver.cpp:240] Iteration 43, loss = 4.05348
I0402 02:06:04.947830 16384 solver.cpp:256]     Train net output #0: loss = 4.05348 (* 1 = 4.05348 loss)
I0402 02:06:04.947839 16384 sgd_solver.cpp:106] Iteration 43, lr = 0.0001
I0402 02:06:05.277992 16384 solver.cpp:240] Iteration 44, loss = 4.00086
I0402 02:06:05.278038 16384 solver.cpp:256]     Train net output #0: loss = 4.00086 (* 1 = 4.00086 loss)
I0402 02:06:05.278046 16384 sgd_solver.cpp:106] Iteration 44, lr = 0.0001
I0402 02:06:05.609335 16384 solver.cpp:240] Iteration 45, loss = 4.11511
I0402 02:06:05.609383 16384 solver.cpp:256]     Train net output #0: loss = 4.11511 (* 1 = 4.11511 loss)
I0402 02:06:05.609391 16384 sgd_solver.cpp:106] Iteration 45, lr = 0.0001
I0402 02:06:05.939910 16384 solver.cpp:240] Iteration 46, loss = 4.01989
I0402 02:06:05.939944 16384 solver.cpp:256]     Train net output #0: loss = 4.01989 (* 1 = 4.01989 loss)
I0402 02:06:05.939952 16384 sgd_solver.cpp:106] Iteration 46, lr = 0.0001
I0402 02:06:06.268715 16384 solver.cpp:240] Iteration 47, loss = 3.92809
I0402 02:06:06.268762 16384 solver.cpp:256]     Train net output #0: loss = 3.92809 (* 1 = 3.92809 loss)
I0402 02:06:06.268770 16384 sgd_solver.cpp:106] Iteration 47, lr = 0.0001
I0402 02:06:06.598356 16384 solver.cpp:240] Iteration 48, loss = 4.02274
I0402 02:06:06.598392 16384 solver.cpp:256]     Train net output #0: loss = 4.02274 (* 1 = 4.02274 loss)
I0402 02:06:06.598402 16384 sgd_solver.cpp:106] Iteration 48, lr = 0.0001
I0402 02:06:06.928083 16384 solver.cpp:240] Iteration 49, loss = 3.95545
I0402 02:06:06.928131 16384 solver.cpp:256]     Train net output #0: loss = 3.95545 (* 1 = 3.95545 loss)
I0402 02:06:06.928139 16384 sgd_solver.cpp:106] Iteration 49, lr = 0.0001
I0402 02:06:07.257555 16384 solver.cpp:240] Iteration 50, loss = 4.09532
I0402 02:06:07.257591 16384 solver.cpp:256]     Train net output #0: loss = 4.09532 (* 1 = 4.09532 loss)
I0402 02:06:07.257598 16384 sgd_solver.cpp:106] Iteration 50, lr = 0.0001
I0402 02:06:07.587047 16384 solver.cpp:240] Iteration 51, loss = 3.97615
I0402 02:06:07.587080 16384 solver.cpp:256]     Train net output #0: loss = 3.97615 (* 1 = 3.97615 loss)
I0402 02:06:07.587090 16384 sgd_solver.cpp:106] Iteration 51, lr = 0.0001
I0402 02:06:07.916391 16384 solver.cpp:240] Iteration 52, loss = 4.01389
I0402 02:06:07.916442 16384 solver.cpp:256]     Train net output #0: loss = 4.01389 (* 1 = 4.01389 loss)
I0402 02:06:07.916450 16384 sgd_solver.cpp:106] Iteration 52, lr = 0.0001
I0402 02:06:08.243908 16384 solver.cpp:240] Iteration 53, loss = 4.03277
I0402 02:06:08.243954 16384 solver.cpp:256]     Train net output #0: loss = 4.03277 (* 1 = 4.03277 loss)
I0402 02:06:08.243963 16384 sgd_solver.cpp:106] Iteration 53, lr = 0.0001
I0402 02:06:08.572737 16384 solver.cpp:240] Iteration 54, loss = 3.91776
I0402 02:06:08.572788 16384 solver.cpp:256]     Train net output #0: loss = 3.91776 (* 1 = 3.91776 loss)
I0402 02:06:08.572795 16384 sgd_solver.cpp:106] Iteration 54, lr = 0.0001
