I0409 19:55:50.697655 20605 caffe.cpp:217] Using GPUs 1
I0409 19:55:50.987120 20605 caffe.cpp:222] GPU 1: GeForce GTX 1070
I0409 19:55:51.657693 20605 solver.cpp:60] Initializing solver from parameters: 
train_net: "./Prototxt/experiment_7/rtsd-r1/CoNorm/trial_1/train.prototxt"
test_net: "./Prototxt/experiment_7/rtsd-r1/CoNorm/trial_1/test.prototxt"
test_iter: 8
test_interval: 25
base_lr: 5e-05
display: 1
max_iter: 2500
lr_policy: "step"
gamma: 0.5
momentum: 0.9
weight_decay: 0.0005
stepsize: 500
snapshot: 250
snapshot_prefix: "./snapshots/experiment_7/rtsd-r1/CoNorm/trial_1/snap"
solver_mode: GPU
device_id: 1
train_state {
  level: 0
  stage: ""
}
iter_size: 1
type: "Adam"
I0409 19:55:51.657837 20605 solver.cpp:93] Creating training net from train_net file: ./Prototxt/experiment_7/rtsd-r1/CoNorm/trial_1/train.prototxt
I0409 19:55:51.658082 20605 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_1
I0409 19:55:51.658092 20605 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_5
I0409 19:55:51.658187 20605 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: false
    crop_size: 48
    mean_value: 132
    mean_value: 132
    mean_value: 131
  }
  data_param {
    source: "../local_data/lmdb/rtsd-r1/CoNorm/train/lmdb"
    batch_size: 1024
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 7
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_sTanH"
  type: "TanH"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 150
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_sTanH"
  type: "TanH"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 250
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_sTanH"
  type: "TanH"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc4_300"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc4_300"
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fc4_sTanH"
  type: "TanH"
  bottom: "fc4_300"
  top: "fc4_300"
}
layer {
  name: "drop4"
  type: "Dropout"
  bottom: "fc4_300"
  top: "fc4_300"
  dropout_param {
    dropout_ratio: 0.4
  }
}
layer {
  name: "fc5_67"
  type: "InnerProduct"
  bottom: "fc4_300"
  top: "fc5_classes"
  inner_product_param {
    num_output: 67
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc5_classes"
  bottom: "label"
  top: "loss"
}
I0409 19:55:51.658272 20605 layer_factory.hpp:77] Creating layer data
I0409 19:55:51.659338 20605 net.cpp:100] Creating Layer data
I0409 19:55:51.659353 20605 net.cpp:408] data -> data
I0409 19:55:51.659390 20605 net.cpp:408] data -> label
I0409 19:55:51.660979 20685 db_lmdb.cpp:35] Opened lmdb ../local_data/lmdb/rtsd-r1/CoNorm/train/lmdb
I0409 19:55:51.677130 20605 data_layer.cpp:41] output data size: 1024,3,48,48
I0409 19:55:51.722375 20605 net.cpp:150] Setting up data
I0409 19:55:51.722409 20605 net.cpp:157] Top shape: 1024 3 48 48 (7077888)
I0409 19:55:51.722453 20605 net.cpp:157] Top shape: 1024 (1024)
I0409 19:55:51.722457 20605 net.cpp:165] Memory required for data: 28315648
I0409 19:55:51.722467 20605 layer_factory.hpp:77] Creating layer conv1
I0409 19:55:51.722488 20605 net.cpp:100] Creating Layer conv1
I0409 19:55:51.722496 20605 net.cpp:434] conv1 <- data
I0409 19:55:51.722512 20605 net.cpp:408] conv1 -> conv1
I0409 19:55:51.995779 20605 net.cpp:150] Setting up conv1
I0409 19:55:51.995808 20605 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0409 19:55:51.995812 20605 net.cpp:165] Memory required for data: 750850048
I0409 19:55:51.995834 20605 layer_factory.hpp:77] Creating layer conv1_sTanH
I0409 19:55:51.995847 20605 net.cpp:100] Creating Layer conv1_sTanH
I0409 19:55:51.995865 20605 net.cpp:434] conv1_sTanH <- conv1
I0409 19:55:51.995872 20605 net.cpp:395] conv1_sTanH -> conv1 (in-place)
I0409 19:55:51.996075 20605 net.cpp:150] Setting up conv1_sTanH
I0409 19:55:51.996088 20605 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0409 19:55:51.996091 20605 net.cpp:165] Memory required for data: 1473384448
I0409 19:55:51.996095 20605 layer_factory.hpp:77] Creating layer pool1
I0409 19:55:51.996104 20605 net.cpp:100] Creating Layer pool1
I0409 19:55:51.996107 20605 net.cpp:434] pool1 <- conv1
I0409 19:55:51.996114 20605 net.cpp:408] pool1 -> pool1
I0409 19:55:51.996165 20605 net.cpp:150] Setting up pool1
I0409 19:55:51.996175 20605 net.cpp:157] Top shape: 1024 100 21 21 (45158400)
I0409 19:55:51.996177 20605 net.cpp:165] Memory required for data: 1654018048
I0409 19:55:51.996181 20605 layer_factory.hpp:77] Creating layer conv2
I0409 19:55:51.996191 20605 net.cpp:100] Creating Layer conv2
I0409 19:55:51.996196 20605 net.cpp:434] conv2 <- pool1
I0409 19:55:51.996201 20605 net.cpp:408] conv2 -> conv2
I0409 19:55:52.000567 20605 net.cpp:150] Setting up conv2
I0409 19:55:52.000586 20605 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0409 19:55:52.000591 20605 net.cpp:165] Memory required for data: 1853083648
I0409 19:55:52.000612 20605 layer_factory.hpp:77] Creating layer conv2_sTanH
I0409 19:55:52.000620 20605 net.cpp:100] Creating Layer conv2_sTanH
I0409 19:55:52.000625 20605 net.cpp:434] conv2_sTanH <- conv2
I0409 19:55:52.000630 20605 net.cpp:395] conv2_sTanH -> conv2 (in-place)
I0409 19:55:52.001338 20605 net.cpp:150] Setting up conv2_sTanH
I0409 19:55:52.001353 20605 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0409 19:55:52.001358 20605 net.cpp:165] Memory required for data: 2052149248
I0409 19:55:52.001363 20605 layer_factory.hpp:77] Creating layer pool2
I0409 19:55:52.001369 20605 net.cpp:100] Creating Layer pool2
I0409 19:55:52.001372 20605 net.cpp:434] pool2 <- conv2
I0409 19:55:52.001379 20605 net.cpp:408] pool2 -> pool2
I0409 19:55:52.001422 20605 net.cpp:150] Setting up pool2
I0409 19:55:52.001430 20605 net.cpp:157] Top shape: 1024 150 9 9 (12441600)
I0409 19:55:52.001433 20605 net.cpp:165] Memory required for data: 2101915648
I0409 19:55:52.001436 20605 layer_factory.hpp:77] Creating layer conv3
I0409 19:55:52.001446 20605 net.cpp:100] Creating Layer conv3
I0409 19:55:52.001449 20605 net.cpp:434] conv3 <- pool2
I0409 19:55:52.001454 20605 net.cpp:408] conv3 -> conv3
I0409 19:55:52.006893 20605 net.cpp:150] Setting up conv3
I0409 19:55:52.006911 20605 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0409 19:55:52.006916 20605 net.cpp:165] Memory required for data: 2138779648
I0409 19:55:52.006937 20605 layer_factory.hpp:77] Creating layer conv3_sTanH
I0409 19:55:52.006945 20605 net.cpp:100] Creating Layer conv3_sTanH
I0409 19:55:52.006950 20605 net.cpp:434] conv3_sTanH <- conv3
I0409 19:55:52.006955 20605 net.cpp:395] conv3_sTanH -> conv3 (in-place)
I0409 19:55:52.007705 20605 net.cpp:150] Setting up conv3_sTanH
I0409 19:55:52.007721 20605 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0409 19:55:52.007725 20605 net.cpp:165] Memory required for data: 2175643648
I0409 19:55:52.007730 20605 layer_factory.hpp:77] Creating layer pool3
I0409 19:55:52.007736 20605 net.cpp:100] Creating Layer pool3
I0409 19:55:52.007740 20605 net.cpp:434] pool3 <- conv3
I0409 19:55:52.007763 20605 net.cpp:408] pool3 -> pool3
I0409 19:55:52.007812 20605 net.cpp:150] Setting up pool3
I0409 19:55:52.007820 20605 net.cpp:157] Top shape: 1024 250 3 3 (2304000)
I0409 19:55:52.007823 20605 net.cpp:165] Memory required for data: 2184859648
I0409 19:55:52.007827 20605 layer_factory.hpp:77] Creating layer fc4_300
I0409 19:55:52.007833 20605 net.cpp:100] Creating Layer fc4_300
I0409 19:55:52.007839 20605 net.cpp:434] fc4_300 <- pool3
I0409 19:55:52.007844 20605 net.cpp:408] fc4_300 -> fc4_300
I0409 19:55:52.013165 20605 net.cpp:150] Setting up fc4_300
I0409 19:55:52.013181 20605 net.cpp:157] Top shape: 1024 300 (307200)
I0409 19:55:52.013185 20605 net.cpp:165] Memory required for data: 2186088448
I0409 19:55:52.013203 20605 layer_factory.hpp:77] Creating layer fc4_sTanH
I0409 19:55:52.013211 20605 net.cpp:100] Creating Layer fc4_sTanH
I0409 19:55:52.013214 20605 net.cpp:434] fc4_sTanH <- fc4_300
I0409 19:55:52.013219 20605 net.cpp:395] fc4_sTanH -> fc4_300 (in-place)
I0409 19:55:52.013406 20605 net.cpp:150] Setting up fc4_sTanH
I0409 19:55:52.013417 20605 net.cpp:157] Top shape: 1024 300 (307200)
I0409 19:55:52.013422 20605 net.cpp:165] Memory required for data: 2187317248
I0409 19:55:52.013424 20605 layer_factory.hpp:77] Creating layer drop4
I0409 19:55:52.013432 20605 net.cpp:100] Creating Layer drop4
I0409 19:55:52.013435 20605 net.cpp:434] drop4 <- fc4_300
I0409 19:55:52.013440 20605 net.cpp:395] drop4 -> fc4_300 (in-place)
I0409 19:55:52.013469 20605 net.cpp:150] Setting up drop4
I0409 19:55:52.013476 20605 net.cpp:157] Top shape: 1024 300 (307200)
I0409 19:55:52.013479 20605 net.cpp:165] Memory required for data: 2188546048
I0409 19:55:52.013483 20605 layer_factory.hpp:77] Creating layer fc5_67
I0409 19:55:52.013489 20605 net.cpp:100] Creating Layer fc5_67
I0409 19:55:52.013491 20605 net.cpp:434] fc5_67 <- fc4_300
I0409 19:55:52.013496 20605 net.cpp:408] fc5_67 -> fc5_classes
I0409 19:55:52.014731 20605 net.cpp:150] Setting up fc5_67
I0409 19:55:52.014746 20605 net.cpp:157] Top shape: 1024 67 (68608)
I0409 19:55:52.014751 20605 net.cpp:165] Memory required for data: 2188820480
I0409 19:55:52.014773 20605 layer_factory.hpp:77] Creating layer loss
I0409 19:55:52.014781 20605 net.cpp:100] Creating Layer loss
I0409 19:55:52.014786 20605 net.cpp:434] loss <- fc5_classes
I0409 19:55:52.014789 20605 net.cpp:434] loss <- label
I0409 19:55:52.014796 20605 net.cpp:408] loss -> loss
I0409 19:55:52.014808 20605 layer_factory.hpp:77] Creating layer loss
I0409 19:55:52.015133 20605 net.cpp:150] Setting up loss
I0409 19:55:52.015146 20605 net.cpp:157] Top shape: (1)
I0409 19:55:52.015147 20605 net.cpp:160]     with loss weight 1
I0409 19:55:52.015172 20605 net.cpp:165] Memory required for data: 2188820484
I0409 19:55:52.015175 20605 net.cpp:226] loss needs backward computation.
I0409 19:55:52.015183 20605 net.cpp:226] fc5_67 needs backward computation.
I0409 19:55:52.015188 20605 net.cpp:226] drop4 needs backward computation.
I0409 19:55:52.015192 20605 net.cpp:226] fc4_sTanH needs backward computation.
I0409 19:55:52.015194 20605 net.cpp:226] fc4_300 needs backward computation.
I0409 19:55:52.015197 20605 net.cpp:226] pool3 needs backward computation.
I0409 19:55:52.015200 20605 net.cpp:226] conv3_sTanH needs backward computation.
I0409 19:55:52.015203 20605 net.cpp:226] conv3 needs backward computation.
I0409 19:55:52.015206 20605 net.cpp:226] pool2 needs backward computation.
I0409 19:55:52.015210 20605 net.cpp:226] conv2_sTanH needs backward computation.
I0409 19:55:52.015213 20605 net.cpp:226] conv2 needs backward computation.
I0409 19:55:52.015216 20605 net.cpp:226] pool1 needs backward computation.
I0409 19:55:52.015219 20605 net.cpp:226] conv1_sTanH needs backward computation.
I0409 19:55:52.015223 20605 net.cpp:226] conv1 needs backward computation.
I0409 19:55:52.015226 20605 net.cpp:228] data does not need backward computation.
I0409 19:55:52.015229 20605 net.cpp:270] This network produces output loss
I0409 19:55:52.015241 20605 net.cpp:283] Network initialization done.
I0409 19:55:52.015447 20605 solver.cpp:193] Creating test net (#0) specified by test_net file: ./Prototxt/experiment_7/rtsd-r1/CoNorm/trial_1/test.prototxt
I0409 19:55:52.015571 20605 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 48
    mean_value: 133
    mean_value: 133
    mean_value: 132
  }
  data_param {
    source: "../local_data/lmdb/rtsd-r1/CoNorm/test/lmdb"
    batch_size: 1024
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 7
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_sTanH"
  type: "TanH"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 150
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_sTanH"
  type: "TanH"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 250
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_sTanH"
  type: "TanH"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc4_300"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc4_300"
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fc4_sTanH"
  type: "TanH"
  bottom: "fc4_300"
  top: "fc4_300"
}
layer {
  name: "drop4"
  type: "Dropout"
  bottom: "fc4_300"
  top: "fc4_300"
  dropout_param {
    dropout_ratio: 0.4
  }
}
layer {
  name: "fc5_67"
  type: "InnerProduct"
  bottom: "fc4_300"
  top: "fc5_classes"
  inner_product_param {
    num_output: 67
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc5_classes"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy_1"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_1"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 1
  }
}
layer {
  name: "accuracy_5"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0409 19:55:52.015652 20605 layer_factory.hpp:77] Creating layer data
I0409 19:55:52.016255 20605 net.cpp:100] Creating Layer data
I0409 19:55:52.016268 20605 net.cpp:408] data -> data
I0409 19:55:52.016276 20605 net.cpp:408] data -> label
I0409 19:55:52.017452 20702 db_lmdb.cpp:35] Opened lmdb ../local_data/lmdb/rtsd-r1/CoNorm/test/lmdb
I0409 19:55:52.017607 20605 data_layer.cpp:41] output data size: 1024,3,48,48
I0409 19:55:52.063727 20605 net.cpp:150] Setting up data
I0409 19:55:52.063755 20605 net.cpp:157] Top shape: 1024 3 48 48 (7077888)
I0409 19:55:52.063760 20605 net.cpp:157] Top shape: 1024 (1024)
I0409 19:55:52.063762 20605 net.cpp:165] Memory required for data: 28315648
I0409 19:55:52.063768 20605 layer_factory.hpp:77] Creating layer label_data_1_split
I0409 19:55:52.063792 20605 net.cpp:100] Creating Layer label_data_1_split
I0409 19:55:52.063796 20605 net.cpp:434] label_data_1_split <- label
I0409 19:55:52.063804 20605 net.cpp:408] label_data_1_split -> label_data_1_split_0
I0409 19:55:52.063814 20605 net.cpp:408] label_data_1_split -> label_data_1_split_1
I0409 19:55:52.063822 20605 net.cpp:408] label_data_1_split -> label_data_1_split_2
I0409 19:55:52.063994 20605 net.cpp:150] Setting up label_data_1_split
I0409 19:55:52.064005 20605 net.cpp:157] Top shape: 1024 (1024)
I0409 19:55:52.064009 20605 net.cpp:157] Top shape: 1024 (1024)
I0409 19:55:52.064013 20605 net.cpp:157] Top shape: 1024 (1024)
I0409 19:55:52.064015 20605 net.cpp:165] Memory required for data: 28327936
I0409 19:55:52.064019 20605 layer_factory.hpp:77] Creating layer conv1
I0409 19:55:52.064031 20605 net.cpp:100] Creating Layer conv1
I0409 19:55:52.064035 20605 net.cpp:434] conv1 <- data
I0409 19:55:52.064041 20605 net.cpp:408] conv1 -> conv1
I0409 19:55:52.072418 20605 net.cpp:150] Setting up conv1
I0409 19:55:52.072435 20605 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0409 19:55:52.072439 20605 net.cpp:165] Memory required for data: 750862336
I0409 19:55:52.072453 20605 layer_factory.hpp:77] Creating layer conv1_sTanH
I0409 19:55:52.072459 20605 net.cpp:100] Creating Layer conv1_sTanH
I0409 19:55:52.072463 20605 net.cpp:434] conv1_sTanH <- conv1
I0409 19:55:52.072468 20605 net.cpp:395] conv1_sTanH -> conv1 (in-place)
I0409 19:55:52.072644 20605 net.cpp:150] Setting up conv1_sTanH
I0409 19:55:52.072656 20605 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0409 19:55:52.072659 20605 net.cpp:165] Memory required for data: 1473396736
I0409 19:55:52.072664 20605 layer_factory.hpp:77] Creating layer pool1
I0409 19:55:52.072671 20605 net.cpp:100] Creating Layer pool1
I0409 19:55:52.072676 20605 net.cpp:434] pool1 <- conv1
I0409 19:55:52.072682 20605 net.cpp:408] pool1 -> pool1
I0409 19:55:52.072727 20605 net.cpp:150] Setting up pool1
I0409 19:55:52.072736 20605 net.cpp:157] Top shape: 1024 100 21 21 (45158400)
I0409 19:55:52.072738 20605 net.cpp:165] Memory required for data: 1654030336
I0409 19:55:52.072741 20605 layer_factory.hpp:77] Creating layer conv2
I0409 19:55:52.072751 20605 net.cpp:100] Creating Layer conv2
I0409 19:55:52.072755 20605 net.cpp:434] conv2 <- pool1
I0409 19:55:52.072760 20605 net.cpp:408] conv2 -> conv2
I0409 19:55:52.084988 20605 net.cpp:150] Setting up conv2
I0409 19:55:52.085008 20605 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0409 19:55:52.085012 20605 net.cpp:165] Memory required for data: 1853095936
I0409 19:55:52.085022 20605 layer_factory.hpp:77] Creating layer conv2_sTanH
I0409 19:55:52.085031 20605 net.cpp:100] Creating Layer conv2_sTanH
I0409 19:55:52.085058 20605 net.cpp:434] conv2_sTanH <- conv2
I0409 19:55:52.085063 20605 net.cpp:395] conv2_sTanH -> conv2 (in-place)
I0409 19:55:52.085825 20605 net.cpp:150] Setting up conv2_sTanH
I0409 19:55:52.085844 20605 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0409 19:55:52.085849 20605 net.cpp:165] Memory required for data: 2052161536
I0409 19:55:52.085852 20605 layer_factory.hpp:77] Creating layer pool2
I0409 19:55:52.085860 20605 net.cpp:100] Creating Layer pool2
I0409 19:55:52.085862 20605 net.cpp:434] pool2 <- conv2
I0409 19:55:52.085867 20605 net.cpp:408] pool2 -> pool2
I0409 19:55:52.085917 20605 net.cpp:150] Setting up pool2
I0409 19:55:52.085928 20605 net.cpp:157] Top shape: 1024 150 9 9 (12441600)
I0409 19:55:52.085932 20605 net.cpp:165] Memory required for data: 2101927936
I0409 19:55:52.085934 20605 layer_factory.hpp:77] Creating layer conv3
I0409 19:55:52.085945 20605 net.cpp:100] Creating Layer conv3
I0409 19:55:52.085949 20605 net.cpp:434] conv3 <- pool2
I0409 19:55:52.085954 20605 net.cpp:408] conv3 -> conv3
I0409 19:55:52.091545 20605 net.cpp:150] Setting up conv3
I0409 19:55:52.091564 20605 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0409 19:55:52.091568 20605 net.cpp:165] Memory required for data: 2138791936
I0409 19:55:52.091578 20605 layer_factory.hpp:77] Creating layer conv3_sTanH
I0409 19:55:52.091585 20605 net.cpp:100] Creating Layer conv3_sTanH
I0409 19:55:52.091589 20605 net.cpp:434] conv3_sTanH <- conv3
I0409 19:55:52.091594 20605 net.cpp:395] conv3_sTanH -> conv3 (in-place)
I0409 19:55:52.093101 20605 net.cpp:150] Setting up conv3_sTanH
I0409 19:55:52.093120 20605 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0409 19:55:52.093143 20605 net.cpp:165] Memory required for data: 2175655936
I0409 19:55:52.093147 20605 layer_factory.hpp:77] Creating layer pool3
I0409 19:55:52.093155 20605 net.cpp:100] Creating Layer pool3
I0409 19:55:52.093159 20605 net.cpp:434] pool3 <- conv3
I0409 19:55:52.093165 20605 net.cpp:408] pool3 -> pool3
I0409 19:55:52.093215 20605 net.cpp:150] Setting up pool3
I0409 19:55:52.093225 20605 net.cpp:157] Top shape: 1024 250 3 3 (2304000)
I0409 19:55:52.093227 20605 net.cpp:165] Memory required for data: 2184871936
I0409 19:55:52.093230 20605 layer_factory.hpp:77] Creating layer fc4_300
I0409 19:55:52.093237 20605 net.cpp:100] Creating Layer fc4_300
I0409 19:55:52.093241 20605 net.cpp:434] fc4_300 <- pool3
I0409 19:55:52.093246 20605 net.cpp:408] fc4_300 -> fc4_300
I0409 19:55:52.098778 20605 net.cpp:150] Setting up fc4_300
I0409 19:55:52.098796 20605 net.cpp:157] Top shape: 1024 300 (307200)
I0409 19:55:52.098799 20605 net.cpp:165] Memory required for data: 2186100736
I0409 19:55:52.098806 20605 layer_factory.hpp:77] Creating layer fc4_sTanH
I0409 19:55:52.098812 20605 net.cpp:100] Creating Layer fc4_sTanH
I0409 19:55:52.098816 20605 net.cpp:434] fc4_sTanH <- fc4_300
I0409 19:55:52.098822 20605 net.cpp:395] fc4_sTanH -> fc4_300 (in-place)
I0409 19:55:52.099012 20605 net.cpp:150] Setting up fc4_sTanH
I0409 19:55:52.099023 20605 net.cpp:157] Top shape: 1024 300 (307200)
I0409 19:55:52.099026 20605 net.cpp:165] Memory required for data: 2187329536
I0409 19:55:52.099030 20605 layer_factory.hpp:77] Creating layer drop4
I0409 19:55:52.099036 20605 net.cpp:100] Creating Layer drop4
I0409 19:55:52.099040 20605 net.cpp:434] drop4 <- fc4_300
I0409 19:55:52.099045 20605 net.cpp:395] drop4 -> fc4_300 (in-place)
I0409 19:55:52.099074 20605 net.cpp:150] Setting up drop4
I0409 19:55:52.099081 20605 net.cpp:157] Top shape: 1024 300 (307200)
I0409 19:55:52.099086 20605 net.cpp:165] Memory required for data: 2188558336
I0409 19:55:52.099103 20605 layer_factory.hpp:77] Creating layer fc5_67
I0409 19:55:52.099110 20605 net.cpp:100] Creating Layer fc5_67
I0409 19:55:52.099113 20605 net.cpp:434] fc5_67 <- fc4_300
I0409 19:55:52.099118 20605 net.cpp:408] fc5_67 -> fc5_classes
I0409 19:55:52.099367 20605 net.cpp:150] Setting up fc5_67
I0409 19:55:52.099375 20605 net.cpp:157] Top shape: 1024 67 (68608)
I0409 19:55:52.099378 20605 net.cpp:165] Memory required for data: 2188832768
I0409 19:55:52.099388 20605 layer_factory.hpp:77] Creating layer fc5_classes_fc5_67_0_split
I0409 19:55:52.099395 20605 net.cpp:100] Creating Layer fc5_classes_fc5_67_0_split
I0409 19:55:52.099398 20605 net.cpp:434] fc5_classes_fc5_67_0_split <- fc5_classes
I0409 19:55:52.099403 20605 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_0
I0409 19:55:52.099414 20605 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_1
I0409 19:55:52.099421 20605 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_2
I0409 19:55:52.099468 20605 net.cpp:150] Setting up fc5_classes_fc5_67_0_split
I0409 19:55:52.099474 20605 net.cpp:157] Top shape: 1024 67 (68608)
I0409 19:55:52.099478 20605 net.cpp:157] Top shape: 1024 67 (68608)
I0409 19:55:52.099481 20605 net.cpp:157] Top shape: 1024 67 (68608)
I0409 19:55:52.099483 20605 net.cpp:165] Memory required for data: 2189656064
I0409 19:55:52.099486 20605 layer_factory.hpp:77] Creating layer loss
I0409 19:55:52.099494 20605 net.cpp:100] Creating Layer loss
I0409 19:55:52.099498 20605 net.cpp:434] loss <- fc5_classes_fc5_67_0_split_0
I0409 19:55:52.099514 20605 net.cpp:434] loss <- label_data_1_split_0
I0409 19:55:52.099519 20605 net.cpp:408] loss -> loss
I0409 19:55:52.099530 20605 layer_factory.hpp:77] Creating layer loss
I0409 19:55:52.099850 20605 net.cpp:150] Setting up loss
I0409 19:55:52.099861 20605 net.cpp:157] Top shape: (1)
I0409 19:55:52.099864 20605 net.cpp:160]     with loss weight 1
I0409 19:55:52.099875 20605 net.cpp:165] Memory required for data: 2189656068
I0409 19:55:52.099894 20605 layer_factory.hpp:77] Creating layer accuracy_1
I0409 19:55:52.099902 20605 net.cpp:100] Creating Layer accuracy_1
I0409 19:55:52.099920 20605 net.cpp:434] accuracy_1 <- fc5_classes_fc5_67_0_split_1
I0409 19:55:52.099926 20605 net.cpp:434] accuracy_1 <- label_data_1_split_1
I0409 19:55:52.099932 20605 net.cpp:408] accuracy_1 -> accuracy_1
I0409 19:55:52.099941 20605 net.cpp:150] Setting up accuracy_1
I0409 19:55:52.099948 20605 net.cpp:157] Top shape: (1)
I0409 19:55:52.099951 20605 net.cpp:165] Memory required for data: 2189656072
I0409 19:55:52.099954 20605 layer_factory.hpp:77] Creating layer accuracy_5
I0409 19:55:52.099959 20605 net.cpp:100] Creating Layer accuracy_5
I0409 19:55:52.099962 20605 net.cpp:434] accuracy_5 <- fc5_classes_fc5_67_0_split_2
I0409 19:55:52.099967 20605 net.cpp:434] accuracy_5 <- label_data_1_split_2
I0409 19:55:52.099972 20605 net.cpp:408] accuracy_5 -> accuracy_5
I0409 19:55:52.099977 20605 net.cpp:150] Setting up accuracy_5
I0409 19:55:52.099980 20605 net.cpp:157] Top shape: (1)
I0409 19:55:52.099983 20605 net.cpp:165] Memory required for data: 2189656076
I0409 19:55:52.099987 20605 net.cpp:228] accuracy_5 does not need backward computation.
I0409 19:55:52.099989 20605 net.cpp:228] accuracy_1 does not need backward computation.
I0409 19:55:52.099993 20605 net.cpp:226] loss needs backward computation.
I0409 19:55:52.099997 20605 net.cpp:226] fc5_classes_fc5_67_0_split needs backward computation.
I0409 19:55:52.100000 20605 net.cpp:226] fc5_67 needs backward computation.
I0409 19:55:52.100003 20605 net.cpp:226] drop4 needs backward computation.
I0409 19:55:52.100010 20605 net.cpp:226] fc4_sTanH needs backward computation.
I0409 19:55:52.100019 20605 net.cpp:226] fc4_300 needs backward computation.
I0409 19:55:52.100023 20605 net.cpp:226] pool3 needs backward computation.
I0409 19:55:52.100025 20605 net.cpp:226] conv3_sTanH needs backward computation.
I0409 19:55:52.100028 20605 net.cpp:226] conv3 needs backward computation.
I0409 19:55:52.100031 20605 net.cpp:226] pool2 needs backward computation.
I0409 19:55:52.100034 20605 net.cpp:226] conv2_sTanH needs backward computation.
I0409 19:55:52.100038 20605 net.cpp:226] conv2 needs backward computation.
I0409 19:55:52.100041 20605 net.cpp:226] pool1 needs backward computation.
I0409 19:55:52.100044 20605 net.cpp:226] conv1_sTanH needs backward computation.
I0409 19:55:52.100046 20605 net.cpp:226] conv1 needs backward computation.
I0409 19:55:52.100050 20605 net.cpp:228] label_data_1_split does not need backward computation.
I0409 19:55:52.100059 20605 net.cpp:228] data does not need backward computation.
I0409 19:55:52.100062 20605 net.cpp:270] This network produces output accuracy_1
I0409 19:55:52.100065 20605 net.cpp:270] This network produces output accuracy_5
I0409 19:55:52.100069 20605 net.cpp:270] This network produces output loss
I0409 19:55:52.100085 20605 net.cpp:283] Network initialization done.
I0409 19:55:52.100138 20605 solver.cpp:72] Solver scaffolding done.
I0409 19:55:52.100634 20605 caffe.cpp:251] Starting Optimization
I0409 19:55:52.100641 20605 solver.cpp:291] Solving 
I0409 19:55:52.100644 20605 solver.cpp:292] Learning Rate Policy: step
I0409 19:55:52.104640 20605 solver.cpp:349] Iteration 0, Testing net (#0)
I0409 19:55:52.106580 20605 blocking_queue.cpp:50] Data layer prefetch queue empty
I0409 19:55:52.893901 20605 solver.cpp:416]     Test net output #0: accuracy_1 = 0.00537109
I0409 19:55:52.893932 20605 solver.cpp:416]     Test net output #1: accuracy_5 = 0.0723877
I0409 19:55:52.893942 20605 solver.cpp:416]     Test net output #2: loss = 4.30954 (* 1 = 4.30954 loss)
I0409 19:55:53.005117 20605 solver.cpp:240] Iteration 0, loss = 4.40836
I0409 19:55:53.005151 20605 solver.cpp:256]     Train net output #0: loss = 4.40836 (* 1 = 4.40836 loss)
I0409 19:55:53.005165 20605 sgd_solver.cpp:106] Iteration 0, lr = 5e-05
I0409 19:55:53.318769 20605 solver.cpp:240] Iteration 1, loss = 4.25956
I0409 19:55:53.318814 20605 solver.cpp:256]     Train net output #0: loss = 4.25956 (* 1 = 4.25956 loss)
I0409 19:55:53.318821 20605 sgd_solver.cpp:106] Iteration 1, lr = 5e-05
I0409 19:55:53.632944 20605 solver.cpp:240] Iteration 2, loss = 4.1611
I0409 19:55:53.632994 20605 solver.cpp:256]     Train net output #0: loss = 4.1611 (* 1 = 4.1611 loss)
I0409 19:55:53.633003 20605 sgd_solver.cpp:106] Iteration 2, lr = 5e-05
I0409 19:55:53.949832 20605 solver.cpp:240] Iteration 3, loss = 4.0652
I0409 19:55:53.949864 20605 solver.cpp:256]     Train net output #0: loss = 4.0652 (* 1 = 4.0652 loss)
I0409 19:55:53.949872 20605 sgd_solver.cpp:106] Iteration 3, lr = 5e-05
I0409 19:55:54.264176 20605 solver.cpp:240] Iteration 4, loss = 3.9712
I0409 19:55:54.264209 20605 solver.cpp:256]     Train net output #0: loss = 3.9712 (* 1 = 3.9712 loss)
I0409 19:55:54.264216 20605 sgd_solver.cpp:106] Iteration 4, lr = 5e-05
I0409 19:55:54.581462 20605 solver.cpp:240] Iteration 5, loss = 3.97329
I0409 19:55:54.581506 20605 solver.cpp:256]     Train net output #0: loss = 3.97329 (* 1 = 3.97329 loss)
I0409 19:55:54.581513 20605 sgd_solver.cpp:106] Iteration 5, lr = 5e-05
I0409 19:55:54.899621 20605 solver.cpp:240] Iteration 6, loss = 3.8855
I0409 19:55:54.899664 20605 solver.cpp:256]     Train net output #0: loss = 3.8855 (* 1 = 3.8855 loss)
I0409 19:55:54.899672 20605 sgd_solver.cpp:106] Iteration 6, lr = 5e-05
I0409 19:55:55.220002 20605 solver.cpp:240] Iteration 7, loss = 3.89538
I0409 19:55:55.220036 20605 solver.cpp:256]     Train net output #0: loss = 3.89538 (* 1 = 3.89538 loss)
I0409 19:55:55.220043 20605 sgd_solver.cpp:106] Iteration 7, lr = 5e-05
I0409 19:55:55.540647 20605 solver.cpp:240] Iteration 8, loss = 3.86729
I0409 19:55:55.540680 20605 solver.cpp:256]     Train net output #0: loss = 3.86729 (* 1 = 3.86729 loss)
I0409 19:55:55.540688 20605 sgd_solver.cpp:106] Iteration 8, lr = 5e-05
I0409 19:55:55.852116 20605 solver.cpp:240] Iteration 9, loss = 3.84208
I0409 19:55:55.852149 20605 solver.cpp:256]     Train net output #0: loss = 3.84208 (* 1 = 3.84208 loss)
I0409 19:55:55.852157 20605 sgd_solver.cpp:106] Iteration 9, lr = 5e-05
I0409 19:55:56.167294 20605 solver.cpp:240] Iteration 10, loss = 3.83211
I0409 19:55:56.167326 20605 solver.cpp:256]     Train net output #0: loss = 3.83211 (* 1 = 3.83211 loss)
I0409 19:55:56.167335 20605 sgd_solver.cpp:106] Iteration 10, lr = 5e-05
I0409 19:55:56.481526 20605 solver.cpp:240] Iteration 11, loss = 3.80976
I0409 19:55:56.481559 20605 solver.cpp:256]     Train net output #0: loss = 3.80976 (* 1 = 3.80976 loss)
I0409 19:55:56.481565 20605 sgd_solver.cpp:106] Iteration 11, lr = 5e-05
I0409 19:55:56.797569 20605 solver.cpp:240] Iteration 12, loss = 3.76721
I0409 19:55:56.797600 20605 solver.cpp:256]     Train net output #0: loss = 3.76721 (* 1 = 3.76721 loss)
I0409 19:55:56.797607 20605 sgd_solver.cpp:106] Iteration 12, lr = 5e-05
I0409 19:55:57.111568 20605 solver.cpp:240] Iteration 13, loss = 3.79768
I0409 19:55:57.111598 20605 solver.cpp:256]     Train net output #0: loss = 3.79768 (* 1 = 3.79768 loss)
I0409 19:55:57.111618 20605 sgd_solver.cpp:106] Iteration 13, lr = 5e-05
I0409 19:55:57.428586 20605 solver.cpp:240] Iteration 14, loss = 3.85973
I0409 19:55:57.428632 20605 solver.cpp:256]     Train net output #0: loss = 3.85973 (* 1 = 3.85973 loss)
I0409 19:55:57.428639 20605 sgd_solver.cpp:106] Iteration 14, lr = 5e-05
I0409 19:55:57.746549 20605 solver.cpp:240] Iteration 15, loss = 3.80121
I0409 19:55:57.746579 20605 solver.cpp:256]     Train net output #0: loss = 3.80121 (* 1 = 3.80121 loss)
I0409 19:55:57.746587 20605 sgd_solver.cpp:106] Iteration 15, lr = 5e-05
I0409 19:55:58.067172 20605 solver.cpp:240] Iteration 16, loss = 3.84598
I0409 19:55:58.067201 20605 solver.cpp:256]     Train net output #0: loss = 3.84598 (* 1 = 3.84598 loss)
I0409 19:55:58.067209 20605 sgd_solver.cpp:106] Iteration 16, lr = 5e-05
I0409 19:55:58.388072 20605 solver.cpp:240] Iteration 17, loss = 3.77056
I0409 19:55:58.388103 20605 solver.cpp:256]     Train net output #0: loss = 3.77056 (* 1 = 3.77056 loss)
I0409 19:55:58.388111 20605 sgd_solver.cpp:106] Iteration 17, lr = 5e-05
I0409 19:55:58.698698 20605 solver.cpp:240] Iteration 18, loss = 3.77672
I0409 19:55:58.698729 20605 solver.cpp:256]     Train net output #0: loss = 3.77672 (* 1 = 3.77672 loss)
I0409 19:55:58.698757 20605 sgd_solver.cpp:106] Iteration 18, lr = 5e-05
I0409 19:55:59.012941 20605 solver.cpp:240] Iteration 19, loss = 3.71416
I0409 19:55:59.012971 20605 solver.cpp:256]     Train net output #0: loss = 3.71416 (* 1 = 3.71416 loss)
I0409 19:55:59.012979 20605 sgd_solver.cpp:106] Iteration 19, lr = 5e-05
I0409 19:55:59.328953 20605 solver.cpp:240] Iteration 20, loss = 3.79719
I0409 19:55:59.328984 20605 solver.cpp:256]     Train net output #0: loss = 3.79719 (* 1 = 3.79719 loss)
I0409 19:55:59.328991 20605 sgd_solver.cpp:106] Iteration 20, lr = 5e-05
I0409 19:55:59.643743 20605 solver.cpp:240] Iteration 21, loss = 3.72308
I0409 19:55:59.643775 20605 solver.cpp:256]     Train net output #0: loss = 3.72308 (* 1 = 3.72308 loss)
I0409 19:55:59.643784 20605 sgd_solver.cpp:106] Iteration 21, lr = 5e-05
I0409 19:55:59.959321 20605 solver.cpp:240] Iteration 22, loss = 3.8512
I0409 19:55:59.959350 20605 solver.cpp:256]     Train net output #0: loss = 3.8512 (* 1 = 3.8512 loss)
I0409 19:55:59.959357 20605 sgd_solver.cpp:106] Iteration 22, lr = 5e-05
I0409 19:56:00.276796 20605 solver.cpp:240] Iteration 23, loss = 3.76036
I0409 19:56:00.276829 20605 solver.cpp:256]     Train net output #0: loss = 3.76036 (* 1 = 3.76036 loss)
I0409 19:56:00.276835 20605 sgd_solver.cpp:106] Iteration 23, lr = 5e-05
I0409 19:56:00.595209 20605 solver.cpp:240] Iteration 24, loss = 3.78438
I0409 19:56:00.595243 20605 solver.cpp:256]     Train net output #0: loss = 3.78438 (* 1 = 3.78438 loss)
I0409 19:56:00.595252 20605 sgd_solver.cpp:106] Iteration 24, lr = 5e-05
I0409 19:56:00.595465 20605 solver.cpp:349] Iteration 25, Testing net (#0)
I0409 19:56:01.567077 20605 solver.cpp:416]     Test net output #0: accuracy_1 = 0.135132
I0409 19:56:01.567106 20605 solver.cpp:416]     Test net output #1: accuracy_5 = 0.3125
I0409 19:56:01.567127 20605 solver.cpp:416]     Test net output #2: loss = 3.69565 (* 1 = 3.69565 loss)
I0409 19:56:01.656235 20605 solver.cpp:240] Iteration 25, loss = 3.79132
I0409 19:56:01.656266 20605 solver.cpp:256]     Train net output #0: loss = 3.79132 (* 1 = 3.79132 loss)
I0409 19:56:01.656275 20605 sgd_solver.cpp:106] Iteration 25, lr = 5e-05
I0409 19:56:01.971423 20605 solver.cpp:240] Iteration 26, loss = 3.77823
I0409 19:56:01.971467 20605 solver.cpp:256]     Train net output #0: loss = 3.77823 (* 1 = 3.77823 loss)
I0409 19:56:01.971474 20605 sgd_solver.cpp:106] Iteration 26, lr = 5e-05
I0409 19:56:02.285703 20605 solver.cpp:240] Iteration 27, loss = 3.7744
I0409 19:56:02.285729 20605 solver.cpp:256]     Train net output #0: loss = 3.7744 (* 1 = 3.7744 loss)
I0409 19:56:02.285737 20605 sgd_solver.cpp:106] Iteration 27, lr = 5e-05
I0409 19:56:02.603606 20605 solver.cpp:240] Iteration 28, loss = 3.81097
I0409 19:56:02.603638 20605 solver.cpp:256]     Train net output #0: loss = 3.81097 (* 1 = 3.81097 loss)
I0409 19:56:02.603646 20605 sgd_solver.cpp:106] Iteration 28, lr = 5e-05
I0409 19:56:02.921603 20605 solver.cpp:240] Iteration 29, loss = 3.7411
I0409 19:56:02.921633 20605 solver.cpp:256]     Train net output #0: loss = 3.7411 (* 1 = 3.7411 loss)
I0409 19:56:02.921641 20605 sgd_solver.cpp:106] Iteration 29, lr = 5e-05
I0409 19:56:03.242925 20605 solver.cpp:240] Iteration 30, loss = 3.78268
I0409 19:56:03.242956 20605 solver.cpp:256]     Train net output #0: loss = 3.78268 (* 1 = 3.78268 loss)
I0409 19:56:03.242964 20605 sgd_solver.cpp:106] Iteration 30, lr = 5e-05
I0409 19:56:03.551877 20605 solver.cpp:240] Iteration 31, loss = 3.73972
I0409 19:56:03.551929 20605 solver.cpp:256]     Train net output #0: loss = 3.73972 (* 1 = 3.73972 loss)
I0409 19:56:03.551936 20605 sgd_solver.cpp:106] Iteration 31, lr = 5e-05
I0409 19:56:03.865217 20605 solver.cpp:240] Iteration 32, loss = 3.78802
I0409 19:56:03.865259 20605 solver.cpp:256]     Train net output #0: loss = 3.78802 (* 1 = 3.78802 loss)
I0409 19:56:03.865267 20605 sgd_solver.cpp:106] Iteration 32, lr = 5e-05
I0409 19:56:04.180133 20605 solver.cpp:240] Iteration 33, loss = 3.75798
I0409 19:56:04.180164 20605 solver.cpp:256]     Train net output #0: loss = 3.75798 (* 1 = 3.75798 loss)
I0409 19:56:04.180203 20605 sgd_solver.cpp:106] Iteration 33, lr = 5e-05
I0409 19:56:04.495192 20605 solver.cpp:240] Iteration 34, loss = 3.77417
I0409 19:56:04.495220 20605 solver.cpp:256]     Train net output #0: loss = 3.77417 (* 1 = 3.77417 loss)
I0409 19:56:04.495231 20605 sgd_solver.cpp:106] Iteration 34, lr = 5e-05
I0409 19:56:04.809052 20605 solver.cpp:240] Iteration 35, loss = 3.74537
I0409 19:56:04.809083 20605 solver.cpp:256]     Train net output #0: loss = 3.74537 (* 1 = 3.74537 loss)
I0409 19:56:04.809092 20605 sgd_solver.cpp:106] Iteration 35, lr = 5e-05
I0409 19:56:05.126126 20605 solver.cpp:240] Iteration 36, loss = 3.77933
I0409 19:56:05.126155 20605 solver.cpp:256]     Train net output #0: loss = 3.77933 (* 1 = 3.77933 loss)
I0409 19:56:05.126163 20605 sgd_solver.cpp:106] Iteration 36, lr = 5e-05
I0409 19:56:05.444128 20605 solver.cpp:240] Iteration 37, loss = 3.76987
I0409 19:56:05.444159 20605 solver.cpp:256]     Train net output #0: loss = 3.76987 (* 1 = 3.76987 loss)
I0409 19:56:05.444166 20605 sgd_solver.cpp:106] Iteration 37, lr = 5e-05
I0409 19:56:05.764199 20605 solver.cpp:240] Iteration 38, loss = 3.79615
I0409 19:56:05.764233 20605 solver.cpp:256]     Train net output #0: loss = 3.79615 (* 1 = 3.79615 loss)
I0409 19:56:05.764241 20605 sgd_solver.cpp:106] Iteration 38, lr = 5e-05
I0409 19:56:06.085255 20605 solver.cpp:240] Iteration 39, loss = 3.82482
I0409 19:56:06.085297 20605 solver.cpp:256]     Train net output #0: loss = 3.82482 (* 1 = 3.82482 loss)
I0409 19:56:06.085305 20605 sgd_solver.cpp:106] Iteration 39, lr = 5e-05
I0409 19:56:06.393895 20605 solver.cpp:240] Iteration 40, loss = 3.74718
I0409 19:56:06.393939 20605 solver.cpp:256]     Train net output #0: loss = 3.74718 (* 1 = 3.74718 loss)
I0409 19:56:06.393945 20605 sgd_solver.cpp:106] Iteration 40, lr = 5e-05
I0409 19:56:06.711966 20605 solver.cpp:240] Iteration 41, loss = 3.75582
I0409 19:56:06.711997 20605 solver.cpp:256]     Train net output #0: loss = 3.75582 (* 1 = 3.75582 loss)
I0409 19:56:06.712005 20605 sgd_solver.cpp:106] Iteration 41, lr = 5e-05
I0409 19:56:07.026456 20605 solver.cpp:240] Iteration 42, loss = 3.71636
I0409 19:56:07.026499 20605 solver.cpp:256]     Train net output #0: loss = 3.71636 (* 1 = 3.71636 loss)
I0409 19:56:07.026506 20605 sgd_solver.cpp:106] Iteration 42, lr = 5e-05
I0409 19:56:07.340710 20605 solver.cpp:240] Iteration 43, loss = 3.68307
I0409 19:56:07.340740 20605 solver.cpp:256]     Train net output #0: loss = 3.68307 (* 1 = 3.68307 loss)
I0409 19:56:07.340747 20605 sgd_solver.cpp:106] Iteration 43, lr = 5e-05
I0409 19:56:07.655308 20605 solver.cpp:240] Iteration 44, loss = 3.72807
I0409 19:56:07.655347 20605 solver.cpp:256]     Train net output #0: loss = 3.72807 (* 1 = 3.72807 loss)
I0409 19:56:07.655354 20605 sgd_solver.cpp:106] Iteration 44, lr = 5e-05
I0409 19:56:07.972357 20605 solver.cpp:240] Iteration 45, loss = 3.72877
I0409 19:56:07.972386 20605 solver.cpp:256]     Train net output #0: loss = 3.72877 (* 1 = 3.72877 loss)
I0409 19:56:07.972393 20605 sgd_solver.cpp:106] Iteration 45, lr = 5e-05
I0409 19:56:08.290974 20605 solver.cpp:240] Iteration 46, loss = 3.72409
I0409 19:56:08.291007 20605 solver.cpp:256]     Train net output #0: loss = 3.72409 (* 1 = 3.72409 loss)
I0409 19:56:08.291014 20605 sgd_solver.cpp:106] Iteration 46, lr = 5e-05
I0409 19:56:08.610510 20605 solver.cpp:240] Iteration 47, loss = 3.81077
I0409 19:56:08.610548 20605 solver.cpp:256]     Train net output #0: loss = 3.81077 (* 1 = 3.81077 loss)
I0409 19:56:08.610566 20605 sgd_solver.cpp:106] Iteration 47, lr = 5e-05
I0409 19:56:08.931766 20605 solver.cpp:240] Iteration 48, loss = 3.78741
I0409 19:56:08.931807 20605 solver.cpp:256]     Train net output #0: loss = 3.78741 (* 1 = 3.78741 loss)
I0409 19:56:08.931815 20605 sgd_solver.cpp:106] Iteration 48, lr = 5e-05
I0409 19:56:09.240983 20605 solver.cpp:240] Iteration 49, loss = 3.7857
I0409 19:56:09.241013 20605 solver.cpp:256]     Train net output #0: loss = 3.7857 (* 1 = 3.7857 loss)
I0409 19:56:09.241020 20605 sgd_solver.cpp:106] Iteration 49, lr = 5e-05
I0409 19:56:09.241247 20605 solver.cpp:349] Iteration 50, Testing net (#0)
I0409 19:56:10.213250 20605 solver.cpp:416]     Test net output #0: accuracy_1 = 0.133789
I0409 19:56:10.213277 20605 solver.cpp:416]     Test net output #1: accuracy_5 = 0.309204
I0409 19:56:10.213286 20605 solver.cpp:416]     Test net output #2: loss = 3.67699 (* 1 = 3.67699 loss)
I0409 19:56:10.303097 20605 solver.cpp:240] Iteration 50, loss = 3.72939
I0409 19:56:10.303139 20605 solver.cpp:256]     Train net output #0: loss = 3.72939 (* 1 = 3.72939 loss)
I0409 19:56:10.303148 20605 sgd_solver.cpp:106] Iteration 50, lr = 5e-05
I0409 19:56:10.624353 20605 solver.cpp:240] Iteration 51, loss = 3.72399
I0409 19:56:10.624397 20605 solver.cpp:256]     Train net output #0: loss = 3.72399 (* 1 = 3.72399 loss)
I0409 19:56:10.624404 20605 sgd_solver.cpp:106] Iteration 51, lr = 5e-05
I0409 19:56:10.946882 20605 solver.cpp:240] Iteration 52, loss = 3.71865
I0409 19:56:10.946923 20605 solver.cpp:256]     Train net output #0: loss = 3.71865 (* 1 = 3.71865 loss)
I0409 19:56:10.946930 20605 sgd_solver.cpp:106] Iteration 52, lr = 5e-05
I0409 19:56:11.267715 20605 solver.cpp:240] Iteration 53, loss = 3.71467
I0409 19:56:11.267748 20605 solver.cpp:256]     Train net output #0: loss = 3.71467 (* 1 = 3.71467 loss)
I0409 19:56:11.267755 20605 sgd_solver.cpp:106] Iteration 53, lr = 5e-05
I0409 19:56:11.591594 20605 solver.cpp:240] Iteration 54, loss = 3.7068
I0409 19:56:11.591627 20605 solver.cpp:256]     Train net output #0: loss = 3.7068 (* 1 = 3.7068 loss)
I0409 19:56:11.591635 20605 sgd_solver.cpp:106] Iteration 54, lr = 5e-05
I0409 19:56:11.910414 20605 solver.cpp:240] Iteration 55, loss = 3.74778
I0409 19:56:11.910446 20605 solver.cpp:256]     Train net output #0: loss = 3.74778 (* 1 = 3.74778 loss)
I0409 19:56:11.910454 20605 sgd_solver.cpp:106] Iteration 55, lr = 5e-05
I0409 19:56:12.227149 20605 solver.cpp:240] Iteration 56, loss = 3.71397
I0409 19:56:12.227179 20605 solver.cpp:256]     Train net output #0: loss = 3.71397 (* 1 = 3.71397 loss)
I0409 19:56:12.227186 20605 sgd_solver.cpp:106] Iteration 56, lr = 5e-05
I0409 19:56:12.545125 20605 solver.cpp:240] Iteration 57, loss = 3.7054
I0409 19:56:12.545156 20605 solver.cpp:256]     Train net output #0: loss = 3.7054 (* 1 = 3.7054 loss)
I0409 19:56:12.545163 20605 sgd_solver.cpp:106] Iteration 57, lr = 5e-05
I0409 19:56:12.863462 20605 solver.cpp:240] Iteration 58, loss = 3.72351
I0409 19:56:12.863494 20605 solver.cpp:256]     Train net output #0: loss = 3.72351 (* 1 = 3.72351 loss)
I0409 19:56:12.863502 20605 sgd_solver.cpp:106] Iteration 58, lr = 5e-05
