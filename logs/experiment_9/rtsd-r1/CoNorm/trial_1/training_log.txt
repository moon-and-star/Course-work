I0411 13:20:14.661103 10031 caffe.cpp:217] Using GPUs 1
I0411 13:20:14.981761 10031 caffe.cpp:222] GPU 1: GeForce GTX 1070
I0411 13:20:15.880213 10031 solver.cpp:60] Initializing solver from parameters: 
train_net: "./Prototxt/experiment_9/rtsd-r1/CoNorm/trial_1/train.prototxt"
test_net: "./Prototxt/experiment_9/rtsd-r1/CoNorm/trial_1/test.prototxt"
test_iter: 8
test_interval: 25
base_lr: 1e-06
display: 1
max_iter: 2500
lr_policy: "step"
gamma: 0.5
momentum: 0.9
weight_decay: 0.0005
stepsize: 500
snapshot: 250
snapshot_prefix: "./snapshots/experiment_9/rtsd-r1/CoNorm/trial_1/snap"
solver_mode: GPU
device_id: 1
train_state {
  level: 0
  stage: ""
}
iter_size: 1
type: "Adam"
I0411 13:20:15.880357 10031 solver.cpp:93] Creating training net from train_net file: ./Prototxt/experiment_9/rtsd-r1/CoNorm/trial_1/train.prototxt
I0411 13:20:15.880659 10031 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_1
I0411 13:20:15.880671 10031 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_5
I0411 13:20:15.880821 10031 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.0039215689
    mirror: false
    crop_size: 48
    mean_value: 132
    mean_value: 132
    mean_value: 131
  }
  data_param {
    source: "../local_data/lmdb/rtsd-r1/CoNorm/train/lmdb"
    batch_size: 1024
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 7
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_prescale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv1_sTanH"
  type: "TanH"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_postscale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 150
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_prescale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv2_sTanH"
  type: "TanH"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_postscale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 250
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_prescale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv3_sTanH"
  type: "TanH"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_postscale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc4_300"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc4_300"
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fc4_prescale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "fc4_sTanH"
  type: "TanH"
  bottom: "fc4_300"
  top: "fc4_300"
}
layer {
  name: "fc4_postscale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "drop4"
  type: "Dropout"
  bottom: "fc4_300"
  top: "fc4_300"
  dropout_param {
    dropout_ratio: 0.4
  }
}
layer {
  name: "fc5_67"
  type: "InnerProduct"
  bottom: "fc4_300"
  top: "fc5_classes"
  inner_product_param {
    num_output: 67
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc5_classes"
  bottom: "label"
  top: "loss"
}
I0411 13:20:15.880925 10031 layer_factory.hpp:77] Creating layer data
I0411 13:20:15.882058 10031 net.cpp:100] Creating Layer data
I0411 13:20:15.882077 10031 net.cpp:408] data -> data
I0411 13:20:15.882100 10031 net.cpp:408] data -> label
I0411 13:20:15.884228 10130 db_lmdb.cpp:35] Opened lmdb ../local_data/lmdb/rtsd-r1/CoNorm/train/lmdb
I0411 13:20:15.902812 10031 data_layer.cpp:41] output data size: 1024,3,48,48
I0411 13:20:15.950364 10031 net.cpp:150] Setting up data
I0411 13:20:15.950394 10031 net.cpp:157] Top shape: 1024 3 48 48 (7077888)
I0411 13:20:15.950400 10031 net.cpp:157] Top shape: 1024 (1024)
I0411 13:20:15.950405 10031 net.cpp:165] Memory required for data: 28315648
I0411 13:20:15.950415 10031 layer_factory.hpp:77] Creating layer conv1
I0411 13:20:15.950438 10031 net.cpp:100] Creating Layer conv1
I0411 13:20:15.950444 10031 net.cpp:434] conv1 <- data
I0411 13:20:15.950458 10031 net.cpp:408] conv1 -> conv1
I0411 13:20:16.552623 10031 net.cpp:150] Setting up conv1
I0411 13:20:16.552654 10031 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 13:20:16.552657 10031 net.cpp:165] Memory required for data: 750850048
I0411 13:20:16.552680 10031 layer_factory.hpp:77] Creating layer conv1_prescale
I0411 13:20:16.552696 10031 net.cpp:100] Creating Layer conv1_prescale
I0411 13:20:16.552702 10031 net.cpp:434] conv1_prescale <- conv1
I0411 13:20:16.552708 10031 net.cpp:395] conv1_prescale -> conv1 (in-place)
I0411 13:20:16.552817 10031 net.cpp:150] Setting up conv1_prescale
I0411 13:20:16.552827 10031 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 13:20:16.552830 10031 net.cpp:165] Memory required for data: 1473384448
I0411 13:20:16.552837 10031 layer_factory.hpp:77] Creating layer conv1_sTanH
I0411 13:20:16.552845 10031 net.cpp:100] Creating Layer conv1_sTanH
I0411 13:20:16.552847 10031 net.cpp:434] conv1_sTanH <- conv1
I0411 13:20:16.552852 10031 net.cpp:395] conv1_sTanH -> conv1 (in-place)
I0411 13:20:16.553047 10031 net.cpp:150] Setting up conv1_sTanH
I0411 13:20:16.553062 10031 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 13:20:16.553067 10031 net.cpp:165] Memory required for data: 2195918848
I0411 13:20:16.553071 10031 layer_factory.hpp:77] Creating layer conv1_postscale
I0411 13:20:16.553079 10031 net.cpp:100] Creating Layer conv1_postscale
I0411 13:20:16.553083 10031 net.cpp:434] conv1_postscale <- conv1
I0411 13:20:16.553088 10031 net.cpp:395] conv1_postscale -> conv1 (in-place)
I0411 13:20:16.553184 10031 net.cpp:150] Setting up conv1_postscale
I0411 13:20:16.553191 10031 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 13:20:16.553194 10031 net.cpp:165] Memory required for data: 2918453248
I0411 13:20:16.553200 10031 layer_factory.hpp:77] Creating layer pool1
I0411 13:20:16.553206 10031 net.cpp:100] Creating Layer pool1
I0411 13:20:16.553210 10031 net.cpp:434] pool1 <- conv1
I0411 13:20:16.553215 10031 net.cpp:408] pool1 -> pool1
I0411 13:20:16.553287 10031 net.cpp:150] Setting up pool1
I0411 13:20:16.553297 10031 net.cpp:157] Top shape: 1024 100 21 21 (45158400)
I0411 13:20:16.553299 10031 net.cpp:165] Memory required for data: 3099086848
I0411 13:20:16.553303 10031 layer_factory.hpp:77] Creating layer conv2
I0411 13:20:16.553313 10031 net.cpp:100] Creating Layer conv2
I0411 13:20:16.553318 10031 net.cpp:434] conv2 <- pool1
I0411 13:20:16.553323 10031 net.cpp:408] conv2 -> conv2
I0411 13:20:16.566525 10031 net.cpp:150] Setting up conv2
I0411 13:20:16.566545 10031 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 13:20:16.566550 10031 net.cpp:165] Memory required for data: 3298152448
I0411 13:20:16.566560 10031 layer_factory.hpp:77] Creating layer conv2_prescale
I0411 13:20:16.566573 10031 net.cpp:100] Creating Layer conv2_prescale
I0411 13:20:16.566579 10031 net.cpp:434] conv2_prescale <- conv2
I0411 13:20:16.566584 10031 net.cpp:395] conv2_prescale -> conv2 (in-place)
I0411 13:20:16.566700 10031 net.cpp:150] Setting up conv2_prescale
I0411 13:20:16.566709 10031 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 13:20:16.566712 10031 net.cpp:165] Memory required for data: 3497218048
I0411 13:20:16.566717 10031 layer_factory.hpp:77] Creating layer conv2_sTanH
I0411 13:20:16.566725 10031 net.cpp:100] Creating Layer conv2_sTanH
I0411 13:20:16.566730 10031 net.cpp:434] conv2_sTanH <- conv2
I0411 13:20:16.566735 10031 net.cpp:395] conv2_sTanH -> conv2 (in-place)
I0411 13:20:16.567692 10031 net.cpp:150] Setting up conv2_sTanH
I0411 13:20:16.567708 10031 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 13:20:16.567713 10031 net.cpp:165] Memory required for data: 3696283648
I0411 13:20:16.567716 10031 layer_factory.hpp:77] Creating layer conv2_postscale
I0411 13:20:16.567723 10031 net.cpp:100] Creating Layer conv2_postscale
I0411 13:20:16.567728 10031 net.cpp:434] conv2_postscale <- conv2
I0411 13:20:16.567734 10031 net.cpp:395] conv2_postscale -> conv2 (in-place)
I0411 13:20:16.567836 10031 net.cpp:150] Setting up conv2_postscale
I0411 13:20:16.567845 10031 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 13:20:16.567848 10031 net.cpp:165] Memory required for data: 3895349248
I0411 13:20:16.567853 10031 layer_factory.hpp:77] Creating layer pool2
I0411 13:20:16.567859 10031 net.cpp:100] Creating Layer pool2
I0411 13:20:16.567863 10031 net.cpp:434] pool2 <- conv2
I0411 13:20:16.567870 10031 net.cpp:408] pool2 -> pool2
I0411 13:20:16.567932 10031 net.cpp:150] Setting up pool2
I0411 13:20:16.567942 10031 net.cpp:157] Top shape: 1024 150 9 9 (12441600)
I0411 13:20:16.567945 10031 net.cpp:165] Memory required for data: 3945115648
I0411 13:20:16.567948 10031 layer_factory.hpp:77] Creating layer conv3
I0411 13:20:16.567958 10031 net.cpp:100] Creating Layer conv3
I0411 13:20:16.567962 10031 net.cpp:434] conv3 <- pool2
I0411 13:20:16.567970 10031 net.cpp:408] conv3 -> conv3
I0411 13:20:16.578279 10031 net.cpp:150] Setting up conv3
I0411 13:20:16.578296 10031 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 13:20:16.578300 10031 net.cpp:165] Memory required for data: 3981979648
I0411 13:20:16.578310 10031 layer_factory.hpp:77] Creating layer conv3_prescale
I0411 13:20:16.578321 10031 net.cpp:100] Creating Layer conv3_prescale
I0411 13:20:16.578326 10031 net.cpp:434] conv3_prescale <- conv3
I0411 13:20:16.578331 10031 net.cpp:395] conv3_prescale -> conv3 (in-place)
I0411 13:20:16.578431 10031 net.cpp:150] Setting up conv3_prescale
I0411 13:20:16.578440 10031 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 13:20:16.578444 10031 net.cpp:165] Memory required for data: 4018843648
I0411 13:20:16.578447 10031 layer_factory.hpp:77] Creating layer conv3_sTanH
I0411 13:20:16.578454 10031 net.cpp:100] Creating Layer conv3_sTanH
I0411 13:20:16.578457 10031 net.cpp:434] conv3_sTanH <- conv3
I0411 13:20:16.578464 10031 net.cpp:395] conv3_sTanH -> conv3 (in-place)
I0411 13:20:16.580117 10031 net.cpp:150] Setting up conv3_sTanH
I0411 13:20:16.580137 10031 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 13:20:16.580140 10031 net.cpp:165] Memory required for data: 4055707648
I0411 13:20:16.580157 10031 layer_factory.hpp:77] Creating layer conv3_postscale
I0411 13:20:16.580169 10031 net.cpp:100] Creating Layer conv3_postscale
I0411 13:20:16.580174 10031 net.cpp:434] conv3_postscale <- conv3
I0411 13:20:16.580180 10031 net.cpp:395] conv3_postscale -> conv3 (in-place)
I0411 13:20:16.580287 10031 net.cpp:150] Setting up conv3_postscale
I0411 13:20:16.580297 10031 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 13:20:16.580302 10031 net.cpp:165] Memory required for data: 4092571648
I0411 13:20:16.580307 10031 layer_factory.hpp:77] Creating layer pool3
I0411 13:20:16.580314 10031 net.cpp:100] Creating Layer pool3
I0411 13:20:16.580319 10031 net.cpp:434] pool3 <- conv3
I0411 13:20:16.580324 10031 net.cpp:408] pool3 -> pool3
I0411 13:20:16.580365 10031 net.cpp:150] Setting up pool3
I0411 13:20:16.580374 10031 net.cpp:157] Top shape: 1024 250 3 3 (2304000)
I0411 13:20:16.580375 10031 net.cpp:165] Memory required for data: 4101787648
I0411 13:20:16.580379 10031 layer_factory.hpp:77] Creating layer fc4_300
I0411 13:20:16.580389 10031 net.cpp:100] Creating Layer fc4_300
I0411 13:20:16.580394 10031 net.cpp:434] fc4_300 <- pool3
I0411 13:20:16.580399 10031 net.cpp:408] fc4_300 -> fc4_300
I0411 13:20:16.585912 10031 net.cpp:150] Setting up fc4_300
I0411 13:20:16.585929 10031 net.cpp:157] Top shape: 1024 300 (307200)
I0411 13:20:16.585933 10031 net.cpp:165] Memory required for data: 4103016448
I0411 13:20:16.585940 10031 layer_factory.hpp:77] Creating layer fc4_prescale
I0411 13:20:16.585950 10031 net.cpp:100] Creating Layer fc4_prescale
I0411 13:20:16.585957 10031 net.cpp:434] fc4_prescale <- fc4_300
I0411 13:20:16.585961 10031 net.cpp:395] fc4_prescale -> fc4_300 (in-place)
I0411 13:20:16.586055 10031 net.cpp:150] Setting up fc4_prescale
I0411 13:20:16.586063 10031 net.cpp:157] Top shape: 1024 300 (307200)
I0411 13:20:16.586066 10031 net.cpp:165] Memory required for data: 4104245248
I0411 13:20:16.586071 10031 layer_factory.hpp:77] Creating layer fc4_sTanH
I0411 13:20:16.586076 10031 net.cpp:100] Creating Layer fc4_sTanH
I0411 13:20:16.586078 10031 net.cpp:434] fc4_sTanH <- fc4_300
I0411 13:20:16.586082 10031 net.cpp:395] fc4_sTanH -> fc4_300 (in-place)
I0411 13:20:16.586273 10031 net.cpp:150] Setting up fc4_sTanH
I0411 13:20:16.586285 10031 net.cpp:157] Top shape: 1024 300 (307200)
I0411 13:20:16.586288 10031 net.cpp:165] Memory required for data: 4105474048
I0411 13:20:16.586292 10031 layer_factory.hpp:77] Creating layer fc4_postscale
I0411 13:20:16.586299 10031 net.cpp:100] Creating Layer fc4_postscale
I0411 13:20:16.586304 10031 net.cpp:434] fc4_postscale <- fc4_300
I0411 13:20:16.586308 10031 net.cpp:395] fc4_postscale -> fc4_300 (in-place)
I0411 13:20:16.586406 10031 net.cpp:150] Setting up fc4_postscale
I0411 13:20:16.586414 10031 net.cpp:157] Top shape: 1024 300 (307200)
I0411 13:20:16.586417 10031 net.cpp:165] Memory required for data: 4106702848
I0411 13:20:16.586431 10031 layer_factory.hpp:77] Creating layer drop4
I0411 13:20:16.586436 10031 net.cpp:100] Creating Layer drop4
I0411 13:20:16.586441 10031 net.cpp:434] drop4 <- fc4_300
I0411 13:20:16.586447 10031 net.cpp:395] drop4 -> fc4_300 (in-place)
I0411 13:20:16.586472 10031 net.cpp:150] Setting up drop4
I0411 13:20:16.586480 10031 net.cpp:157] Top shape: 1024 300 (307200)
I0411 13:20:16.586483 10031 net.cpp:165] Memory required for data: 4107931648
I0411 13:20:16.586485 10031 layer_factory.hpp:77] Creating layer fc5_67
I0411 13:20:16.586498 10031 net.cpp:100] Creating Layer fc5_67
I0411 13:20:16.586503 10031 net.cpp:434] fc5_67 <- fc4_300
I0411 13:20:16.586508 10031 net.cpp:408] fc5_67 -> fc5_classes
I0411 13:20:16.589545 10031 net.cpp:150] Setting up fc5_67
I0411 13:20:16.589567 10031 net.cpp:157] Top shape: 1024 67 (68608)
I0411 13:20:16.589576 10031 net.cpp:165] Memory required for data: 4108206080
I0411 13:20:16.589593 10031 layer_factory.hpp:77] Creating layer loss
I0411 13:20:16.589604 10031 net.cpp:100] Creating Layer loss
I0411 13:20:16.589609 10031 net.cpp:434] loss <- fc5_classes
I0411 13:20:16.589613 10031 net.cpp:434] loss <- label
I0411 13:20:16.589635 10031 net.cpp:408] loss -> loss
I0411 13:20:16.589648 10031 layer_factory.hpp:77] Creating layer loss
I0411 13:20:16.590001 10031 net.cpp:150] Setting up loss
I0411 13:20:16.590014 10031 net.cpp:157] Top shape: (1)
I0411 13:20:16.590018 10031 net.cpp:160]     with loss weight 1
I0411 13:20:16.590032 10031 net.cpp:165] Memory required for data: 4108206084
I0411 13:20:16.590036 10031 net.cpp:226] loss needs backward computation.
I0411 13:20:16.590044 10031 net.cpp:226] fc5_67 needs backward computation.
I0411 13:20:16.590047 10031 net.cpp:226] drop4 needs backward computation.
I0411 13:20:16.590050 10031 net.cpp:226] fc4_postscale needs backward computation.
I0411 13:20:16.590052 10031 net.cpp:226] fc4_sTanH needs backward computation.
I0411 13:20:16.590055 10031 net.cpp:226] fc4_prescale needs backward computation.
I0411 13:20:16.590057 10031 net.cpp:226] fc4_300 needs backward computation.
I0411 13:20:16.590060 10031 net.cpp:226] pool3 needs backward computation.
I0411 13:20:16.590065 10031 net.cpp:226] conv3_postscale needs backward computation.
I0411 13:20:16.590067 10031 net.cpp:226] conv3_sTanH needs backward computation.
I0411 13:20:16.590070 10031 net.cpp:226] conv3_prescale needs backward computation.
I0411 13:20:16.590072 10031 net.cpp:226] conv3 needs backward computation.
I0411 13:20:16.590075 10031 net.cpp:226] pool2 needs backward computation.
I0411 13:20:16.590078 10031 net.cpp:226] conv2_postscale needs backward computation.
I0411 13:20:16.590081 10031 net.cpp:226] conv2_sTanH needs backward computation.
I0411 13:20:16.590085 10031 net.cpp:226] conv2_prescale needs backward computation.
I0411 13:20:16.590086 10031 net.cpp:226] conv2 needs backward computation.
I0411 13:20:16.590090 10031 net.cpp:226] pool1 needs backward computation.
I0411 13:20:16.590093 10031 net.cpp:226] conv1_postscale needs backward computation.
I0411 13:20:16.590095 10031 net.cpp:226] conv1_sTanH needs backward computation.
I0411 13:20:16.590098 10031 net.cpp:226] conv1_prescale needs backward computation.
I0411 13:20:16.590101 10031 net.cpp:226] conv1 needs backward computation.
I0411 13:20:16.590104 10031 net.cpp:228] data does not need backward computation.
I0411 13:20:16.590107 10031 net.cpp:270] This network produces output loss
I0411 13:20:16.590123 10031 net.cpp:283] Network initialization done.
I0411 13:20:16.590391 10031 solver.cpp:193] Creating test net (#0) specified by test_net file: ./Prototxt/experiment_9/rtsd-r1/CoNorm/trial_1/test.prototxt
I0411 13:20:16.590574 10031 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.0039215689
    mirror: false
    crop_size: 48
    mean_value: 133
    mean_value: 133
    mean_value: 132
  }
  data_param {
    source: "../local_data/lmdb/rtsd-r1/CoNorm/test/lmdb"
    batch_size: 1024
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 7
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_prescale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv1_sTanH"
  type: "TanH"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_postscale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 150
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_prescale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv2_sTanH"
  type: "TanH"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_postscale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 250
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_prescale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv3_sTanH"
  type: "TanH"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_postscale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc4_300"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc4_300"
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fc4_prescale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "fc4_sTanH"
  type: "TanH"
  bottom: "fc4_300"
  top: "fc4_300"
}
layer {
  name: "fc4_postscale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "drop4"
  type: "Dropout"
  bottom: "fc4_300"
  top: "fc4_300"
  dropout_param {
    dropout_ratio: 0.4
  }
}
layer {
  name: "fc5_67"
  type: "InnerProduct"
  bottom: "fc4_300"
  top: "fc5_classes"
  inner_product_param {
    num_output: 67
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc5_classes"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy_1"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_1"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 1
  }
}
layer {
  name: "accuracy_5"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0411 13:20:16.590697 10031 layer_factory.hpp:77] Creating layer data
I0411 13:20:16.591367 10031 net.cpp:100] Creating Layer data
I0411 13:20:16.591382 10031 net.cpp:408] data -> data
I0411 13:20:16.591390 10031 net.cpp:408] data -> label
I0411 13:20:16.595777 10203 db_lmdb.cpp:35] Opened lmdb ../local_data/lmdb/rtsd-r1/CoNorm/test/lmdb
I0411 13:20:16.595959 10031 data_layer.cpp:41] output data size: 1024,3,48,48
I0411 13:20:16.646948 10031 net.cpp:150] Setting up data
I0411 13:20:16.646977 10031 net.cpp:157] Top shape: 1024 3 48 48 (7077888)
I0411 13:20:16.646983 10031 net.cpp:157] Top shape: 1024 (1024)
I0411 13:20:16.646986 10031 net.cpp:165] Memory required for data: 28315648
I0411 13:20:16.646992 10031 layer_factory.hpp:77] Creating layer label_data_1_split
I0411 13:20:16.647006 10031 net.cpp:100] Creating Layer label_data_1_split
I0411 13:20:16.647011 10031 net.cpp:434] label_data_1_split <- label
I0411 13:20:16.647019 10031 net.cpp:408] label_data_1_split -> label_data_1_split_0
I0411 13:20:16.647032 10031 net.cpp:408] label_data_1_split -> label_data_1_split_1
I0411 13:20:16.647060 10031 net.cpp:408] label_data_1_split -> label_data_1_split_2
I0411 13:20:16.647140 10031 net.cpp:150] Setting up label_data_1_split
I0411 13:20:16.647150 10031 net.cpp:157] Top shape: 1024 (1024)
I0411 13:20:16.647156 10031 net.cpp:157] Top shape: 1024 (1024)
I0411 13:20:16.647159 10031 net.cpp:157] Top shape: 1024 (1024)
I0411 13:20:16.647161 10031 net.cpp:165] Memory required for data: 28327936
I0411 13:20:16.647164 10031 layer_factory.hpp:77] Creating layer conv1
I0411 13:20:16.647176 10031 net.cpp:100] Creating Layer conv1
I0411 13:20:16.647181 10031 net.cpp:434] conv1 <- data
I0411 13:20:16.647187 10031 net.cpp:408] conv1 -> conv1
I0411 13:20:16.651103 10031 net.cpp:150] Setting up conv1
I0411 13:20:16.651127 10031 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 13:20:16.651131 10031 net.cpp:165] Memory required for data: 750862336
I0411 13:20:16.651144 10031 layer_factory.hpp:77] Creating layer conv1_prescale
I0411 13:20:16.651156 10031 net.cpp:100] Creating Layer conv1_prescale
I0411 13:20:16.651162 10031 net.cpp:434] conv1_prescale <- conv1
I0411 13:20:16.651167 10031 net.cpp:395] conv1_prescale -> conv1 (in-place)
I0411 13:20:16.651275 10031 net.cpp:150] Setting up conv1_prescale
I0411 13:20:16.651284 10031 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 13:20:16.651288 10031 net.cpp:165] Memory required for data: 1473396736
I0411 13:20:16.651294 10031 layer_factory.hpp:77] Creating layer conv1_sTanH
I0411 13:20:16.651309 10031 net.cpp:100] Creating Layer conv1_sTanH
I0411 13:20:16.651314 10031 net.cpp:434] conv1_sTanH <- conv1
I0411 13:20:16.651321 10031 net.cpp:395] conv1_sTanH -> conv1 (in-place)
I0411 13:20:16.651520 10031 net.cpp:150] Setting up conv1_sTanH
I0411 13:20:16.651530 10031 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 13:20:16.651536 10031 net.cpp:165] Memory required for data: 2195931136
I0411 13:20:16.651540 10031 layer_factory.hpp:77] Creating layer conv1_postscale
I0411 13:20:16.651547 10031 net.cpp:100] Creating Layer conv1_postscale
I0411 13:20:16.651551 10031 net.cpp:434] conv1_postscale <- conv1
I0411 13:20:16.651557 10031 net.cpp:395] conv1_postscale -> conv1 (in-place)
I0411 13:20:16.651672 10031 net.cpp:150] Setting up conv1_postscale
I0411 13:20:16.651680 10031 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 13:20:16.651684 10031 net.cpp:165] Memory required for data: 2918465536
I0411 13:20:16.651688 10031 layer_factory.hpp:77] Creating layer pool1
I0411 13:20:16.651695 10031 net.cpp:100] Creating Layer pool1
I0411 13:20:16.651700 10031 net.cpp:434] pool1 <- conv1
I0411 13:20:16.651706 10031 net.cpp:408] pool1 -> pool1
I0411 13:20:16.651749 10031 net.cpp:150] Setting up pool1
I0411 13:20:16.651759 10031 net.cpp:157] Top shape: 1024 100 21 21 (45158400)
I0411 13:20:16.651762 10031 net.cpp:165] Memory required for data: 3099099136
I0411 13:20:16.651765 10031 layer_factory.hpp:77] Creating layer conv2
I0411 13:20:16.651779 10031 net.cpp:100] Creating Layer conv2
I0411 13:20:16.651784 10031 net.cpp:434] conv2 <- pool1
I0411 13:20:16.651789 10031 net.cpp:408] conv2 -> conv2
I0411 13:20:16.655669 10031 net.cpp:150] Setting up conv2
I0411 13:20:16.655689 10031 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 13:20:16.655694 10031 net.cpp:165] Memory required for data: 3298164736
I0411 13:20:16.655704 10031 layer_factory.hpp:77] Creating layer conv2_prescale
I0411 13:20:16.655716 10031 net.cpp:100] Creating Layer conv2_prescale
I0411 13:20:16.655724 10031 net.cpp:434] conv2_prescale <- conv2
I0411 13:20:16.655730 10031 net.cpp:395] conv2_prescale -> conv2 (in-place)
I0411 13:20:16.655843 10031 net.cpp:150] Setting up conv2_prescale
I0411 13:20:16.655851 10031 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 13:20:16.655854 10031 net.cpp:165] Memory required for data: 3497230336
I0411 13:20:16.655859 10031 layer_factory.hpp:77] Creating layer conv2_sTanH
I0411 13:20:16.655864 10031 net.cpp:100] Creating Layer conv2_sTanH
I0411 13:20:16.655869 10031 net.cpp:434] conv2_sTanH <- conv2
I0411 13:20:16.655875 10031 net.cpp:395] conv2_sTanH -> conv2 (in-place)
I0411 13:20:16.656687 10031 net.cpp:150] Setting up conv2_sTanH
I0411 13:20:16.656702 10031 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 13:20:16.656708 10031 net.cpp:165] Memory required for data: 3696295936
I0411 13:20:16.656711 10031 layer_factory.hpp:77] Creating layer conv2_postscale
I0411 13:20:16.656720 10031 net.cpp:100] Creating Layer conv2_postscale
I0411 13:20:16.656725 10031 net.cpp:434] conv2_postscale <- conv2
I0411 13:20:16.656731 10031 net.cpp:395] conv2_postscale -> conv2 (in-place)
I0411 13:20:16.656837 10031 net.cpp:150] Setting up conv2_postscale
I0411 13:20:16.656847 10031 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 13:20:16.656848 10031 net.cpp:165] Memory required for data: 3895361536
I0411 13:20:16.656853 10031 layer_factory.hpp:77] Creating layer pool2
I0411 13:20:16.656859 10031 net.cpp:100] Creating Layer pool2
I0411 13:20:16.656867 10031 net.cpp:434] pool2 <- conv2
I0411 13:20:16.656872 10031 net.cpp:408] pool2 -> pool2
I0411 13:20:16.656920 10031 net.cpp:150] Setting up pool2
I0411 13:20:16.656929 10031 net.cpp:157] Top shape: 1024 150 9 9 (12441600)
I0411 13:20:16.656931 10031 net.cpp:165] Memory required for data: 3945127936
I0411 13:20:16.656934 10031 layer_factory.hpp:77] Creating layer conv3
I0411 13:20:16.656944 10031 net.cpp:100] Creating Layer conv3
I0411 13:20:16.656952 10031 net.cpp:434] conv3 <- pool2
I0411 13:20:16.656957 10031 net.cpp:408] conv3 -> conv3
I0411 13:20:16.662506 10031 net.cpp:150] Setting up conv3
I0411 13:20:16.662524 10031 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 13:20:16.662529 10031 net.cpp:165] Memory required for data: 3981991936
I0411 13:20:16.662544 10031 layer_factory.hpp:77] Creating layer conv3_prescale
I0411 13:20:16.662554 10031 net.cpp:100] Creating Layer conv3_prescale
I0411 13:20:16.662559 10031 net.cpp:434] conv3_prescale <- conv3
I0411 13:20:16.662566 10031 net.cpp:395] conv3_prescale -> conv3 (in-place)
I0411 13:20:16.662680 10031 net.cpp:150] Setting up conv3_prescale
I0411 13:20:16.662690 10031 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 13:20:16.662693 10031 net.cpp:165] Memory required for data: 4018855936
I0411 13:20:16.662699 10031 layer_factory.hpp:77] Creating layer conv3_sTanH
I0411 13:20:16.662708 10031 net.cpp:100] Creating Layer conv3_sTanH
I0411 13:20:16.662713 10031 net.cpp:434] conv3_sTanH <- conv3
I0411 13:20:16.662717 10031 net.cpp:395] conv3_sTanH -> conv3 (in-place)
I0411 13:20:16.663487 10031 net.cpp:150] Setting up conv3_sTanH
I0411 13:20:16.663508 10031 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 13:20:16.663517 10031 net.cpp:165] Memory required for data: 4055719936
I0411 13:20:16.663522 10031 layer_factory.hpp:77] Creating layer conv3_postscale
I0411 13:20:16.663528 10031 net.cpp:100] Creating Layer conv3_postscale
I0411 13:20:16.663532 10031 net.cpp:434] conv3_postscale <- conv3
I0411 13:20:16.663539 10031 net.cpp:395] conv3_postscale -> conv3 (in-place)
I0411 13:20:16.663650 10031 net.cpp:150] Setting up conv3_postscale
I0411 13:20:16.663660 10031 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 13:20:16.663664 10031 net.cpp:165] Memory required for data: 4092583936
I0411 13:20:16.663671 10031 layer_factory.hpp:77] Creating layer pool3
I0411 13:20:16.663679 10031 net.cpp:100] Creating Layer pool3
I0411 13:20:16.663684 10031 net.cpp:434] pool3 <- conv3
I0411 13:20:16.663689 10031 net.cpp:408] pool3 -> pool3
I0411 13:20:16.663733 10031 net.cpp:150] Setting up pool3
I0411 13:20:16.663741 10031 net.cpp:157] Top shape: 1024 250 3 3 (2304000)
I0411 13:20:16.663743 10031 net.cpp:165] Memory required for data: 4101799936
I0411 13:20:16.663748 10031 layer_factory.hpp:77] Creating layer fc4_300
I0411 13:20:16.663755 10031 net.cpp:100] Creating Layer fc4_300
I0411 13:20:16.663760 10031 net.cpp:434] fc4_300 <- pool3
I0411 13:20:16.663764 10031 net.cpp:408] fc4_300 -> fc4_300
I0411 13:20:16.669127 10031 net.cpp:150] Setting up fc4_300
I0411 13:20:16.669143 10031 net.cpp:157] Top shape: 1024 300 (307200)
I0411 13:20:16.669147 10031 net.cpp:165] Memory required for data: 4103028736
I0411 13:20:16.669167 10031 layer_factory.hpp:77] Creating layer fc4_prescale
I0411 13:20:16.669178 10031 net.cpp:100] Creating Layer fc4_prescale
I0411 13:20:16.669181 10031 net.cpp:434] fc4_prescale <- fc4_300
I0411 13:20:16.669186 10031 net.cpp:395] fc4_prescale -> fc4_300 (in-place)
I0411 13:20:16.669322 10031 net.cpp:150] Setting up fc4_prescale
I0411 13:20:16.669339 10031 net.cpp:157] Top shape: 1024 300 (307200)
I0411 13:20:16.669348 10031 net.cpp:165] Memory required for data: 4104257536
I0411 13:20:16.669355 10031 layer_factory.hpp:77] Creating layer fc4_sTanH
I0411 13:20:16.669366 10031 net.cpp:100] Creating Layer fc4_sTanH
I0411 13:20:16.669373 10031 net.cpp:434] fc4_sTanH <- fc4_300
I0411 13:20:16.669384 10031 net.cpp:395] fc4_sTanH -> fc4_300 (in-place)
I0411 13:20:16.669600 10031 net.cpp:150] Setting up fc4_sTanH
I0411 13:20:16.669611 10031 net.cpp:157] Top shape: 1024 300 (307200)
I0411 13:20:16.669615 10031 net.cpp:165] Memory required for data: 4105486336
I0411 13:20:16.669617 10031 layer_factory.hpp:77] Creating layer fc4_postscale
I0411 13:20:16.669626 10031 net.cpp:100] Creating Layer fc4_postscale
I0411 13:20:16.669631 10031 net.cpp:434] fc4_postscale <- fc4_300
I0411 13:20:16.669637 10031 net.cpp:395] fc4_postscale -> fc4_300 (in-place)
I0411 13:20:16.669749 10031 net.cpp:150] Setting up fc4_postscale
I0411 13:20:16.669757 10031 net.cpp:157] Top shape: 1024 300 (307200)
I0411 13:20:16.669760 10031 net.cpp:165] Memory required for data: 4106715136
I0411 13:20:16.669765 10031 layer_factory.hpp:77] Creating layer drop4
I0411 13:20:16.669771 10031 net.cpp:100] Creating Layer drop4
I0411 13:20:16.669776 10031 net.cpp:434] drop4 <- fc4_300
I0411 13:20:16.669782 10031 net.cpp:395] drop4 -> fc4_300 (in-place)
I0411 13:20:16.669816 10031 net.cpp:150] Setting up drop4
I0411 13:20:16.669823 10031 net.cpp:157] Top shape: 1024 300 (307200)
I0411 13:20:16.669826 10031 net.cpp:165] Memory required for data: 4107943936
I0411 13:20:16.669829 10031 layer_factory.hpp:77] Creating layer fc5_67
I0411 13:20:16.669836 10031 net.cpp:100] Creating Layer fc5_67
I0411 13:20:16.669838 10031 net.cpp:434] fc5_67 <- fc4_300
I0411 13:20:16.669844 10031 net.cpp:408] fc5_67 -> fc5_classes
I0411 13:20:16.670099 10031 net.cpp:150] Setting up fc5_67
I0411 13:20:16.670106 10031 net.cpp:157] Top shape: 1024 67 (68608)
I0411 13:20:16.670121 10031 net.cpp:165] Memory required for data: 4108218368
I0411 13:20:16.670133 10031 layer_factory.hpp:77] Creating layer fc5_classes_fc5_67_0_split
I0411 13:20:16.670142 10031 net.cpp:100] Creating Layer fc5_classes_fc5_67_0_split
I0411 13:20:16.670147 10031 net.cpp:434] fc5_classes_fc5_67_0_split <- fc5_classes
I0411 13:20:16.670153 10031 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_0
I0411 13:20:16.670161 10031 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_1
I0411 13:20:16.670167 10031 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_2
I0411 13:20:16.670222 10031 net.cpp:150] Setting up fc5_classes_fc5_67_0_split
I0411 13:20:16.670229 10031 net.cpp:157] Top shape: 1024 67 (68608)
I0411 13:20:16.670233 10031 net.cpp:157] Top shape: 1024 67 (68608)
I0411 13:20:16.670236 10031 net.cpp:157] Top shape: 1024 67 (68608)
I0411 13:20:16.670249 10031 net.cpp:165] Memory required for data: 4109041664
I0411 13:20:16.670253 10031 layer_factory.hpp:77] Creating layer loss
I0411 13:20:16.670258 10031 net.cpp:100] Creating Layer loss
I0411 13:20:16.670262 10031 net.cpp:434] loss <- fc5_classes_fc5_67_0_split_0
I0411 13:20:16.670266 10031 net.cpp:434] loss <- label_data_1_split_0
I0411 13:20:16.670279 10031 net.cpp:408] loss -> loss
I0411 13:20:16.670292 10031 layer_factory.hpp:77] Creating layer loss
I0411 13:20:16.670635 10031 net.cpp:150] Setting up loss
I0411 13:20:16.670650 10031 net.cpp:157] Top shape: (1)
I0411 13:20:16.670653 10031 net.cpp:160]     with loss weight 1
I0411 13:20:16.670663 10031 net.cpp:165] Memory required for data: 4109041668
I0411 13:20:16.670667 10031 layer_factory.hpp:77] Creating layer accuracy_1
I0411 13:20:16.670688 10031 net.cpp:100] Creating Layer accuracy_1
I0411 13:20:16.670694 10031 net.cpp:434] accuracy_1 <- fc5_classes_fc5_67_0_split_1
I0411 13:20:16.670699 10031 net.cpp:434] accuracy_1 <- label_data_1_split_1
I0411 13:20:16.670704 10031 net.cpp:408] accuracy_1 -> accuracy_1
I0411 13:20:16.670719 10031 net.cpp:150] Setting up accuracy_1
I0411 13:20:16.670725 10031 net.cpp:157] Top shape: (1)
I0411 13:20:16.670727 10031 net.cpp:165] Memory required for data: 4109041672
I0411 13:20:16.670730 10031 layer_factory.hpp:77] Creating layer accuracy_5
I0411 13:20:16.670735 10031 net.cpp:100] Creating Layer accuracy_5
I0411 13:20:16.670738 10031 net.cpp:434] accuracy_5 <- fc5_classes_fc5_67_0_split_2
I0411 13:20:16.670742 10031 net.cpp:434] accuracy_5 <- label_data_1_split_2
I0411 13:20:16.670748 10031 net.cpp:408] accuracy_5 -> accuracy_5
I0411 13:20:16.670755 10031 net.cpp:150] Setting up accuracy_5
I0411 13:20:16.670761 10031 net.cpp:157] Top shape: (1)
I0411 13:20:16.670764 10031 net.cpp:165] Memory required for data: 4109041676
I0411 13:20:16.670768 10031 net.cpp:228] accuracy_5 does not need backward computation.
I0411 13:20:16.670773 10031 net.cpp:228] accuracy_1 does not need backward computation.
I0411 13:20:16.670775 10031 net.cpp:226] loss needs backward computation.
I0411 13:20:16.670779 10031 net.cpp:226] fc5_classes_fc5_67_0_split needs backward computation.
I0411 13:20:16.670783 10031 net.cpp:226] fc5_67 needs backward computation.
I0411 13:20:16.670789 10031 net.cpp:226] drop4 needs backward computation.
I0411 13:20:16.670792 10031 net.cpp:226] fc4_postscale needs backward computation.
I0411 13:20:16.670795 10031 net.cpp:226] fc4_sTanH needs backward computation.
I0411 13:20:16.670799 10031 net.cpp:226] fc4_prescale needs backward computation.
I0411 13:20:16.670800 10031 net.cpp:226] fc4_300 needs backward computation.
I0411 13:20:16.670804 10031 net.cpp:226] pool3 needs backward computation.
I0411 13:20:16.670807 10031 net.cpp:226] conv3_postscale needs backward computation.
I0411 13:20:16.670809 10031 net.cpp:226] conv3_sTanH needs backward computation.
I0411 13:20:16.670812 10031 net.cpp:226] conv3_prescale needs backward computation.
I0411 13:20:16.670814 10031 net.cpp:226] conv3 needs backward computation.
I0411 13:20:16.670817 10031 net.cpp:226] pool2 needs backward computation.
I0411 13:20:16.670820 10031 net.cpp:226] conv2_postscale needs backward computation.
I0411 13:20:16.670824 10031 net.cpp:226] conv2_sTanH needs backward computation.
I0411 13:20:16.670826 10031 net.cpp:226] conv2_prescale needs backward computation.
I0411 13:20:16.670837 10031 net.cpp:226] conv2 needs backward computation.
I0411 13:20:16.670840 10031 net.cpp:226] pool1 needs backward computation.
I0411 13:20:16.670845 10031 net.cpp:226] conv1_postscale needs backward computation.
I0411 13:20:16.670846 10031 net.cpp:226] conv1_sTanH needs backward computation.
I0411 13:20:16.670850 10031 net.cpp:226] conv1_prescale needs backward computation.
I0411 13:20:16.670852 10031 net.cpp:226] conv1 needs backward computation.
I0411 13:20:16.670856 10031 net.cpp:228] label_data_1_split does not need backward computation.
I0411 13:20:16.670861 10031 net.cpp:228] data does not need backward computation.
I0411 13:20:16.670863 10031 net.cpp:270] This network produces output accuracy_1
I0411 13:20:16.670866 10031 net.cpp:270] This network produces output accuracy_5
I0411 13:20:16.670869 10031 net.cpp:270] This network produces output loss
I0411 13:20:16.670891 10031 net.cpp:283] Network initialization done.
I0411 13:20:16.670960 10031 solver.cpp:72] Solver scaffolding done.
I0411 13:20:16.671859 10031 caffe.cpp:251] Starting Optimization
I0411 13:20:16.671867 10031 solver.cpp:291] Solving 
I0411 13:20:16.671872 10031 solver.cpp:292] Learning Rate Policy: step
I0411 13:20:16.674444 10031 solver.cpp:349] Iteration 0, Testing net (#0)
I0411 13:20:16.675914 10031 blocking_queue.cpp:50] Data layer prefetch queue empty
I0411 13:20:17.805970 10031 solver.cpp:416]     Test net output #0: accuracy_1 = 0.0108643
I0411 13:20:17.806020 10031 solver.cpp:416]     Test net output #1: accuracy_5 = 0.074707
I0411 13:20:17.806044 10031 solver.cpp:416]     Test net output #2: loss = 4.34974 (* 1 = 4.34974 loss)
I0411 13:20:17.963774 10031 solver.cpp:240] Iteration 0, loss = 4.32628
I0411 13:20:17.963820 10031 solver.cpp:256]     Train net output #0: loss = 4.32628 (* 1 = 4.32628 loss)
I0411 13:20:17.963840 10031 sgd_solver.cpp:106] Iteration 0, lr = 1e-06
I0411 13:20:18.335115 10031 solver.cpp:240] Iteration 1, loss = 4.34556
I0411 13:20:18.335150 10031 solver.cpp:256]     Train net output #0: loss = 4.34556 (* 1 = 4.34556 loss)
I0411 13:20:18.335158 10031 sgd_solver.cpp:106] Iteration 1, lr = 1e-06
I0411 13:20:18.712334 10031 solver.cpp:240] Iteration 2, loss = 4.33845
I0411 13:20:18.712368 10031 solver.cpp:256]     Train net output #0: loss = 4.33845 (* 1 = 4.33845 loss)
I0411 13:20:18.712375 10031 sgd_solver.cpp:106] Iteration 2, lr = 1e-06
I0411 13:20:19.087587 10031 solver.cpp:240] Iteration 3, loss = 4.37047
I0411 13:20:19.087620 10031 solver.cpp:256]     Train net output #0: loss = 4.37047 (* 1 = 4.37047 loss)
I0411 13:20:19.087628 10031 sgd_solver.cpp:106] Iteration 3, lr = 1e-06
I0411 13:20:19.462138 10031 solver.cpp:240] Iteration 4, loss = 4.36086
I0411 13:20:19.462172 10031 solver.cpp:256]     Train net output #0: loss = 4.36086 (* 1 = 4.36086 loss)
I0411 13:20:19.462180 10031 sgd_solver.cpp:106] Iteration 4, lr = 1e-06
I0411 13:20:19.840147 10031 solver.cpp:240] Iteration 5, loss = 4.33561
I0411 13:20:19.840183 10031 solver.cpp:256]     Train net output #0: loss = 4.33561 (* 1 = 4.33561 loss)
I0411 13:20:19.840189 10031 sgd_solver.cpp:106] Iteration 5, lr = 1e-06
I0411 13:20:20.217685 10031 solver.cpp:240] Iteration 6, loss = 4.33984
I0411 13:20:20.217722 10031 solver.cpp:256]     Train net output #0: loss = 4.33984 (* 1 = 4.33984 loss)
I0411 13:20:20.217734 10031 sgd_solver.cpp:106] Iteration 6, lr = 1e-06
I0411 13:20:20.592453 10031 solver.cpp:240] Iteration 7, loss = 4.32007
I0411 13:20:20.592489 10031 solver.cpp:256]     Train net output #0: loss = 4.32007 (* 1 = 4.32007 loss)
I0411 13:20:20.592495 10031 sgd_solver.cpp:106] Iteration 7, lr = 1e-06
I0411 13:20:20.966615 10031 solver.cpp:240] Iteration 8, loss = 4.31101
I0411 13:20:20.966650 10031 solver.cpp:256]     Train net output #0: loss = 4.31101 (* 1 = 4.31101 loss)
I0411 13:20:20.966656 10031 sgd_solver.cpp:106] Iteration 8, lr = 1e-06
I0411 13:20:21.341888 10031 solver.cpp:240] Iteration 9, loss = 4.35008
I0411 13:20:21.341922 10031 solver.cpp:256]     Train net output #0: loss = 4.35008 (* 1 = 4.35008 loss)
I0411 13:20:21.341928 10031 sgd_solver.cpp:106] Iteration 9, lr = 1e-06
I0411 13:20:21.721326 10031 solver.cpp:240] Iteration 10, loss = 4.34862
I0411 13:20:21.721359 10031 solver.cpp:256]     Train net output #0: loss = 4.34862 (* 1 = 4.34862 loss)
I0411 13:20:21.721367 10031 sgd_solver.cpp:106] Iteration 10, lr = 1e-06
I0411 13:20:22.096595 10031 solver.cpp:240] Iteration 11, loss = 4.40358
I0411 13:20:22.096627 10031 solver.cpp:256]     Train net output #0: loss = 4.40358 (* 1 = 4.40358 loss)
I0411 13:20:22.096634 10031 sgd_solver.cpp:106] Iteration 11, lr = 1e-06
I0411 13:20:22.470877 10031 solver.cpp:240] Iteration 12, loss = 4.34036
I0411 13:20:22.470916 10031 solver.cpp:256]     Train net output #0: loss = 4.34036 (* 1 = 4.34036 loss)
I0411 13:20:22.470926 10031 sgd_solver.cpp:106] Iteration 12, lr = 1e-06
I0411 13:20:22.849100 10031 solver.cpp:240] Iteration 13, loss = 4.34226
I0411 13:20:22.849133 10031 solver.cpp:256]     Train net output #0: loss = 4.34226 (* 1 = 4.34226 loss)
I0411 13:20:22.849141 10031 sgd_solver.cpp:106] Iteration 13, lr = 1e-06
I0411 13:20:23.225399 10031 solver.cpp:240] Iteration 14, loss = 4.31694
I0411 13:20:23.225431 10031 solver.cpp:256]     Train net output #0: loss = 4.31694 (* 1 = 4.31694 loss)
I0411 13:20:23.225440 10031 sgd_solver.cpp:106] Iteration 14, lr = 1e-06
I0411 13:20:23.600920 10031 solver.cpp:240] Iteration 15, loss = 4.29731
I0411 13:20:23.600955 10031 solver.cpp:256]     Train net output #0: loss = 4.29731 (* 1 = 4.29731 loss)
I0411 13:20:23.600987 10031 sgd_solver.cpp:106] Iteration 15, lr = 1e-06
I0411 13:20:23.974787 10031 solver.cpp:240] Iteration 16, loss = 4.30696
I0411 13:20:23.974820 10031 solver.cpp:256]     Train net output #0: loss = 4.30696 (* 1 = 4.30696 loss)
I0411 13:20:23.974828 10031 sgd_solver.cpp:106] Iteration 16, lr = 1e-06
I0411 13:20:24.353108 10031 solver.cpp:240] Iteration 17, loss = 4.28943
I0411 13:20:24.353142 10031 solver.cpp:256]     Train net output #0: loss = 4.28943 (* 1 = 4.28943 loss)
I0411 13:20:24.353149 10031 sgd_solver.cpp:106] Iteration 17, lr = 1e-06
I0411 13:20:24.734248 10031 solver.cpp:240] Iteration 18, loss = 4.30728
I0411 13:20:24.734282 10031 solver.cpp:256]     Train net output #0: loss = 4.30728 (* 1 = 4.30728 loss)
I0411 13:20:24.734289 10031 sgd_solver.cpp:106] Iteration 18, lr = 1e-06
I0411 13:20:25.110694 10031 solver.cpp:240] Iteration 19, loss = 4.29856
I0411 13:20:25.110728 10031 solver.cpp:256]     Train net output #0: loss = 4.29856 (* 1 = 4.29856 loss)
I0411 13:20:25.110736 10031 sgd_solver.cpp:106] Iteration 19, lr = 1e-06
I0411 13:20:25.487679 10031 solver.cpp:240] Iteration 20, loss = 4.27942
I0411 13:20:25.487712 10031 solver.cpp:256]     Train net output #0: loss = 4.27942 (* 1 = 4.27942 loss)
I0411 13:20:25.487720 10031 sgd_solver.cpp:106] Iteration 20, lr = 1e-06
I0411 13:20:25.867010 10031 solver.cpp:240] Iteration 21, loss = 4.3125
I0411 13:20:25.867053 10031 solver.cpp:256]     Train net output #0: loss = 4.3125 (* 1 = 4.3125 loss)
I0411 13:20:25.867060 10031 sgd_solver.cpp:106] Iteration 21, lr = 1e-06
I0411 13:20:26.242216 10031 solver.cpp:240] Iteration 22, loss = 4.29022
I0411 13:20:26.242254 10031 solver.cpp:256]     Train net output #0: loss = 4.29022 (* 1 = 4.29022 loss)
I0411 13:20:26.242261 10031 sgd_solver.cpp:106] Iteration 22, lr = 1e-06
I0411 13:20:26.618993 10031 solver.cpp:240] Iteration 23, loss = 4.28323
I0411 13:20:26.619025 10031 solver.cpp:256]     Train net output #0: loss = 4.28323 (* 1 = 4.28323 loss)
I0411 13:20:26.619033 10031 sgd_solver.cpp:106] Iteration 23, lr = 1e-06
I0411 13:20:26.997129 10031 solver.cpp:240] Iteration 24, loss = 4.27877
I0411 13:20:26.997164 10031 solver.cpp:256]     Train net output #0: loss = 4.27877 (* 1 = 4.27877 loss)
I0411 13:20:26.997172 10031 sgd_solver.cpp:106] Iteration 24, lr = 1e-06
I0411 13:20:26.997481 10031 solver.cpp:349] Iteration 25, Testing net (#0)
I0411 13:20:28.304774 10031 solver.cpp:416]     Test net output #0: accuracy_1 = 0.0147705
I0411 13:20:28.304802 10031 solver.cpp:416]     Test net output #1: accuracy_5 = 0.0933838
I0411 13:20:28.304812 10031 solver.cpp:416]     Test net output #2: loss = 4.30363 (* 1 = 4.30363 loss)
I0411 13:20:28.435318 10031 solver.cpp:240] Iteration 25, loss = 4.26249
I0411 13:20:28.435349 10031 solver.cpp:256]     Train net output #0: loss = 4.26249 (* 1 = 4.26249 loss)
I0411 13:20:28.435356 10031 sgd_solver.cpp:106] Iteration 25, lr = 1e-06
I0411 13:20:28.809298 10031 solver.cpp:240] Iteration 26, loss = 4.28881
I0411 13:20:28.809329 10031 solver.cpp:256]     Train net output #0: loss = 4.28881 (* 1 = 4.28881 loss)
I0411 13:20:28.809336 10031 sgd_solver.cpp:106] Iteration 26, lr = 1e-06
I0411 13:20:29.186075 10031 solver.cpp:240] Iteration 27, loss = 4.25606
I0411 13:20:29.186110 10031 solver.cpp:256]     Train net output #0: loss = 4.25606 (* 1 = 4.25606 loss)
I0411 13:20:29.186116 10031 sgd_solver.cpp:106] Iteration 27, lr = 1e-06
I0411 13:20:29.568895 10031 solver.cpp:240] Iteration 28, loss = 4.32859
I0411 13:20:29.568927 10031 solver.cpp:256]     Train net output #0: loss = 4.32859 (* 1 = 4.32859 loss)
I0411 13:20:29.568934 10031 sgd_solver.cpp:106] Iteration 28, lr = 1e-06
I0411 13:20:29.947129 10031 solver.cpp:240] Iteration 29, loss = 4.30533
I0411 13:20:29.947166 10031 solver.cpp:256]     Train net output #0: loss = 4.30533 (* 1 = 4.30533 loss)
I0411 13:20:29.947176 10031 sgd_solver.cpp:106] Iteration 29, lr = 1e-06
I0411 13:20:30.323302 10031 solver.cpp:240] Iteration 30, loss = 4.30065
I0411 13:20:30.323339 10031 solver.cpp:256]     Train net output #0: loss = 4.30065 (* 1 = 4.30065 loss)
I0411 13:20:30.323377 10031 sgd_solver.cpp:106] Iteration 30, lr = 1e-06
I0411 13:20:30.699343 10031 solver.cpp:240] Iteration 31, loss = 4.25795
I0411 13:20:30.699378 10031 solver.cpp:256]     Train net output #0: loss = 4.25795 (* 1 = 4.25795 loss)
I0411 13:20:30.699384 10031 sgd_solver.cpp:106] Iteration 31, lr = 1e-06
I0411 13:20:31.074245 10031 solver.cpp:240] Iteration 32, loss = 4.27779
I0411 13:20:31.074278 10031 solver.cpp:256]     Train net output #0: loss = 4.27779 (* 1 = 4.27779 loss)
I0411 13:20:31.074286 10031 sgd_solver.cpp:106] Iteration 32, lr = 1e-06
I0411 13:20:31.453989 10031 solver.cpp:240] Iteration 33, loss = 4.27946
I0411 13:20:31.454020 10031 solver.cpp:256]     Train net output #0: loss = 4.27946 (* 1 = 4.27946 loss)
I0411 13:20:31.454027 10031 sgd_solver.cpp:106] Iteration 33, lr = 1e-06
I0411 13:20:31.832140 10031 solver.cpp:240] Iteration 34, loss = 4.29023
I0411 13:20:31.832170 10031 solver.cpp:256]     Train net output #0: loss = 4.29023 (* 1 = 4.29023 loss)
I0411 13:20:31.832177 10031 sgd_solver.cpp:106] Iteration 34, lr = 1e-06
I0411 13:20:32.205205 10031 solver.cpp:240] Iteration 35, loss = 4.27892
I0411 13:20:32.205248 10031 solver.cpp:256]     Train net output #0: loss = 4.27892 (* 1 = 4.27892 loss)
I0411 13:20:32.205255 10031 sgd_solver.cpp:106] Iteration 35, lr = 1e-06
I0411 13:20:32.579898 10031 solver.cpp:240] Iteration 36, loss = 4.34524
I0411 13:20:32.579932 10031 solver.cpp:256]     Train net output #0: loss = 4.34524 (* 1 = 4.34524 loss)
I0411 13:20:32.579941 10031 sgd_solver.cpp:106] Iteration 36, lr = 1e-06
I0411 13:20:32.958864 10031 solver.cpp:240] Iteration 37, loss = 4.2777
I0411 13:20:32.958896 10031 solver.cpp:256]     Train net output #0: loss = 4.2777 (* 1 = 4.2777 loss)
I0411 13:20:32.958904 10031 sgd_solver.cpp:106] Iteration 37, lr = 1e-06
I0411 13:20:33.335980 10031 solver.cpp:240] Iteration 38, loss = 4.27721
I0411 13:20:33.336012 10031 solver.cpp:256]     Train net output #0: loss = 4.27721 (* 1 = 4.27721 loss)
I0411 13:20:33.336019 10031 sgd_solver.cpp:106] Iteration 38, lr = 1e-06
I0411 13:20:33.711114 10031 solver.cpp:240] Iteration 39, loss = 4.26562
I0411 13:20:33.711148 10031 solver.cpp:256]     Train net output #0: loss = 4.26562 (* 1 = 4.26562 loss)
I0411 13:20:33.711154 10031 sgd_solver.cpp:106] Iteration 39, lr = 1e-06
I0411 13:20:34.087985 10031 solver.cpp:240] Iteration 40, loss = 4.27816
I0411 13:20:34.088018 10031 solver.cpp:256]     Train net output #0: loss = 4.27816 (* 1 = 4.27816 loss)
I0411 13:20:34.088026 10031 sgd_solver.cpp:106] Iteration 40, lr = 1e-06
I0411 13:20:34.465775 10031 solver.cpp:240] Iteration 41, loss = 4.25058
I0411 13:20:34.465806 10031 solver.cpp:256]     Train net output #0: loss = 4.25058 (* 1 = 4.25058 loss)
I0411 13:20:34.465814 10031 sgd_solver.cpp:106] Iteration 41, lr = 1e-06
I0411 13:20:34.842937 10031 solver.cpp:240] Iteration 42, loss = 4.2477
I0411 13:20:34.842968 10031 solver.cpp:256]     Train net output #0: loss = 4.2477 (* 1 = 4.2477 loss)
I0411 13:20:34.842975 10031 sgd_solver.cpp:106] Iteration 42, lr = 1e-06
I0411 13:20:35.219328 10031 solver.cpp:240] Iteration 43, loss = 4.24694
I0411 13:20:35.219362 10031 solver.cpp:256]     Train net output #0: loss = 4.24694 (* 1 = 4.24694 loss)
I0411 13:20:35.219375 10031 sgd_solver.cpp:106] Iteration 43, lr = 1e-06
I0411 13:20:35.601017 10031 solver.cpp:240] Iteration 44, loss = 4.23805
I0411 13:20:35.601052 10031 solver.cpp:256]     Train net output #0: loss = 4.23805 (* 1 = 4.23805 loss)
I0411 13:20:35.601060 10031 sgd_solver.cpp:106] Iteration 44, lr = 1e-06
I0411 13:20:35.978873 10031 solver.cpp:240] Iteration 45, loss = 4.19993
I0411 13:20:35.978906 10031 solver.cpp:256]     Train net output #0: loss = 4.19993 (* 1 = 4.19993 loss)
I0411 13:20:35.978914 10031 sgd_solver.cpp:106] Iteration 45, lr = 1e-06
I0411 13:20:36.353523 10031 solver.cpp:240] Iteration 46, loss = 4.24447
I0411 13:20:36.353556 10031 solver.cpp:256]     Train net output #0: loss = 4.24447 (* 1 = 4.24447 loss)
I0411 13:20:36.353564 10031 sgd_solver.cpp:106] Iteration 46, lr = 1e-06
I0411 13:20:36.731120 10031 solver.cpp:240] Iteration 47, loss = 4.25368
I0411 13:20:36.731153 10031 solver.cpp:256]     Train net output #0: loss = 4.25368 (* 1 = 4.25368 loss)
I0411 13:20:36.731161 10031 sgd_solver.cpp:106] Iteration 47, lr = 1e-06
I0411 13:20:37.114598 10031 solver.cpp:240] Iteration 48, loss = 4.28307
I0411 13:20:37.114631 10031 solver.cpp:256]     Train net output #0: loss = 4.28307 (* 1 = 4.28307 loss)
I0411 13:20:37.114639 10031 sgd_solver.cpp:106] Iteration 48, lr = 1e-06
I0411 13:20:37.492202 10031 solver.cpp:240] Iteration 49, loss = 4.20239
I0411 13:20:37.492233 10031 solver.cpp:256]     Train net output #0: loss = 4.20239 (* 1 = 4.20239 loss)
I0411 13:20:37.492240 10031 sgd_solver.cpp:106] Iteration 49, lr = 1e-06
I0411 13:20:37.492545 10031 solver.cpp:349] Iteration 50, Testing net (#0)
I0411 13:20:38.802402 10031 solver.cpp:416]     Test net output #0: accuracy_1 = 0.0220947
I0411 13:20:38.802431 10031 solver.cpp:416]     Test net output #1: accuracy_5 = 0.115723
I0411 13:20:38.802441 10031 solver.cpp:416]     Test net output #2: loss = 4.26475 (* 1 = 4.26475 loss)
I0411 13:20:38.932876 10031 solver.cpp:240] Iteration 50, loss = 4.23899
I0411 13:20:38.932909 10031 solver.cpp:256]     Train net output #0: loss = 4.23899 (* 1 = 4.23899 loss)
I0411 13:20:38.932917 10031 sgd_solver.cpp:106] Iteration 50, lr = 1e-06
I0411 13:20:39.311208 10031 solver.cpp:240] Iteration 51, loss = 4.2581
I0411 13:20:39.311242 10031 solver.cpp:256]     Train net output #0: loss = 4.2581 (* 1 = 4.2581 loss)
I0411 13:20:39.311249 10031 sgd_solver.cpp:106] Iteration 51, lr = 1e-06
I0411 13:20:39.688681 10031 solver.cpp:240] Iteration 52, loss = 4.22981
I0411 13:20:39.688716 10031 solver.cpp:256]     Train net output #0: loss = 4.22981 (* 1 = 4.22981 loss)
I0411 13:20:39.688724 10031 sgd_solver.cpp:106] Iteration 52, lr = 1e-06
I0411 13:20:40.063601 10031 solver.cpp:240] Iteration 53, loss = 4.25085
I0411 13:20:40.063635 10031 solver.cpp:256]     Train net output #0: loss = 4.25085 (* 1 = 4.25085 loss)
I0411 13:20:40.063643 10031 sgd_solver.cpp:106] Iteration 53, lr = 1e-06
I0411 13:20:40.443086 10031 solver.cpp:240] Iteration 54, loss = 4.25146
I0411 13:20:40.443123 10031 solver.cpp:256]     Train net output #0: loss = 4.25146 (* 1 = 4.25146 loss)
I0411 13:20:40.443131 10031 sgd_solver.cpp:106] Iteration 54, lr = 1e-06
I0411 13:20:40.822129 10031 solver.cpp:240] Iteration 55, loss = 4.23132
I0411 13:20:40.822160 10031 solver.cpp:256]     Train net output #0: loss = 4.23132 (* 1 = 4.23132 loss)
I0411 13:20:40.822168 10031 sgd_solver.cpp:106] Iteration 55, lr = 1e-06
I0411 13:20:41.199082 10031 solver.cpp:240] Iteration 56, loss = 4.2122
I0411 13:20:41.199117 10031 solver.cpp:256]     Train net output #0: loss = 4.2122 (* 1 = 4.2122 loss)
I0411 13:20:41.199126 10031 sgd_solver.cpp:106] Iteration 56, lr = 1e-06
I0411 13:20:41.576480 10031 solver.cpp:240] Iteration 57, loss = 4.24327
I0411 13:20:41.576514 10031 solver.cpp:256]     Train net output #0: loss = 4.24327 (* 1 = 4.24327 loss)
I0411 13:20:41.576521 10031 sgd_solver.cpp:106] Iteration 57, lr = 1e-06
I0411 13:20:41.956770 10031 solver.cpp:240] Iteration 58, loss = 4.2087
I0411 13:20:41.956802 10031 solver.cpp:256]     Train net output #0: loss = 4.2087 (* 1 = 4.2087 loss)
I0411 13:20:41.956810 10031 sgd_solver.cpp:106] Iteration 58, lr = 1e-06
I0411 13:20:42.335434 10031 solver.cpp:240] Iteration 59, loss = 4.19984
I0411 13:20:42.335467 10031 solver.cpp:256]     Train net output #0: loss = 4.19984 (* 1 = 4.19984 loss)
I0411 13:20:42.335474 10031 sgd_solver.cpp:106] Iteration 59, lr = 1e-06
I0411 13:20:42.709981 10031 solver.cpp:240] Iteration 60, loss = 4.2154
I0411 13:20:42.710014 10031 solver.cpp:256]     Train net output #0: loss = 4.2154 (* 1 = 4.2154 loss)
I0411 13:20:42.710022 10031 sgd_solver.cpp:106] Iteration 60, lr = 1e-06
I0411 13:20:43.088395 10031 solver.cpp:240] Iteration 61, loss = 4.28547
I0411 13:20:43.088439 10031 solver.cpp:256]     Train net output #0: loss = 4.28547 (* 1 = 4.28547 loss)
I0411 13:20:43.088449 10031 sgd_solver.cpp:106] Iteration 61, lr = 1e-06
I0411 13:20:43.471557 10031 solver.cpp:240] Iteration 62, loss = 4.2214
I0411 13:20:43.471593 10031 solver.cpp:256]     Train net output #0: loss = 4.2214 (* 1 = 4.2214 loss)
I0411 13:20:43.471603 10031 sgd_solver.cpp:106] Iteration 62, lr = 1e-06
I0411 13:20:43.851531 10031 solver.cpp:240] Iteration 63, loss = 4.22731
I0411 13:20:43.851563 10031 solver.cpp:256]     Train net output #0: loss = 4.22731 (* 1 = 4.22731 loss)
I0411 13:20:43.851572 10031 sgd_solver.cpp:106] Iteration 63, lr = 1e-06
I0411 13:20:44.229295 10031 solver.cpp:240] Iteration 64, loss = 4.22754
I0411 13:20:44.229327 10031 solver.cpp:256]     Train net output #0: loss = 4.22754 (* 1 = 4.22754 loss)
I0411 13:20:44.229336 10031 sgd_solver.cpp:106] Iteration 64, lr = 1e-06
I0411 13:20:44.605906 10031 solver.cpp:240] Iteration 65, loss = 4.19568
I0411 13:20:44.605950 10031 solver.cpp:256]     Train net output #0: loss = 4.19568 (* 1 = 4.19568 loss)
I0411 13:20:44.605959 10031 sgd_solver.cpp:106] Iteration 65, lr = 1e-06
I0411 13:20:44.986595 10031 solver.cpp:240] Iteration 66, loss = 4.21197
I0411 13:20:44.986769 10031 solver.cpp:256]     Train net output #0: loss = 4.21197 (* 1 = 4.21197 loss)
I0411 13:20:44.986780 10031 sgd_solver.cpp:106] Iteration 66, lr = 1e-06
I0411 13:20:45.364549 10031 solver.cpp:240] Iteration 67, loss = 4.19794
I0411 13:20:45.364583 10031 solver.cpp:256]     Train net output #0: loss = 4.19794 (* 1 = 4.19794 loss)
I0411 13:20:45.364590 10031 sgd_solver.cpp:106] Iteration 67, lr = 1e-06
I0411 13:20:45.740231 10031 solver.cpp:240] Iteration 68, loss = 4.22771
I0411 13:20:45.740262 10031 solver.cpp:256]     Train net output #0: loss = 4.22771 (* 1 = 4.22771 loss)
I0411 13:20:45.740270 10031 sgd_solver.cpp:106] Iteration 68, lr = 1e-06
I0411 13:20:46.118347 10031 solver.cpp:240] Iteration 69, loss = 4.20172
I0411 13:20:46.118382 10031 solver.cpp:256]     Train net output #0: loss = 4.20172 (* 1 = 4.20172 loss)
I0411 13:20:46.118391 10031 sgd_solver.cpp:106] Iteration 69, lr = 1e-06
I0411 13:20:46.500686 10031 solver.cpp:240] Iteration 70, loss = 4.18352
I0411 13:20:46.500720 10031 solver.cpp:256]     Train net output #0: loss = 4.18352 (* 1 = 4.18352 loss)
I0411 13:20:46.500727 10031 sgd_solver.cpp:106] Iteration 70, lr = 1e-06
I0411 13:20:46.878926 10031 solver.cpp:240] Iteration 71, loss = 4.22432
I0411 13:20:46.878958 10031 solver.cpp:256]     Train net output #0: loss = 4.22432 (* 1 = 4.22432 loss)
I0411 13:20:46.878967 10031 sgd_solver.cpp:106] Iteration 71, lr = 1e-06
I0411 13:20:47.255796 10031 solver.cpp:240] Iteration 72, loss = 4.21257
I0411 13:20:47.255831 10031 solver.cpp:256]     Train net output #0: loss = 4.21257 (* 1 = 4.21257 loss)
I0411 13:20:47.255838 10031 sgd_solver.cpp:106] Iteration 72, lr = 1e-06
I0411 13:20:47.637033 10031 solver.cpp:240] Iteration 73, loss = 4.20313
I0411 13:20:47.637066 10031 solver.cpp:256]     Train net output #0: loss = 4.20313 (* 1 = 4.20313 loss)
I0411 13:20:47.637074 10031 sgd_solver.cpp:106] Iteration 73, lr = 1e-06
I0411 13:20:48.018875 10031 solver.cpp:240] Iteration 74, loss = 4.20232
I0411 13:20:48.018908 10031 solver.cpp:256]     Train net output #0: loss = 4.20232 (* 1 = 4.20232 loss)
I0411 13:20:48.018916 10031 sgd_solver.cpp:106] Iteration 74, lr = 1e-06
I0411 13:20:48.019223 10031 solver.cpp:349] Iteration 75, Testing net (#0)
I0411 13:20:49.329538 10031 solver.cpp:416]     Test net output #0: accuracy_1 = 0.034668
I0411 13:20:49.329566 10031 solver.cpp:416]     Test net output #1: accuracy_5 = 0.132568
I0411 13:20:49.329576 10031 solver.cpp:416]     Test net output #2: loss = 4.23204 (* 1 = 4.23204 loss)
I0411 13:20:49.459003 10031 solver.cpp:240] Iteration 75, loss = 4.20557
I0411 13:20:49.459034 10031 solver.cpp:256]     Train net output #0: loss = 4.20557 (* 1 = 4.20557 loss)
I0411 13:20:49.459053 10031 sgd_solver.cpp:106] Iteration 75, lr = 1e-06
I0411 13:20:49.834475 10031 solver.cpp:240] Iteration 76, loss = 4.18361
I0411 13:20:49.834506 10031 solver.cpp:256]     Train net output #0: loss = 4.18361 (* 1 = 4.18361 loss)
I0411 13:20:49.834512 10031 sgd_solver.cpp:106] Iteration 76, lr = 1e-06
I0411 13:20:50.211621 10031 solver.cpp:240] Iteration 77, loss = 4.19432
I0411 13:20:50.211654 10031 solver.cpp:256]     Train net output #0: loss = 4.19432 (* 1 = 4.19432 loss)
I0411 13:20:50.211661 10031 sgd_solver.cpp:106] Iteration 77, lr = 1e-06
I0411 13:20:50.590224 10031 solver.cpp:240] Iteration 78, loss = 4.2262
I0411 13:20:50.590261 10031 solver.cpp:256]     Train net output #0: loss = 4.2262 (* 1 = 4.2262 loss)
I0411 13:20:50.590268 10031 sgd_solver.cpp:106] Iteration 78, lr = 1e-06
I0411 13:20:50.967200 10031 solver.cpp:240] Iteration 79, loss = 4.1848
I0411 13:20:50.967232 10031 solver.cpp:256]     Train net output #0: loss = 4.1848 (* 1 = 4.1848 loss)
I0411 13:20:50.967241 10031 sgd_solver.cpp:106] Iteration 79, lr = 1e-06
I0411 13:20:51.341205 10031 solver.cpp:240] Iteration 80, loss = 4.21896
I0411 13:20:51.341248 10031 solver.cpp:256]     Train net output #0: loss = 4.21896 (* 1 = 4.21896 loss)
I0411 13:20:51.341256 10031 sgd_solver.cpp:106] Iteration 80, lr = 1e-06
I0411 13:20:51.718726 10031 solver.cpp:240] Iteration 81, loss = 4.20062
I0411 13:20:51.718787 10031 solver.cpp:256]     Train net output #0: loss = 4.20062 (* 1 = 4.20062 loss)
I0411 13:20:51.718796 10031 sgd_solver.cpp:106] Iteration 81, lr = 1e-06
I0411 13:20:52.096338 10031 solver.cpp:240] Iteration 82, loss = 4.18601
I0411 13:20:52.096370 10031 solver.cpp:256]     Train net output #0: loss = 4.18601 (* 1 = 4.18601 loss)
I0411 13:20:52.096379 10031 sgd_solver.cpp:106] Iteration 82, lr = 1e-06
I0411 13:20:52.465988 10031 solver.cpp:240] Iteration 83, loss = 4.19578
I0411 13:20:52.466027 10031 solver.cpp:256]     Train net output #0: loss = 4.19578 (* 1 = 4.19578 loss)
I0411 13:20:52.466035 10031 sgd_solver.cpp:106] Iteration 83, lr = 1e-06
I0411 13:20:52.841922 10031 solver.cpp:240] Iteration 84, loss = 4.20073
I0411 13:20:52.841954 10031 solver.cpp:256]     Train net output #0: loss = 4.20073 (* 1 = 4.20073 loss)
I0411 13:20:52.841962 10031 sgd_solver.cpp:106] Iteration 84, lr = 1e-06
I0411 13:20:53.222662 10031 solver.cpp:240] Iteration 85, loss = 4.24473
I0411 13:20:53.222693 10031 solver.cpp:256]     Train net output #0: loss = 4.24473 (* 1 = 4.24473 loss)
I0411 13:20:53.222702 10031 sgd_solver.cpp:106] Iteration 85, lr = 1e-06
I0411 13:20:53.599897 10031 solver.cpp:240] Iteration 86, loss = 4.24472
I0411 13:20:53.599930 10031 solver.cpp:256]     Train net output #0: loss = 4.24472 (* 1 = 4.24472 loss)
I0411 13:20:53.599937 10031 sgd_solver.cpp:106] Iteration 86, lr = 1e-06
I0411 13:20:53.975765 10031 solver.cpp:240] Iteration 87, loss = 4.19004
I0411 13:20:53.975797 10031 solver.cpp:256]     Train net output #0: loss = 4.19004 (* 1 = 4.19004 loss)
I0411 13:20:53.975805 10031 sgd_solver.cpp:106] Iteration 87, lr = 1e-06
I0411 13:20:54.349215 10031 solver.cpp:240] Iteration 88, loss = 4.21402
I0411 13:20:54.349246 10031 solver.cpp:256]     Train net output #0: loss = 4.21402 (* 1 = 4.21402 loss)
I0411 13:20:54.349252 10031 sgd_solver.cpp:106] Iteration 88, lr = 1e-06
I0411 13:20:54.727084 10031 solver.cpp:240] Iteration 89, loss = 4.2282
I0411 13:20:54.727114 10031 solver.cpp:256]     Train net output #0: loss = 4.2282 (* 1 = 4.2282 loss)
I0411 13:20:54.727123 10031 sgd_solver.cpp:106] Iteration 89, lr = 1e-06
I0411 13:20:55.106492 10031 solver.cpp:240] Iteration 90, loss = 4.14991
I0411 13:20:55.106534 10031 solver.cpp:256]     Train net output #0: loss = 4.14991 (* 1 = 4.14991 loss)
I0411 13:20:55.106541 10031 sgd_solver.cpp:106] Iteration 90, lr = 1e-06
I0411 13:20:55.485280 10031 solver.cpp:240] Iteration 91, loss = 4.16194
I0411 13:20:55.485311 10031 solver.cpp:256]     Train net output #0: loss = 4.16194 (* 1 = 4.16194 loss)
I0411 13:20:55.485318 10031 sgd_solver.cpp:106] Iteration 91, lr = 1e-06
I0411 13:20:55.868063 10031 solver.cpp:240] Iteration 92, loss = 4.18532
I0411 13:20:55.868093 10031 solver.cpp:256]     Train net output #0: loss = 4.18532 (* 1 = 4.18532 loss)
I0411 13:20:55.868101 10031 sgd_solver.cpp:106] Iteration 92, lr = 1e-06
