I0411 13:43:01.821914  3145 caffe.cpp:217] Using GPUs 1
I0411 13:43:02.124022  3145 caffe.cpp:222] GPU 1: GeForce GTX 1070
I0411 13:43:03.137217  3145 solver.cpp:60] Initializing solver from parameters: 
train_net: "./Prototxt/experiment_9/rtsd-r1/CoNorm/trial_1/train.prototxt"
test_net: "./Prototxt/experiment_9/rtsd-r1/CoNorm/trial_1/test.prototxt"
test_iter: 8
test_interval: 25
base_lr: 1e-05
display: 1
max_iter: 5000
lr_policy: "step"
gamma: 0.5
momentum: 0.9
weight_decay: 0.0005
stepsize: 500
snapshot: 250
snapshot_prefix: "./snapshots/experiment_9/rtsd-r1/CoNorm/trial_1/snap"
solver_mode: GPU
device_id: 1
train_state {
  level: 0
  stage: ""
}
iter_size: 1
type: "Adam"
I0411 13:43:03.137367  3145 solver.cpp:93] Creating training net from train_net file: ./Prototxt/experiment_9/rtsd-r1/CoNorm/trial_1/train.prototxt
I0411 13:43:03.137676  3145 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_1
I0411 13:43:03.137688  3145 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_5
I0411 13:43:03.137845  3145 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.0039215689
    mirror: false
    crop_size: 48
    mean_value: 132
    mean_value: 132
    mean_value: 131
  }
  data_param {
    source: "../local_data/lmdb/rtsd-r1/CoNorm/train/lmdb"
    batch_size: 1024
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 7
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_prescale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv1_sTanH"
  type: "TanH"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_postscale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 150
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_prescale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv2_sTanH"
  type: "TanH"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_postscale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 250
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_prescale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv3_sTanH"
  type: "TanH"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_postscale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc4_300"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc4_300"
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fc4_prescale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "fc4_sTanH"
  type: "TanH"
  bottom: "fc4_300"
  top: "fc4_300"
}
layer {
  name: "fc4_postscale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "drop4"
  type: "Dropout"
  bottom: "fc4_300"
  top: "fc4_300"
  dropout_param {
    dropout_ratio: 0.6
  }
}
layer {
  name: "fc5_67"
  type: "InnerProduct"
  bottom: "fc4_300"
  top: "fc5_classes"
  inner_product_param {
    num_output: 67
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc5_classes"
  bottom: "label"
  top: "loss"
}
I0411 13:43:03.137979  3145 layer_factory.hpp:77] Creating layer data
I0411 13:43:03.153537  3145 net.cpp:100] Creating Layer data
I0411 13:43:03.153564  3145 net.cpp:408] data -> data
I0411 13:43:03.153599  3145 net.cpp:408] data -> label
I0411 13:43:03.160295  3269 db_lmdb.cpp:35] Opened lmdb ../local_data/lmdb/rtsd-r1/CoNorm/train/lmdb
I0411 13:43:03.177291  3145 data_layer.cpp:41] output data size: 1024,3,48,48
I0411 13:43:03.233376  3145 net.cpp:150] Setting up data
I0411 13:43:03.233412  3145 net.cpp:157] Top shape: 1024 3 48 48 (7077888)
I0411 13:43:03.233420  3145 net.cpp:157] Top shape: 1024 (1024)
I0411 13:43:03.233425  3145 net.cpp:165] Memory required for data: 28315648
I0411 13:43:03.233438  3145 layer_factory.hpp:77] Creating layer conv1
I0411 13:43:03.233469  3145 net.cpp:100] Creating Layer conv1
I0411 13:43:03.233479  3145 net.cpp:434] conv1 <- data
I0411 13:43:03.233496  3145 net.cpp:408] conv1 -> conv1
I0411 13:43:03.928939  3145 net.cpp:150] Setting up conv1
I0411 13:43:03.928984  3145 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 13:43:03.928994  3145 net.cpp:165] Memory required for data: 750850048
I0411 13:43:03.929033  3145 layer_factory.hpp:77] Creating layer conv1_prescale
I0411 13:43:03.929061  3145 net.cpp:100] Creating Layer conv1_prescale
I0411 13:43:03.929074  3145 net.cpp:434] conv1_prescale <- conv1
I0411 13:43:03.929096  3145 net.cpp:395] conv1_prescale -> conv1 (in-place)
I0411 13:43:03.929312  3145 net.cpp:150] Setting up conv1_prescale
I0411 13:43:03.929330  3145 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 13:43:03.929340  3145 net.cpp:165] Memory required for data: 1473384448
I0411 13:43:03.929353  3145 layer_factory.hpp:77] Creating layer conv1_sTanH
I0411 13:43:03.929369  3145 net.cpp:100] Creating Layer conv1_sTanH
I0411 13:43:03.929378  3145 net.cpp:434] conv1_sTanH <- conv1
I0411 13:43:03.929391  3145 net.cpp:395] conv1_sTanH -> conv1 (in-place)
I0411 13:43:03.929767  3145 net.cpp:150] Setting up conv1_sTanH
I0411 13:43:03.929790  3145 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 13:43:03.929800  3145 net.cpp:165] Memory required for data: 2195918848
I0411 13:43:03.929807  3145 layer_factory.hpp:77] Creating layer conv1_postscale
I0411 13:43:03.929827  3145 net.cpp:100] Creating Layer conv1_postscale
I0411 13:43:03.929838  3145 net.cpp:434] conv1_postscale <- conv1
I0411 13:43:03.929853  3145 net.cpp:395] conv1_postscale -> conv1 (in-place)
I0411 13:43:03.930044  3145 net.cpp:150] Setting up conv1_postscale
I0411 13:43:03.930061  3145 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 13:43:03.930070  3145 net.cpp:165] Memory required for data: 2918453248
I0411 13:43:03.930080  3145 layer_factory.hpp:77] Creating layer pool1
I0411 13:43:03.930099  3145 net.cpp:100] Creating Layer pool1
I0411 13:43:03.930109  3145 net.cpp:434] pool1 <- conv1
I0411 13:43:03.930120  3145 net.cpp:408] pool1 -> pool1
I0411 13:43:03.930245  3145 net.cpp:150] Setting up pool1
I0411 13:43:03.930265  3145 net.cpp:157] Top shape: 1024 100 21 21 (45158400)
I0411 13:43:03.930274  3145 net.cpp:165] Memory required for data: 3099086848
I0411 13:43:03.930282  3145 layer_factory.hpp:77] Creating layer conv2
I0411 13:43:03.930305  3145 net.cpp:100] Creating Layer conv2
I0411 13:43:03.930315  3145 net.cpp:434] conv2 <- pool1
I0411 13:43:03.930333  3145 net.cpp:408] conv2 -> conv2
I0411 13:43:03.940001  3145 net.cpp:150] Setting up conv2
I0411 13:43:03.940035  3145 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 13:43:03.940044  3145 net.cpp:165] Memory required for data: 3298152448
I0411 13:43:03.940064  3145 layer_factory.hpp:77] Creating layer conv2_prescale
I0411 13:43:03.940081  3145 net.cpp:100] Creating Layer conv2_prescale
I0411 13:43:03.940091  3145 net.cpp:434] conv2_prescale <- conv2
I0411 13:43:03.940106  3145 net.cpp:395] conv2_prescale -> conv2 (in-place)
I0411 13:43:03.940316  3145 net.cpp:150] Setting up conv2_prescale
I0411 13:43:03.940337  3145 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 13:43:03.940346  3145 net.cpp:165] Memory required for data: 3497218048
I0411 13:43:03.940356  3145 layer_factory.hpp:77] Creating layer conv2_sTanH
I0411 13:43:03.940374  3145 net.cpp:100] Creating Layer conv2_sTanH
I0411 13:43:03.940383  3145 net.cpp:434] conv2_sTanH <- conv2
I0411 13:43:03.940393  3145 net.cpp:395] conv2_sTanH -> conv2 (in-place)
I0411 13:43:03.941484  3145 net.cpp:150] Setting up conv2_sTanH
I0411 13:43:03.941501  3145 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 13:43:03.941505  3145 net.cpp:165] Memory required for data: 3696283648
I0411 13:43:03.941509  3145 layer_factory.hpp:77] Creating layer conv2_postscale
I0411 13:43:03.941519  3145 net.cpp:100] Creating Layer conv2_postscale
I0411 13:43:03.941527  3145 net.cpp:434] conv2_postscale <- conv2
I0411 13:43:03.941537  3145 net.cpp:395] conv2_postscale -> conv2 (in-place)
I0411 13:43:03.941684  3145 net.cpp:150] Setting up conv2_postscale
I0411 13:43:03.941696  3145 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 13:43:03.941700  3145 net.cpp:165] Memory required for data: 3895349248
I0411 13:43:03.941706  3145 layer_factory.hpp:77] Creating layer pool2
I0411 13:43:03.941715  3145 net.cpp:100] Creating Layer pool2
I0411 13:43:03.941718  3145 net.cpp:434] pool2 <- conv2
I0411 13:43:03.941725  3145 net.cpp:408] pool2 -> pool2
I0411 13:43:03.941797  3145 net.cpp:150] Setting up pool2
I0411 13:43:03.941812  3145 net.cpp:157] Top shape: 1024 150 9 9 (12441600)
I0411 13:43:03.941819  3145 net.cpp:165] Memory required for data: 3945115648
I0411 13:43:03.941825  3145 layer_factory.hpp:77] Creating layer conv3
I0411 13:43:03.941843  3145 net.cpp:100] Creating Layer conv3
I0411 13:43:03.941853  3145 net.cpp:434] conv3 <- pool2
I0411 13:43:03.941859  3145 net.cpp:408] conv3 -> conv3
I0411 13:43:03.949766  3145 net.cpp:150] Setting up conv3
I0411 13:43:03.949785  3145 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 13:43:03.949790  3145 net.cpp:165] Memory required for data: 3981979648
I0411 13:43:03.949803  3145 layer_factory.hpp:77] Creating layer conv3_prescale
I0411 13:43:03.949813  3145 net.cpp:100] Creating Layer conv3_prescale
I0411 13:43:03.949817  3145 net.cpp:434] conv3_prescale <- conv3
I0411 13:43:03.949825  3145 net.cpp:395] conv3_prescale -> conv3 (in-place)
I0411 13:43:03.949931  3145 net.cpp:150] Setting up conv3_prescale
I0411 13:43:03.949942  3145 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 13:43:03.949945  3145 net.cpp:165] Memory required for data: 4018843648
I0411 13:43:03.949951  3145 layer_factory.hpp:77] Creating layer conv3_sTanH
I0411 13:43:03.949957  3145 net.cpp:100] Creating Layer conv3_sTanH
I0411 13:43:03.949961  3145 net.cpp:434] conv3_sTanH <- conv3
I0411 13:43:03.949966  3145 net.cpp:395] conv3_sTanH -> conv3 (in-place)
I0411 13:43:03.951122  3145 net.cpp:150] Setting up conv3_sTanH
I0411 13:43:03.951140  3145 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 13:43:03.951144  3145 net.cpp:165] Memory required for data: 4055707648
I0411 13:43:03.951166  3145 layer_factory.hpp:77] Creating layer conv3_postscale
I0411 13:43:03.951175  3145 net.cpp:100] Creating Layer conv3_postscale
I0411 13:43:03.951179  3145 net.cpp:434] conv3_postscale <- conv3
I0411 13:43:03.951191  3145 net.cpp:395] conv3_postscale -> conv3 (in-place)
I0411 13:43:03.951308  3145 net.cpp:150] Setting up conv3_postscale
I0411 13:43:03.951318  3145 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 13:43:03.951320  3145 net.cpp:165] Memory required for data: 4092571648
I0411 13:43:03.951326  3145 layer_factory.hpp:77] Creating layer pool3
I0411 13:43:03.951335  3145 net.cpp:100] Creating Layer pool3
I0411 13:43:03.951339  3145 net.cpp:434] pool3 <- conv3
I0411 13:43:03.951345  3145 net.cpp:408] pool3 -> pool3
I0411 13:43:03.951390  3145 net.cpp:150] Setting up pool3
I0411 13:43:03.951398  3145 net.cpp:157] Top shape: 1024 250 3 3 (2304000)
I0411 13:43:03.951402  3145 net.cpp:165] Memory required for data: 4101787648
I0411 13:43:03.951406  3145 layer_factory.hpp:77] Creating layer fc4_300
I0411 13:43:03.951416  3145 net.cpp:100] Creating Layer fc4_300
I0411 13:43:03.951421  3145 net.cpp:434] fc4_300 <- pool3
I0411 13:43:03.951428  3145 net.cpp:408] fc4_300 -> fc4_300
I0411 13:43:03.959247  3145 net.cpp:150] Setting up fc4_300
I0411 13:43:03.959267  3145 net.cpp:157] Top shape: 1024 300 (307200)
I0411 13:43:03.959271  3145 net.cpp:165] Memory required for data: 4103016448
I0411 13:43:03.959278  3145 layer_factory.hpp:77] Creating layer fc4_prescale
I0411 13:43:03.959286  3145 net.cpp:100] Creating Layer fc4_prescale
I0411 13:43:03.959290  3145 net.cpp:434] fc4_prescale <- fc4_300
I0411 13:43:03.959300  3145 net.cpp:395] fc4_prescale -> fc4_300 (in-place)
I0411 13:43:03.959399  3145 net.cpp:150] Setting up fc4_prescale
I0411 13:43:03.959408  3145 net.cpp:157] Top shape: 1024 300 (307200)
I0411 13:43:03.959413  3145 net.cpp:165] Memory required for data: 4104245248
I0411 13:43:03.959417  3145 layer_factory.hpp:77] Creating layer fc4_sTanH
I0411 13:43:03.959424  3145 net.cpp:100] Creating Layer fc4_sTanH
I0411 13:43:03.959427  3145 net.cpp:434] fc4_sTanH <- fc4_300
I0411 13:43:03.959432  3145 net.cpp:395] fc4_sTanH -> fc4_300 (in-place)
I0411 13:43:03.959642  3145 net.cpp:150] Setting up fc4_sTanH
I0411 13:43:03.959656  3145 net.cpp:157] Top shape: 1024 300 (307200)
I0411 13:43:03.959659  3145 net.cpp:165] Memory required for data: 4105474048
I0411 13:43:03.959663  3145 layer_factory.hpp:77] Creating layer fc4_postscale
I0411 13:43:03.959671  3145 net.cpp:100] Creating Layer fc4_postscale
I0411 13:43:03.959673  3145 net.cpp:434] fc4_postscale <- fc4_300
I0411 13:43:03.959681  3145 net.cpp:395] fc4_postscale -> fc4_300 (in-place)
I0411 13:43:03.959790  3145 net.cpp:150] Setting up fc4_postscale
I0411 13:43:03.959800  3145 net.cpp:157] Top shape: 1024 300 (307200)
I0411 13:43:03.959803  3145 net.cpp:165] Memory required for data: 4106702848
I0411 13:43:03.959808  3145 layer_factory.hpp:77] Creating layer drop4
I0411 13:43:03.959815  3145 net.cpp:100] Creating Layer drop4
I0411 13:43:03.959818  3145 net.cpp:434] drop4 <- fc4_300
I0411 13:43:03.959823  3145 net.cpp:395] drop4 -> fc4_300 (in-place)
I0411 13:43:03.959853  3145 net.cpp:150] Setting up drop4
I0411 13:43:03.959861  3145 net.cpp:157] Top shape: 1024 300 (307200)
I0411 13:43:03.959866  3145 net.cpp:165] Memory required for data: 4107931648
I0411 13:43:03.959868  3145 layer_factory.hpp:77] Creating layer fc5_67
I0411 13:43:03.959877  3145 net.cpp:100] Creating Layer fc5_67
I0411 13:43:03.959887  3145 net.cpp:434] fc5_67 <- fc4_300
I0411 13:43:03.959893  3145 net.cpp:408] fc5_67 -> fc5_classes
I0411 13:43:03.965627  3145 net.cpp:150] Setting up fc5_67
I0411 13:43:03.965646  3145 net.cpp:157] Top shape: 1024 67 (68608)
I0411 13:43:03.965651  3145 net.cpp:165] Memory required for data: 4108206080
I0411 13:43:03.965662  3145 layer_factory.hpp:77] Creating layer loss
I0411 13:43:03.965669  3145 net.cpp:100] Creating Layer loss
I0411 13:43:03.965673  3145 net.cpp:434] loss <- fc5_classes
I0411 13:43:03.965678  3145 net.cpp:434] loss <- label
I0411 13:43:03.965698  3145 net.cpp:408] loss -> loss
I0411 13:43:03.965714  3145 layer_factory.hpp:77] Creating layer loss
I0411 13:43:03.966109  3145 net.cpp:150] Setting up loss
I0411 13:43:03.966123  3145 net.cpp:157] Top shape: (1)
I0411 13:43:03.966127  3145 net.cpp:160]     with loss weight 1
I0411 13:43:03.966146  3145 net.cpp:165] Memory required for data: 4108206084
I0411 13:43:03.966150  3145 net.cpp:226] loss needs backward computation.
I0411 13:43:03.966157  3145 net.cpp:226] fc5_67 needs backward computation.
I0411 13:43:03.966161  3145 net.cpp:226] drop4 needs backward computation.
I0411 13:43:03.966164  3145 net.cpp:226] fc4_postscale needs backward computation.
I0411 13:43:03.966168  3145 net.cpp:226] fc4_sTanH needs backward computation.
I0411 13:43:03.966171  3145 net.cpp:226] fc4_prescale needs backward computation.
I0411 13:43:03.966174  3145 net.cpp:226] fc4_300 needs backward computation.
I0411 13:43:03.966178  3145 net.cpp:226] pool3 needs backward computation.
I0411 13:43:03.966182  3145 net.cpp:226] conv3_postscale needs backward computation.
I0411 13:43:03.966186  3145 net.cpp:226] conv3_sTanH needs backward computation.
I0411 13:43:03.966188  3145 net.cpp:226] conv3_prescale needs backward computation.
I0411 13:43:03.966192  3145 net.cpp:226] conv3 needs backward computation.
I0411 13:43:03.966195  3145 net.cpp:226] pool2 needs backward computation.
I0411 13:43:03.966198  3145 net.cpp:226] conv2_postscale needs backward computation.
I0411 13:43:03.966202  3145 net.cpp:226] conv2_sTanH needs backward computation.
I0411 13:43:03.966205  3145 net.cpp:226] conv2_prescale needs backward computation.
I0411 13:43:03.966208  3145 net.cpp:226] conv2 needs backward computation.
I0411 13:43:03.966212  3145 net.cpp:226] pool1 needs backward computation.
I0411 13:43:03.966215  3145 net.cpp:226] conv1_postscale needs backward computation.
I0411 13:43:03.966219  3145 net.cpp:226] conv1_sTanH needs backward computation.
I0411 13:43:03.966223  3145 net.cpp:226] conv1_prescale needs backward computation.
I0411 13:43:03.966226  3145 net.cpp:226] conv1 needs backward computation.
I0411 13:43:03.966230  3145 net.cpp:228] data does not need backward computation.
I0411 13:43:03.966234  3145 net.cpp:270] This network produces output loss
I0411 13:43:03.966253  3145 net.cpp:283] Network initialization done.
I0411 13:43:03.966560  3145 solver.cpp:193] Creating test net (#0) specified by test_net file: ./Prototxt/experiment_9/rtsd-r1/CoNorm/trial_1/test.prototxt
I0411 13:43:03.966768  3145 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.0039215689
    mirror: false
    crop_size: 48
    mean_value: 133
    mean_value: 133
    mean_value: 132
  }
  data_param {
    source: "../local_data/lmdb/rtsd-r1/CoNorm/test/lmdb"
    batch_size: 1024
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 7
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_prescale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv1_sTanH"
  type: "TanH"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_postscale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 150
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_prescale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv2_sTanH"
  type: "TanH"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_postscale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 250
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_prescale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv3_sTanH"
  type: "TanH"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_postscale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc4_300"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc4_300"
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fc4_prescale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "fc4_sTanH"
  type: "TanH"
  bottom: "fc4_300"
  top: "fc4_300"
}
layer {
  name: "fc4_postscale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "drop4"
  type: "Dropout"
  bottom: "fc4_300"
  top: "fc4_300"
  dropout_param {
    dropout_ratio: 0.6
  }
}
layer {
  name: "fc5_67"
  type: "InnerProduct"
  bottom: "fc4_300"
  top: "fc5_classes"
  inner_product_param {
    num_output: 67
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc5_classes"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy_1"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_1"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 1
  }
}
layer {
  name: "accuracy_5"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0411 13:43:03.966902  3145 layer_factory.hpp:77] Creating layer data
I0411 13:43:03.967638  3145 net.cpp:100] Creating Layer data
I0411 13:43:03.967653  3145 net.cpp:408] data -> data
I0411 13:43:03.967664  3145 net.cpp:408] data -> label
I0411 13:43:03.972548  3342 db_lmdb.cpp:35] Opened lmdb ../local_data/lmdb/rtsd-r1/CoNorm/test/lmdb
I0411 13:43:03.972738  3145 data_layer.cpp:41] output data size: 1024,3,48,48
I0411 13:43:04.016206  3145 net.cpp:150] Setting up data
I0411 13:43:04.016237  3145 net.cpp:157] Top shape: 1024 3 48 48 (7077888)
I0411 13:43:04.016242  3145 net.cpp:157] Top shape: 1024 (1024)
I0411 13:43:04.016244  3145 net.cpp:165] Memory required for data: 28315648
I0411 13:43:04.016250  3145 layer_factory.hpp:77] Creating layer label_data_1_split
I0411 13:43:04.016265  3145 net.cpp:100] Creating Layer label_data_1_split
I0411 13:43:04.016269  3145 net.cpp:434] label_data_1_split <- label
I0411 13:43:04.016283  3145 net.cpp:408] label_data_1_split -> label_data_1_split_0
I0411 13:43:04.016295  3145 net.cpp:408] label_data_1_split -> label_data_1_split_1
I0411 13:43:04.016324  3145 net.cpp:408] label_data_1_split -> label_data_1_split_2
I0411 13:43:04.016424  3145 net.cpp:150] Setting up label_data_1_split
I0411 13:43:04.016436  3145 net.cpp:157] Top shape: 1024 (1024)
I0411 13:43:04.016440  3145 net.cpp:157] Top shape: 1024 (1024)
I0411 13:43:04.016443  3145 net.cpp:157] Top shape: 1024 (1024)
I0411 13:43:04.016446  3145 net.cpp:165] Memory required for data: 28327936
I0411 13:43:04.016450  3145 layer_factory.hpp:77] Creating layer conv1
I0411 13:43:04.016463  3145 net.cpp:100] Creating Layer conv1
I0411 13:43:04.016468  3145 net.cpp:434] conv1 <- data
I0411 13:43:04.016474  3145 net.cpp:408] conv1 -> conv1
I0411 13:43:04.019032  3145 net.cpp:150] Setting up conv1
I0411 13:43:04.019049  3145 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 13:43:04.019053  3145 net.cpp:165] Memory required for data: 750862336
I0411 13:43:04.019065  3145 layer_factory.hpp:77] Creating layer conv1_prescale
I0411 13:43:04.019086  3145 net.cpp:100] Creating Layer conv1_prescale
I0411 13:43:04.019090  3145 net.cpp:434] conv1_prescale <- conv1
I0411 13:43:04.019095  3145 net.cpp:395] conv1_prescale -> conv1 (in-place)
I0411 13:43:04.021427  3145 net.cpp:150] Setting up conv1_prescale
I0411 13:43:04.021443  3145 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 13:43:04.021447  3145 net.cpp:165] Memory required for data: 1473396736
I0411 13:43:04.021456  3145 layer_factory.hpp:77] Creating layer conv1_sTanH
I0411 13:43:04.021466  3145 net.cpp:100] Creating Layer conv1_sTanH
I0411 13:43:04.021471  3145 net.cpp:434] conv1_sTanH <- conv1
I0411 13:43:04.021477  3145 net.cpp:395] conv1_sTanH -> conv1 (in-place)
I0411 13:43:04.021709  3145 net.cpp:150] Setting up conv1_sTanH
I0411 13:43:04.021723  3145 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 13:43:04.021728  3145 net.cpp:165] Memory required for data: 2195931136
I0411 13:43:04.021730  3145 layer_factory.hpp:77] Creating layer conv1_postscale
I0411 13:43:04.021736  3145 net.cpp:100] Creating Layer conv1_postscale
I0411 13:43:04.021740  3145 net.cpp:434] conv1_postscale <- conv1
I0411 13:43:04.021746  3145 net.cpp:395] conv1_postscale -> conv1 (in-place)
I0411 13:43:04.021862  3145 net.cpp:150] Setting up conv1_postscale
I0411 13:43:04.021869  3145 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 13:43:04.021872  3145 net.cpp:165] Memory required for data: 2918465536
I0411 13:43:04.021878  3145 layer_factory.hpp:77] Creating layer pool1
I0411 13:43:04.021885  3145 net.cpp:100] Creating Layer pool1
I0411 13:43:04.021890  3145 net.cpp:434] pool1 <- conv1
I0411 13:43:04.021898  3145 net.cpp:408] pool1 -> pool1
I0411 13:43:04.021942  3145 net.cpp:150] Setting up pool1
I0411 13:43:04.021950  3145 net.cpp:157] Top shape: 1024 100 21 21 (45158400)
I0411 13:43:04.021953  3145 net.cpp:165] Memory required for data: 3099099136
I0411 13:43:04.021957  3145 layer_factory.hpp:77] Creating layer conv2
I0411 13:43:04.021967  3145 net.cpp:100] Creating Layer conv2
I0411 13:43:04.021972  3145 net.cpp:434] conv2 <- pool1
I0411 13:43:04.021980  3145 net.cpp:408] conv2 -> conv2
I0411 13:43:04.026456  3145 net.cpp:150] Setting up conv2
I0411 13:43:04.026476  3145 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 13:43:04.026480  3145 net.cpp:165] Memory required for data: 3298164736
I0411 13:43:04.026494  3145 layer_factory.hpp:77] Creating layer conv2_prescale
I0411 13:43:04.026504  3145 net.cpp:100] Creating Layer conv2_prescale
I0411 13:43:04.026507  3145 net.cpp:434] conv2_prescale <- conv2
I0411 13:43:04.026517  3145 net.cpp:395] conv2_prescale -> conv2 (in-place)
I0411 13:43:04.026638  3145 net.cpp:150] Setting up conv2_prescale
I0411 13:43:04.026648  3145 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 13:43:04.026650  3145 net.cpp:165] Memory required for data: 3497230336
I0411 13:43:04.026655  3145 layer_factory.hpp:77] Creating layer conv2_sTanH
I0411 13:43:04.026660  3145 net.cpp:100] Creating Layer conv2_sTanH
I0411 13:43:04.026664  3145 net.cpp:434] conv2_sTanH <- conv2
I0411 13:43:04.026669  3145 net.cpp:395] conv2_sTanH -> conv2 (in-place)
I0411 13:43:04.027479  3145 net.cpp:150] Setting up conv2_sTanH
I0411 13:43:04.027493  3145 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 13:43:04.027498  3145 net.cpp:165] Memory required for data: 3696295936
I0411 13:43:04.027500  3145 layer_factory.hpp:77] Creating layer conv2_postscale
I0411 13:43:04.027509  3145 net.cpp:100] Creating Layer conv2_postscale
I0411 13:43:04.027513  3145 net.cpp:434] conv2_postscale <- conv2
I0411 13:43:04.027519  3145 net.cpp:395] conv2_postscale -> conv2 (in-place)
I0411 13:43:04.027626  3145 net.cpp:150] Setting up conv2_postscale
I0411 13:43:04.027636  3145 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 13:43:04.027638  3145 net.cpp:165] Memory required for data: 3895361536
I0411 13:43:04.027643  3145 layer_factory.hpp:77] Creating layer pool2
I0411 13:43:04.027652  3145 net.cpp:100] Creating Layer pool2
I0411 13:43:04.027657  3145 net.cpp:434] pool2 <- conv2
I0411 13:43:04.027662  3145 net.cpp:408] pool2 -> pool2
I0411 13:43:04.027709  3145 net.cpp:150] Setting up pool2
I0411 13:43:04.027719  3145 net.cpp:157] Top shape: 1024 150 9 9 (12441600)
I0411 13:43:04.027721  3145 net.cpp:165] Memory required for data: 3945127936
I0411 13:43:04.027724  3145 layer_factory.hpp:77] Creating layer conv3
I0411 13:43:04.027734  3145 net.cpp:100] Creating Layer conv3
I0411 13:43:04.027740  3145 net.cpp:434] conv3 <- pool2
I0411 13:43:04.027747  3145 net.cpp:408] conv3 -> conv3
I0411 13:43:04.033340  3145 net.cpp:150] Setting up conv3
I0411 13:43:04.033360  3145 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 13:43:04.033363  3145 net.cpp:165] Memory required for data: 3981991936
I0411 13:43:04.033373  3145 layer_factory.hpp:77] Creating layer conv3_prescale
I0411 13:43:04.033381  3145 net.cpp:100] Creating Layer conv3_prescale
I0411 13:43:04.033386  3145 net.cpp:434] conv3_prescale <- conv3
I0411 13:43:04.033392  3145 net.cpp:395] conv3_prescale -> conv3 (in-place)
I0411 13:43:04.033491  3145 net.cpp:150] Setting up conv3_prescale
I0411 13:43:04.033500  3145 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 13:43:04.033504  3145 net.cpp:165] Memory required for data: 4018855936
I0411 13:43:04.033509  3145 layer_factory.hpp:77] Creating layer conv3_sTanH
I0411 13:43:04.033514  3145 net.cpp:100] Creating Layer conv3_sTanH
I0411 13:43:04.033516  3145 net.cpp:434] conv3_sTanH <- conv3
I0411 13:43:04.033522  3145 net.cpp:395] conv3_sTanH -> conv3 (in-place)
I0411 13:43:04.039948  3145 net.cpp:150] Setting up conv3_sTanH
I0411 13:43:04.039968  3145 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 13:43:04.039971  3145 net.cpp:165] Memory required for data: 4055719936
I0411 13:43:04.039975  3145 layer_factory.hpp:77] Creating layer conv3_postscale
I0411 13:43:04.039983  3145 net.cpp:100] Creating Layer conv3_postscale
I0411 13:43:04.039988  3145 net.cpp:434] conv3_postscale <- conv3
I0411 13:43:04.039994  3145 net.cpp:395] conv3_postscale -> conv3 (in-place)
I0411 13:43:04.040104  3145 net.cpp:150] Setting up conv3_postscale
I0411 13:43:04.040114  3145 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 13:43:04.040117  3145 net.cpp:165] Memory required for data: 4092583936
I0411 13:43:04.040123  3145 layer_factory.hpp:77] Creating layer pool3
I0411 13:43:04.040133  3145 net.cpp:100] Creating Layer pool3
I0411 13:43:04.040138  3145 net.cpp:434] pool3 <- conv3
I0411 13:43:04.040144  3145 net.cpp:408] pool3 -> pool3
I0411 13:43:04.040189  3145 net.cpp:150] Setting up pool3
I0411 13:43:04.040195  3145 net.cpp:157] Top shape: 1024 250 3 3 (2304000)
I0411 13:43:04.040200  3145 net.cpp:165] Memory required for data: 4101799936
I0411 13:43:04.040204  3145 layer_factory.hpp:77] Creating layer fc4_300
I0411 13:43:04.040210  3145 net.cpp:100] Creating Layer fc4_300
I0411 13:43:04.040213  3145 net.cpp:434] fc4_300 <- pool3
I0411 13:43:04.040220  3145 net.cpp:408] fc4_300 -> fc4_300
I0411 13:43:04.045828  3145 net.cpp:150] Setting up fc4_300
I0411 13:43:04.045843  3145 net.cpp:157] Top shape: 1024 300 (307200)
I0411 13:43:04.045847  3145 net.cpp:165] Memory required for data: 4103028736
I0411 13:43:04.045869  3145 layer_factory.hpp:77] Creating layer fc4_prescale
I0411 13:43:04.045879  3145 net.cpp:100] Creating Layer fc4_prescale
I0411 13:43:04.045883  3145 net.cpp:434] fc4_prescale <- fc4_300
I0411 13:43:04.045891  3145 net.cpp:395] fc4_prescale -> fc4_300 (in-place)
I0411 13:43:04.045989  3145 net.cpp:150] Setting up fc4_prescale
I0411 13:43:04.045999  3145 net.cpp:157] Top shape: 1024 300 (307200)
I0411 13:43:04.046001  3145 net.cpp:165] Memory required for data: 4104257536
I0411 13:43:04.046006  3145 layer_factory.hpp:77] Creating layer fc4_sTanH
I0411 13:43:04.046013  3145 net.cpp:100] Creating Layer fc4_sTanH
I0411 13:43:04.046016  3145 net.cpp:434] fc4_sTanH <- fc4_300
I0411 13:43:04.046021  3145 net.cpp:395] fc4_sTanH -> fc4_300 (in-place)
I0411 13:43:04.046214  3145 net.cpp:150] Setting up fc4_sTanH
I0411 13:43:04.046226  3145 net.cpp:157] Top shape: 1024 300 (307200)
I0411 13:43:04.046228  3145 net.cpp:165] Memory required for data: 4105486336
I0411 13:43:04.046233  3145 layer_factory.hpp:77] Creating layer fc4_postscale
I0411 13:43:04.046241  3145 net.cpp:100] Creating Layer fc4_postscale
I0411 13:43:04.046244  3145 net.cpp:434] fc4_postscale <- fc4_300
I0411 13:43:04.046252  3145 net.cpp:395] fc4_postscale -> fc4_300 (in-place)
I0411 13:43:04.046355  3145 net.cpp:150] Setting up fc4_postscale
I0411 13:43:04.046363  3145 net.cpp:157] Top shape: 1024 300 (307200)
I0411 13:43:04.046366  3145 net.cpp:165] Memory required for data: 4106715136
I0411 13:43:04.046370  3145 layer_factory.hpp:77] Creating layer drop4
I0411 13:43:04.046378  3145 net.cpp:100] Creating Layer drop4
I0411 13:43:04.046383  3145 net.cpp:434] drop4 <- fc4_300
I0411 13:43:04.046389  3145 net.cpp:395] drop4 -> fc4_300 (in-place)
I0411 13:43:04.046416  3145 net.cpp:150] Setting up drop4
I0411 13:43:04.046422  3145 net.cpp:157] Top shape: 1024 300 (307200)
I0411 13:43:04.046424  3145 net.cpp:165] Memory required for data: 4107943936
I0411 13:43:04.046427  3145 layer_factory.hpp:77] Creating layer fc5_67
I0411 13:43:04.046433  3145 net.cpp:100] Creating Layer fc5_67
I0411 13:43:04.046445  3145 net.cpp:434] fc5_67 <- fc4_300
I0411 13:43:04.046452  3145 net.cpp:408] fc5_67 -> fc5_classes
I0411 13:43:04.046705  3145 net.cpp:150] Setting up fc5_67
I0411 13:43:04.046715  3145 net.cpp:157] Top shape: 1024 67 (68608)
I0411 13:43:04.046717  3145 net.cpp:165] Memory required for data: 4108218368
I0411 13:43:04.046726  3145 layer_factory.hpp:77] Creating layer fc5_classes_fc5_67_0_split
I0411 13:43:04.046733  3145 net.cpp:100] Creating Layer fc5_classes_fc5_67_0_split
I0411 13:43:04.046737  3145 net.cpp:434] fc5_classes_fc5_67_0_split <- fc5_classes
I0411 13:43:04.046742  3145 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_0
I0411 13:43:04.046751  3145 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_1
I0411 13:43:04.046756  3145 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_2
I0411 13:43:04.046813  3145 net.cpp:150] Setting up fc5_classes_fc5_67_0_split
I0411 13:43:04.046819  3145 net.cpp:157] Top shape: 1024 67 (68608)
I0411 13:43:04.046823  3145 net.cpp:157] Top shape: 1024 67 (68608)
I0411 13:43:04.046826  3145 net.cpp:157] Top shape: 1024 67 (68608)
I0411 13:43:04.046829  3145 net.cpp:165] Memory required for data: 4109041664
I0411 13:43:04.046833  3145 layer_factory.hpp:77] Creating layer loss
I0411 13:43:04.046838  3145 net.cpp:100] Creating Layer loss
I0411 13:43:04.046841  3145 net.cpp:434] loss <- fc5_classes_fc5_67_0_split_0
I0411 13:43:04.046859  3145 net.cpp:434] loss <- label_data_1_split_0
I0411 13:43:04.046864  3145 net.cpp:408] loss -> loss
I0411 13:43:04.046876  3145 layer_factory.hpp:77] Creating layer loss
I0411 13:43:04.047220  3145 net.cpp:150] Setting up loss
I0411 13:43:04.047232  3145 net.cpp:157] Top shape: (1)
I0411 13:43:04.047235  3145 net.cpp:160]     with loss weight 1
I0411 13:43:04.047246  3145 net.cpp:165] Memory required for data: 4109041668
I0411 13:43:04.047250  3145 layer_factory.hpp:77] Creating layer accuracy_1
I0411 13:43:04.047268  3145 net.cpp:100] Creating Layer accuracy_1
I0411 13:43:04.047274  3145 net.cpp:434] accuracy_1 <- fc5_classes_fc5_67_0_split_1
I0411 13:43:04.047279  3145 net.cpp:434] accuracy_1 <- label_data_1_split_1
I0411 13:43:04.047286  3145 net.cpp:408] accuracy_1 -> accuracy_1
I0411 13:43:04.047297  3145 net.cpp:150] Setting up accuracy_1
I0411 13:43:04.047302  3145 net.cpp:157] Top shape: (1)
I0411 13:43:04.047305  3145 net.cpp:165] Memory required for data: 4109041672
I0411 13:43:04.047309  3145 layer_factory.hpp:77] Creating layer accuracy_5
I0411 13:43:04.047317  3145 net.cpp:100] Creating Layer accuracy_5
I0411 13:43:04.047322  3145 net.cpp:434] accuracy_5 <- fc5_classes_fc5_67_0_split_2
I0411 13:43:04.047327  3145 net.cpp:434] accuracy_5 <- label_data_1_split_2
I0411 13:43:04.047332  3145 net.cpp:408] accuracy_5 -> accuracy_5
I0411 13:43:04.047338  3145 net.cpp:150] Setting up accuracy_5
I0411 13:43:04.047343  3145 net.cpp:157] Top shape: (1)
I0411 13:43:04.047345  3145 net.cpp:165] Memory required for data: 4109041676
I0411 13:43:04.047348  3145 net.cpp:228] accuracy_5 does not need backward computation.
I0411 13:43:04.047353  3145 net.cpp:228] accuracy_1 does not need backward computation.
I0411 13:43:04.047356  3145 net.cpp:226] loss needs backward computation.
I0411 13:43:04.047360  3145 net.cpp:226] fc5_classes_fc5_67_0_split needs backward computation.
I0411 13:43:04.047363  3145 net.cpp:226] fc5_67 needs backward computation.
I0411 13:43:04.047366  3145 net.cpp:226] drop4 needs backward computation.
I0411 13:43:04.047369  3145 net.cpp:226] fc4_postscale needs backward computation.
I0411 13:43:04.047372  3145 net.cpp:226] fc4_sTanH needs backward computation.
I0411 13:43:04.047374  3145 net.cpp:226] fc4_prescale needs backward computation.
I0411 13:43:04.047377  3145 net.cpp:226] fc4_300 needs backward computation.
I0411 13:43:04.047380  3145 net.cpp:226] pool3 needs backward computation.
I0411 13:43:04.047384  3145 net.cpp:226] conv3_postscale needs backward computation.
I0411 13:43:04.047386  3145 net.cpp:226] conv3_sTanH needs backward computation.
I0411 13:43:04.047389  3145 net.cpp:226] conv3_prescale needs backward computation.
I0411 13:43:04.047391  3145 net.cpp:226] conv3 needs backward computation.
I0411 13:43:04.047395  3145 net.cpp:226] pool2 needs backward computation.
I0411 13:43:04.047399  3145 net.cpp:226] conv2_postscale needs backward computation.
I0411 13:43:04.047401  3145 net.cpp:226] conv2_sTanH needs backward computation.
I0411 13:43:04.047405  3145 net.cpp:226] conv2_prescale needs backward computation.
I0411 13:43:04.047406  3145 net.cpp:226] conv2 needs backward computation.
I0411 13:43:04.047410  3145 net.cpp:226] pool1 needs backward computation.
I0411 13:43:04.047413  3145 net.cpp:226] conv1_postscale needs backward computation.
I0411 13:43:04.047417  3145 net.cpp:226] conv1_sTanH needs backward computation.
I0411 13:43:04.047420  3145 net.cpp:226] conv1_prescale needs backward computation.
I0411 13:43:04.047423  3145 net.cpp:226] conv1 needs backward computation.
I0411 13:43:04.047427  3145 net.cpp:228] label_data_1_split does not need backward computation.
I0411 13:43:04.047430  3145 net.cpp:228] data does not need backward computation.
I0411 13:43:04.047433  3145 net.cpp:270] This network produces output accuracy_1
I0411 13:43:04.047437  3145 net.cpp:270] This network produces output accuracy_5
I0411 13:43:04.047441  3145 net.cpp:270] This network produces output loss
I0411 13:43:04.047462  3145 net.cpp:283] Network initialization done.
I0411 13:43:04.047540  3145 solver.cpp:72] Solver scaffolding done.
I0411 13:43:04.048465  3145 caffe.cpp:251] Starting Optimization
I0411 13:43:04.048475  3145 solver.cpp:291] Solving 
I0411 13:43:04.048477  3145 solver.cpp:292] Learning Rate Policy: step
I0411 13:43:04.054474  3145 solver.cpp:349] Iteration 0, Testing net (#0)
I0411 13:43:04.061092  3145 blocking_queue.cpp:50] Data layer prefetch queue empty
I0411 13:43:05.181571  3145 solver.cpp:416]     Test net output #0: accuracy_1 = 0.00415039
I0411 13:43:05.181623  3145 solver.cpp:416]     Test net output #1: accuracy_5 = 0.0384521
I0411 13:43:05.181638  3145 solver.cpp:416]     Test net output #2: loss = 4.32762 (* 1 = 4.32762 loss)
I0411 13:43:05.347280  3145 solver.cpp:240] Iteration 0, loss = 4.5796
I0411 13:43:05.347313  3145 solver.cpp:256]     Train net output #0: loss = 4.5796 (* 1 = 4.5796 loss)
I0411 13:43:05.347326  3145 sgd_solver.cpp:106] Iteration 0, lr = 1e-05
I0411 13:43:05.720630  3145 solver.cpp:240] Iteration 1, loss = 4.5504
I0411 13:43:05.720679  3145 solver.cpp:256]     Train net output #0: loss = 4.5504 (* 1 = 4.5504 loss)
I0411 13:43:05.720688  3145 sgd_solver.cpp:106] Iteration 1, lr = 1e-05
I0411 13:43:06.096052  3145 solver.cpp:240] Iteration 2, loss = 4.56085
I0411 13:43:06.096088  3145 solver.cpp:256]     Train net output #0: loss = 4.56085 (* 1 = 4.56085 loss)
I0411 13:43:06.096096  3145 sgd_solver.cpp:106] Iteration 2, lr = 1e-05
I0411 13:43:06.471241  3145 solver.cpp:240] Iteration 3, loss = 4.60009
I0411 13:43:06.471273  3145 solver.cpp:256]     Train net output #0: loss = 4.60009 (* 1 = 4.60009 loss)
I0411 13:43:06.471282  3145 sgd_solver.cpp:106] Iteration 3, lr = 1e-05
I0411 13:43:06.846534  3145 solver.cpp:240] Iteration 4, loss = 4.55714
I0411 13:43:06.846566  3145 solver.cpp:256]     Train net output #0: loss = 4.55714 (* 1 = 4.55714 loss)
I0411 13:43:06.846575  3145 sgd_solver.cpp:106] Iteration 4, lr = 1e-05
I0411 13:43:07.221354  3145 solver.cpp:240] Iteration 5, loss = 4.60375
I0411 13:43:07.221410  3145 solver.cpp:256]     Train net output #0: loss = 4.60375 (* 1 = 4.60375 loss)
I0411 13:43:07.221428  3145 sgd_solver.cpp:106] Iteration 5, lr = 1e-05
I0411 13:43:07.594425  3145 solver.cpp:240] Iteration 6, loss = 4.56731
I0411 13:43:07.594458  3145 solver.cpp:256]     Train net output #0: loss = 4.56731 (* 1 = 4.56731 loss)
I0411 13:43:07.594466  3145 sgd_solver.cpp:106] Iteration 6, lr = 1e-05
I0411 13:43:07.968708  3145 solver.cpp:240] Iteration 7, loss = 4.59658
I0411 13:43:07.968740  3145 solver.cpp:256]     Train net output #0: loss = 4.59658 (* 1 = 4.59658 loss)
I0411 13:43:07.968749  3145 sgd_solver.cpp:106] Iteration 7, lr = 1e-05
I0411 13:43:08.347216  3145 solver.cpp:240] Iteration 8, loss = 4.58478
I0411 13:43:08.347249  3145 solver.cpp:256]     Train net output #0: loss = 4.58478 (* 1 = 4.58478 loss)
I0411 13:43:08.347256  3145 sgd_solver.cpp:106] Iteration 8, lr = 1e-05
I0411 13:43:08.723017  3145 solver.cpp:240] Iteration 9, loss = 4.58906
I0411 13:43:08.723055  3145 solver.cpp:256]     Train net output #0: loss = 4.58906 (* 1 = 4.58906 loss)
I0411 13:43:08.723064  3145 sgd_solver.cpp:106] Iteration 9, lr = 1e-05
I0411 13:43:09.097987  3145 solver.cpp:240] Iteration 10, loss = 4.574
I0411 13:43:09.098024  3145 solver.cpp:256]     Train net output #0: loss = 4.574 (* 1 = 4.574 loss)
I0411 13:43:09.098036  3145 sgd_solver.cpp:106] Iteration 10, lr = 1e-05
I0411 13:43:09.472380  3145 solver.cpp:240] Iteration 11, loss = 4.59084
I0411 13:43:09.472424  3145 solver.cpp:256]     Train net output #0: loss = 4.59084 (* 1 = 4.59084 loss)
I0411 13:43:09.472432  3145 sgd_solver.cpp:106] Iteration 11, lr = 1e-05
I0411 13:43:09.844867  3145 solver.cpp:240] Iteration 12, loss = 4.62821
I0411 13:43:09.844899  3145 solver.cpp:256]     Train net output #0: loss = 4.62821 (* 1 = 4.62821 loss)
I0411 13:43:09.844908  3145 sgd_solver.cpp:106] Iteration 12, lr = 1e-05
I0411 13:43:10.220011  3145 solver.cpp:240] Iteration 13, loss = 4.68037
I0411 13:43:10.220043  3145 solver.cpp:256]     Train net output #0: loss = 4.68037 (* 1 = 4.68037 loss)
I0411 13:43:10.220052  3145 sgd_solver.cpp:106] Iteration 13, lr = 1e-05
I0411 13:43:10.595409  3145 solver.cpp:240] Iteration 14, loss = 4.66199
I0411 13:43:10.595444  3145 solver.cpp:256]     Train net output #0: loss = 4.66199 (* 1 = 4.66199 loss)
I0411 13:43:10.595453  3145 sgd_solver.cpp:106] Iteration 14, lr = 1e-05
I0411 13:43:10.971472  3145 solver.cpp:240] Iteration 15, loss = 4.6599
I0411 13:43:10.971504  3145 solver.cpp:256]     Train net output #0: loss = 4.6599 (* 1 = 4.6599 loss)
I0411 13:43:10.971541  3145 sgd_solver.cpp:106] Iteration 15, lr = 1e-05
I0411 13:43:11.346628  3145 solver.cpp:240] Iteration 16, loss = 4.66592
I0411 13:43:11.346660  3145 solver.cpp:256]     Train net output #0: loss = 4.66592 (* 1 = 4.66592 loss)
I0411 13:43:11.346668  3145 sgd_solver.cpp:106] Iteration 16, lr = 1e-05
I0411 13:43:11.723922  3145 solver.cpp:240] Iteration 17, loss = 4.66079
I0411 13:43:11.723954  3145 solver.cpp:256]     Train net output #0: loss = 4.66079 (* 1 = 4.66079 loss)
I0411 13:43:11.723963  3145 sgd_solver.cpp:106] Iteration 17, lr = 1e-05
I0411 13:43:12.099436  3145 solver.cpp:240] Iteration 18, loss = 4.67733
I0411 13:43:12.099467  3145 solver.cpp:256]     Train net output #0: loss = 4.67733 (* 1 = 4.67733 loss)
I0411 13:43:12.099474  3145 sgd_solver.cpp:106] Iteration 18, lr = 1e-05
I0411 13:43:12.475160  3145 solver.cpp:240] Iteration 19, loss = 4.69917
I0411 13:43:12.475194  3145 solver.cpp:256]     Train net output #0: loss = 4.69917 (* 1 = 4.69917 loss)
I0411 13:43:12.475203  3145 sgd_solver.cpp:106] Iteration 19, lr = 1e-05
I0411 13:43:12.850502  3145 solver.cpp:240] Iteration 20, loss = 4.6792
I0411 13:43:12.850535  3145 solver.cpp:256]     Train net output #0: loss = 4.6792 (* 1 = 4.6792 loss)
I0411 13:43:12.850543  3145 sgd_solver.cpp:106] Iteration 20, lr = 1e-05
I0411 13:43:13.228348  3145 solver.cpp:240] Iteration 21, loss = 4.70939
I0411 13:43:13.228394  3145 solver.cpp:256]     Train net output #0: loss = 4.70939 (* 1 = 4.70939 loss)
I0411 13:43:13.228404  3145 sgd_solver.cpp:106] Iteration 21, lr = 1e-05
I0411 13:43:13.603552  3145 solver.cpp:240] Iteration 22, loss = 4.70323
I0411 13:43:13.603586  3145 solver.cpp:256]     Train net output #0: loss = 4.70323 (* 1 = 4.70323 loss)
I0411 13:43:13.603595  3145 sgd_solver.cpp:106] Iteration 22, lr = 1e-05
I0411 13:43:13.979549  3145 solver.cpp:240] Iteration 23, loss = 4.73363
I0411 13:43:13.979583  3145 solver.cpp:256]     Train net output #0: loss = 4.73363 (* 1 = 4.73363 loss)
I0411 13:43:13.979590  3145 sgd_solver.cpp:106] Iteration 23, lr = 1e-05
I0411 13:43:14.357970  3145 solver.cpp:240] Iteration 24, loss = 4.72933
I0411 13:43:14.358003  3145 solver.cpp:256]     Train net output #0: loss = 4.72933 (* 1 = 4.72933 loss)
I0411 13:43:14.358011  3145 sgd_solver.cpp:106] Iteration 24, lr = 1e-05
I0411 13:43:14.358319  3145 solver.cpp:349] Iteration 25, Testing net (#0)
I0411 13:43:15.666687  3145 solver.cpp:416]     Test net output #0: accuracy_1 = 0.0012207
I0411 13:43:15.666715  3145 solver.cpp:416]     Test net output #1: accuracy_5 = 0.0367432
I0411 13:43:15.666725  3145 solver.cpp:416]     Test net output #2: loss = 4.3399 (* 1 = 4.3399 loss)
I0411 13:43:15.795773  3145 solver.cpp:240] Iteration 25, loss = 4.76665
I0411 13:43:15.795806  3145 solver.cpp:256]     Train net output #0: loss = 4.76665 (* 1 = 4.76665 loss)
I0411 13:43:15.795815  3145 sgd_solver.cpp:106] Iteration 25, lr = 1e-05
I0411 13:43:16.170459  3145 solver.cpp:240] Iteration 26, loss = 4.67672
I0411 13:43:16.170492  3145 solver.cpp:256]     Train net output #0: loss = 4.67672 (* 1 = 4.67672 loss)
I0411 13:43:16.170500  3145 sgd_solver.cpp:106] Iteration 26, lr = 1e-05
I0411 13:43:16.547399  3145 solver.cpp:240] Iteration 27, loss = 4.79282
I0411 13:43:16.547430  3145 solver.cpp:256]     Train net output #0: loss = 4.79282 (* 1 = 4.79282 loss)
I0411 13:43:16.547439  3145 sgd_solver.cpp:106] Iteration 27, lr = 1e-05
I0411 13:43:16.923702  3145 solver.cpp:240] Iteration 28, loss = 4.83582
I0411 13:43:16.923737  3145 solver.cpp:256]     Train net output #0: loss = 4.83582 (* 1 = 4.83582 loss)
I0411 13:43:16.923744  3145 sgd_solver.cpp:106] Iteration 28, lr = 1e-05
I0411 13:43:17.300541  3145 solver.cpp:240] Iteration 29, loss = 4.84242
I0411 13:43:17.300575  3145 solver.cpp:256]     Train net output #0: loss = 4.84242 (* 1 = 4.84242 loss)
I0411 13:43:17.300582  3145 sgd_solver.cpp:106] Iteration 29, lr = 1e-05
I0411 13:43:17.676100  3145 solver.cpp:240] Iteration 30, loss = 4.81563
I0411 13:43:17.676129  3145 solver.cpp:256]     Train net output #0: loss = 4.81563 (* 1 = 4.81563 loss)
I0411 13:43:17.676162  3145 sgd_solver.cpp:106] Iteration 30, lr = 1e-05
I0411 13:43:18.051213  3145 solver.cpp:240] Iteration 31, loss = 4.88562
I0411 13:43:18.051244  3145 solver.cpp:256]     Train net output #0: loss = 4.88562 (* 1 = 4.88562 loss)
I0411 13:43:18.051254  3145 sgd_solver.cpp:106] Iteration 31, lr = 1e-05
I0411 13:43:18.428638  3145 solver.cpp:240] Iteration 32, loss = 4.88768
I0411 13:43:18.428669  3145 solver.cpp:256]     Train net output #0: loss = 4.88768 (* 1 = 4.88768 loss)
I0411 13:43:18.428678  3145 sgd_solver.cpp:106] Iteration 32, lr = 1e-05
I0411 13:43:18.805897  3145 solver.cpp:240] Iteration 33, loss = 4.91497
I0411 13:43:18.805930  3145 solver.cpp:256]     Train net output #0: loss = 4.91497 (* 1 = 4.91497 loss)
I0411 13:43:18.805939  3145 sgd_solver.cpp:106] Iteration 33, lr = 1e-05
I0411 13:43:19.181383  3145 solver.cpp:240] Iteration 34, loss = 4.99416
I0411 13:43:19.181416  3145 solver.cpp:256]     Train net output #0: loss = 4.99416 (* 1 = 4.99416 loss)
I0411 13:43:19.181423  3145 sgd_solver.cpp:106] Iteration 34, lr = 1e-05
I0411 13:43:19.556648  3145 solver.cpp:240] Iteration 35, loss = 4.87161
I0411 13:43:19.556685  3145 solver.cpp:256]     Train net output #0: loss = 4.87161 (* 1 = 4.87161 loss)
I0411 13:43:19.556694  3145 sgd_solver.cpp:106] Iteration 35, lr = 1e-05
I0411 13:43:19.933665  3145 solver.cpp:240] Iteration 36, loss = 4.85272
I0411 13:43:19.933697  3145 solver.cpp:256]     Train net output #0: loss = 4.85272 (* 1 = 4.85272 loss)
I0411 13:43:19.933704  3145 sgd_solver.cpp:106] Iteration 36, lr = 1e-05
I0411 13:43:20.311478  3145 solver.cpp:240] Iteration 37, loss = 4.99244
I0411 13:43:20.311514  3145 solver.cpp:256]     Train net output #0: loss = 4.99244 (* 1 = 4.99244 loss)
I0411 13:43:20.311522  3145 sgd_solver.cpp:106] Iteration 37, lr = 1e-05
I0411 13:43:20.687722  3145 solver.cpp:240] Iteration 38, loss = 5.12079
I0411 13:43:20.687757  3145 solver.cpp:256]     Train net output #0: loss = 5.12079 (* 1 = 5.12079 loss)
I0411 13:43:20.687764  3145 sgd_solver.cpp:106] Iteration 38, lr = 1e-05
I0411 13:43:21.062158  3145 solver.cpp:240] Iteration 39, loss = 4.97098
I0411 13:43:21.062202  3145 solver.cpp:256]     Train net output #0: loss = 4.97098 (* 1 = 4.97098 loss)
I0411 13:43:21.062211  3145 sgd_solver.cpp:106] Iteration 39, lr = 1e-05
I0411 13:43:21.439435  3145 solver.cpp:240] Iteration 40, loss = 5.02851
I0411 13:43:21.439467  3145 solver.cpp:256]     Train net output #0: loss = 5.02851 (* 1 = 5.02851 loss)
I0411 13:43:21.439476  3145 sgd_solver.cpp:106] Iteration 40, lr = 1e-05
I0411 13:43:21.817067  3145 solver.cpp:240] Iteration 41, loss = 5.09871
I0411 13:43:21.817100  3145 solver.cpp:256]     Train net output #0: loss = 5.09871 (* 1 = 5.09871 loss)
I0411 13:43:21.817108  3145 sgd_solver.cpp:106] Iteration 41, lr = 1e-05
I0411 13:43:22.191121  3145 solver.cpp:240] Iteration 42, loss = 5.12133
I0411 13:43:22.191166  3145 solver.cpp:256]     Train net output #0: loss = 5.12133 (* 1 = 5.12133 loss)
I0411 13:43:22.191174  3145 sgd_solver.cpp:106] Iteration 42, lr = 1e-05
I0411 13:43:22.566115  3145 solver.cpp:240] Iteration 43, loss = 5.15687
I0411 13:43:22.566148  3145 solver.cpp:256]     Train net output #0: loss = 5.15687 (* 1 = 5.15687 loss)
I0411 13:43:22.566156  3145 sgd_solver.cpp:106] Iteration 43, lr = 1e-05
I0411 13:43:22.944290  3145 solver.cpp:240] Iteration 44, loss = 5.17346
I0411 13:43:22.944321  3145 solver.cpp:256]     Train net output #0: loss = 5.17346 (* 1 = 5.17346 loss)
I0411 13:43:22.944329  3145 sgd_solver.cpp:106] Iteration 44, lr = 1e-05
I0411 13:43:23.321631  3145 solver.cpp:240] Iteration 45, loss = 5.29922
I0411 13:43:23.321665  3145 solver.cpp:256]     Train net output #0: loss = 5.29922 (* 1 = 5.29922 loss)
I0411 13:43:23.321672  3145 sgd_solver.cpp:106] Iteration 45, lr = 1e-05
I0411 13:43:23.698925  3145 solver.cpp:240] Iteration 46, loss = 5.25441
I0411 13:43:23.698959  3145 solver.cpp:256]     Train net output #0: loss = 5.25441 (* 1 = 5.25441 loss)
I0411 13:43:23.698967  3145 sgd_solver.cpp:106] Iteration 46, lr = 1e-05
I0411 13:43:24.079067  3145 solver.cpp:240] Iteration 47, loss = 5.15993
I0411 13:43:24.079102  3145 solver.cpp:256]     Train net output #0: loss = 5.15993 (* 1 = 5.15993 loss)
I0411 13:43:24.079109  3145 sgd_solver.cpp:106] Iteration 47, lr = 1e-05
I0411 13:43:24.455868  3145 solver.cpp:240] Iteration 48, loss = 5.24977
I0411 13:43:24.455909  3145 solver.cpp:256]     Train net output #0: loss = 5.24977 (* 1 = 5.24977 loss)
I0411 13:43:24.455916  3145 sgd_solver.cpp:106] Iteration 48, lr = 1e-05
I0411 13:43:24.829973  3145 solver.cpp:240] Iteration 49, loss = 5.30537
I0411 13:43:24.830005  3145 solver.cpp:256]     Train net output #0: loss = 5.30537 (* 1 = 5.30537 loss)
I0411 13:43:24.830013  3145 sgd_solver.cpp:106] Iteration 49, lr = 1e-05
I0411 13:43:24.830317  3145 solver.cpp:349] Iteration 50, Testing net (#0)
I0411 13:43:26.133716  3145 solver.cpp:416]     Test net output #0: accuracy_1 = 0
I0411 13:43:26.133744  3145 solver.cpp:416]     Test net output #1: accuracy_5 = 0.0402832
I0411 13:43:26.133754  3145 solver.cpp:416]     Test net output #2: loss = 4.63318 (* 1 = 4.63318 loss)
I0411 13:43:26.263777  3145 solver.cpp:240] Iteration 50, loss = 5.30564
I0411 13:43:26.263823  3145 solver.cpp:256]     Train net output #0: loss = 5.30564 (* 1 = 5.30564 loss)
I0411 13:43:26.263830  3145 sgd_solver.cpp:106] Iteration 50, lr = 1e-05
I0411 13:43:26.640204  3145 solver.cpp:240] Iteration 51, loss = 5.34768
I0411 13:43:26.640238  3145 solver.cpp:256]     Train net output #0: loss = 5.34768 (* 1 = 5.34768 loss)
I0411 13:43:26.640245  3145 sgd_solver.cpp:106] Iteration 51, lr = 1e-05
I0411 13:43:27.015372  3145 solver.cpp:240] Iteration 52, loss = 5.36295
I0411 13:43:27.015403  3145 solver.cpp:256]     Train net output #0: loss = 5.36295 (* 1 = 5.36295 loss)
I0411 13:43:27.015411  3145 sgd_solver.cpp:106] Iteration 52, lr = 1e-05
I0411 13:43:27.393816  3145 solver.cpp:240] Iteration 53, loss = 5.4526
I0411 13:43:27.393854  3145 solver.cpp:256]     Train net output #0: loss = 5.4526 (* 1 = 5.4526 loss)
I0411 13:43:27.393862  3145 sgd_solver.cpp:106] Iteration 53, lr = 1e-05
I0411 13:43:27.771621  3145 solver.cpp:240] Iteration 54, loss = 5.41069
I0411 13:43:27.771654  3145 solver.cpp:256]     Train net output #0: loss = 5.41069 (* 1 = 5.41069 loss)
I0411 13:43:27.771662  3145 sgd_solver.cpp:106] Iteration 54, lr = 1e-05
I0411 13:43:28.148804  3145 solver.cpp:240] Iteration 55, loss = 5.36186
I0411 13:43:28.148838  3145 solver.cpp:256]     Train net output #0: loss = 5.36186 (* 1 = 5.36186 loss)
I0411 13:43:28.148845  3145 sgd_solver.cpp:106] Iteration 55, lr = 1e-05
I0411 13:43:28.522516  3145 solver.cpp:240] Iteration 56, loss = 5.4698
I0411 13:43:28.522550  3145 solver.cpp:256]     Train net output #0: loss = 5.4698 (* 1 = 5.4698 loss)
I0411 13:43:28.522558  3145 sgd_solver.cpp:106] Iteration 56, lr = 1e-05
I0411 13:43:28.900231  3145 solver.cpp:240] Iteration 57, loss = 5.56259
I0411 13:43:28.900264  3145 solver.cpp:256]     Train net output #0: loss = 5.56259 (* 1 = 5.56259 loss)
I0411 13:43:28.900274  3145 sgd_solver.cpp:106] Iteration 57, lr = 1e-05
I0411 13:43:29.275153  3145 solver.cpp:240] Iteration 58, loss = 5.50785
I0411 13:43:29.275197  3145 solver.cpp:256]     Train net output #0: loss = 5.50785 (* 1 = 5.50785 loss)
I0411 13:43:29.275205  3145 sgd_solver.cpp:106] Iteration 58, lr = 1e-05
I0411 13:43:29.652539  3145 solver.cpp:240] Iteration 59, loss = 5.7081
I0411 13:43:29.652570  3145 solver.cpp:256]     Train net output #0: loss = 5.7081 (* 1 = 5.7081 loss)
I0411 13:43:29.652580  3145 sgd_solver.cpp:106] Iteration 59, lr = 1e-05
I0411 13:43:30.026259  3145 solver.cpp:240] Iteration 60, loss = 5.57926
I0411 13:43:30.026290  3145 solver.cpp:256]     Train net output #0: loss = 5.57926 (* 1 = 5.57926 loss)
I0411 13:43:30.026298  3145 sgd_solver.cpp:106] Iteration 60, lr = 1e-05
I0411 13:43:30.401113  3145 solver.cpp:240] Iteration 61, loss = 5.51942
I0411 13:43:30.401145  3145 solver.cpp:256]     Train net output #0: loss = 5.51942 (* 1 = 5.51942 loss)
I0411 13:43:30.401154  3145 sgd_solver.cpp:106] Iteration 61, lr = 1e-05
I0411 13:43:30.783733  3145 solver.cpp:240] Iteration 62, loss = 5.696
I0411 13:43:30.783767  3145 solver.cpp:256]     Train net output #0: loss = 5.696 (* 1 = 5.696 loss)
I0411 13:43:30.783776  3145 sgd_solver.cpp:106] Iteration 62, lr = 1e-05
I0411 13:43:31.162323  3145 solver.cpp:240] Iteration 63, loss = 5.67836
I0411 13:43:31.162370  3145 solver.cpp:256]     Train net output #0: loss = 5.67836 (* 1 = 5.67836 loss)
I0411 13:43:31.162377  3145 sgd_solver.cpp:106] Iteration 63, lr = 1e-05
I0411 13:43:31.540393  3145 solver.cpp:240] Iteration 64, loss = 5.61372
I0411 13:43:31.540426  3145 solver.cpp:256]     Train net output #0: loss = 5.61372 (* 1 = 5.61372 loss)
I0411 13:43:31.540433  3145 sgd_solver.cpp:106] Iteration 64, lr = 1e-05
I0411 13:43:31.915467  3145 solver.cpp:240] Iteration 65, loss = 5.74957
I0411 13:43:31.915865  3145 solver.cpp:256]     Train net output #0: loss = 5.74957 (* 1 = 5.74957 loss)
I0411 13:43:31.915876  3145 sgd_solver.cpp:106] Iteration 65, lr = 1e-05
I0411 13:43:32.293653  3145 solver.cpp:240] Iteration 66, loss = 5.76429
I0411 13:43:32.293689  3145 solver.cpp:256]     Train net output #0: loss = 5.76429 (* 1 = 5.76429 loss)
I0411 13:43:32.293696  3145 sgd_solver.cpp:106] Iteration 66, lr = 1e-05
I0411 13:43:32.671039  3145 solver.cpp:240] Iteration 67, loss = 5.67371
I0411 13:43:32.671072  3145 solver.cpp:256]     Train net output #0: loss = 5.67371 (* 1 = 5.67371 loss)
I0411 13:43:32.671080  3145 sgd_solver.cpp:106] Iteration 67, lr = 1e-05
I0411 13:43:33.049789  3145 solver.cpp:240] Iteration 68, loss = 5.73893
I0411 13:43:33.049823  3145 solver.cpp:256]     Train net output #0: loss = 5.73893 (* 1 = 5.73893 loss)
I0411 13:43:33.049831  3145 sgd_solver.cpp:106] Iteration 68, lr = 1e-05
I0411 13:43:33.424587  3145 solver.cpp:240] Iteration 69, loss = 5.81028
I0411 13:43:33.424620  3145 solver.cpp:256]     Train net output #0: loss = 5.81028 (* 1 = 5.81028 loss)
I0411 13:43:33.424628  3145 sgd_solver.cpp:106] Iteration 69, lr = 1e-05
I0411 13:43:33.803406  3145 solver.cpp:240] Iteration 70, loss = 5.87451
I0411 13:43:33.803438  3145 solver.cpp:256]     Train net output #0: loss = 5.87451 (* 1 = 5.87451 loss)
I0411 13:43:33.803447  3145 sgd_solver.cpp:106] Iteration 70, lr = 1e-05
I0411 13:43:34.177701  3145 solver.cpp:240] Iteration 71, loss = 5.80219
I0411 13:43:34.177732  3145 solver.cpp:256]     Train net output #0: loss = 5.80219 (* 1 = 5.80219 loss)
I0411 13:43:34.177741  3145 sgd_solver.cpp:106] Iteration 71, lr = 1e-05
I0411 13:43:34.556177  3145 solver.cpp:240] Iteration 72, loss = 5.70406
I0411 13:43:34.556210  3145 solver.cpp:256]     Train net output #0: loss = 5.70406 (* 1 = 5.70406 loss)
I0411 13:43:34.556217  3145 sgd_solver.cpp:106] Iteration 72, lr = 1e-05
I0411 13:43:34.932313  3145 solver.cpp:240] Iteration 73, loss = 5.87644
I0411 13:43:34.932345  3145 solver.cpp:256]     Train net output #0: loss = 5.87644 (* 1 = 5.87644 loss)
I0411 13:43:34.932353  3145 sgd_solver.cpp:106] Iteration 73, lr = 1e-05
I0411 13:43:35.307593  3145 solver.cpp:240] Iteration 74, loss = 5.82983
I0411 13:43:35.307627  3145 solver.cpp:256]     Train net output #0: loss = 5.82983 (* 1 = 5.82983 loss)
I0411 13:43:35.307636  3145 sgd_solver.cpp:106] Iteration 74, lr = 1e-05
I0411 13:43:35.307957  3145 solver.cpp:349] Iteration 75, Testing net (#0)
I0411 13:43:36.614222  3145 solver.cpp:416]     Test net output #0: accuracy_1 = 0.000244141
I0411 13:43:36.614249  3145 solver.cpp:416]     Test net output #1: accuracy_5 = 0.0557861
I0411 13:43:36.614271  3145 solver.cpp:416]     Test net output #2: loss = 5.00244 (* 1 = 5.00244 loss)
I0411 13:43:36.744027  3145 solver.cpp:240] Iteration 75, loss = 5.78138
I0411 13:43:36.744060  3145 solver.cpp:256]     Train net output #0: loss = 5.78138 (* 1 = 5.78138 loss)
I0411 13:43:36.744067  3145 sgd_solver.cpp:106] Iteration 75, lr = 1e-05
I0411 13:43:37.126895  3145 solver.cpp:240] Iteration 76, loss = 5.93004
I0411 13:43:37.126927  3145 solver.cpp:256]     Train net output #0: loss = 5.93004 (* 1 = 5.93004 loss)
I0411 13:43:37.126935  3145 sgd_solver.cpp:106] Iteration 76, lr = 1e-05
I0411 13:43:37.505800  3145 solver.cpp:240] Iteration 77, loss = 5.84646
I0411 13:43:37.505834  3145 solver.cpp:256]     Train net output #0: loss = 5.84646 (* 1 = 5.84646 loss)
I0411 13:43:37.505842  3145 sgd_solver.cpp:106] Iteration 77, lr = 1e-05
I0411 13:43:37.883265  3145 solver.cpp:240] Iteration 78, loss = 5.84046
I0411 13:43:37.883298  3145 solver.cpp:256]     Train net output #0: loss = 5.84046 (* 1 = 5.84046 loss)
I0411 13:43:37.883306  3145 sgd_solver.cpp:106] Iteration 78, lr = 1e-05
I0411 13:43:38.259707  3145 solver.cpp:240] Iteration 79, loss = 5.97793
I0411 13:43:38.259740  3145 solver.cpp:256]     Train net output #0: loss = 5.97793 (* 1 = 5.97793 loss)
I0411 13:43:38.259749  3145 sgd_solver.cpp:106] Iteration 79, lr = 1e-05
I0411 13:43:38.641120  3145 solver.cpp:240] Iteration 80, loss = 5.8644
I0411 13:43:38.641173  3145 solver.cpp:256]     Train net output #0: loss = 5.8644 (* 1 = 5.8644 loss)
I0411 13:43:38.641182  3145 sgd_solver.cpp:106] Iteration 80, lr = 1e-05
I0411 13:43:39.019351  3145 solver.cpp:240] Iteration 81, loss = 6.04671
I0411 13:43:39.019383  3145 solver.cpp:256]     Train net output #0: loss = 6.04671 (* 1 = 6.04671 loss)
I0411 13:43:39.019392  3145 sgd_solver.cpp:106] Iteration 81, lr = 1e-05
I0411 13:43:39.393996  3145 solver.cpp:240] Iteration 82, loss = 5.98068
I0411 13:43:39.394028  3145 solver.cpp:256]     Train net output #0: loss = 5.98068 (* 1 = 5.98068 loss)
I0411 13:43:39.394037  3145 sgd_solver.cpp:106] Iteration 82, lr = 1e-05
I0411 13:43:39.772747  3145 solver.cpp:240] Iteration 83, loss = 5.84926
I0411 13:43:39.772780  3145 solver.cpp:256]     Train net output #0: loss = 5.84926 (* 1 = 5.84926 loss)
I0411 13:43:39.772789  3145 sgd_solver.cpp:106] Iteration 83, lr = 1e-05
I0411 13:43:40.155722  3145 solver.cpp:240] Iteration 84, loss = 6.09887
I0411 13:43:40.155761  3145 solver.cpp:256]     Train net output #0: loss = 6.09887 (* 1 = 6.09887 loss)
I0411 13:43:40.155771  3145 sgd_solver.cpp:106] Iteration 84, lr = 1e-05
I0411 13:43:40.534310  3145 solver.cpp:240] Iteration 85, loss = 5.89975
I0411 13:43:40.534344  3145 solver.cpp:256]     Train net output #0: loss = 5.89975 (* 1 = 5.89975 loss)
I0411 13:43:40.534353  3145 sgd_solver.cpp:106] Iteration 85, lr = 1e-05
I0411 13:43:40.912784  3145 solver.cpp:240] Iteration 86, loss = 5.9014
I0411 13:43:40.912829  3145 solver.cpp:256]     Train net output #0: loss = 5.9014 (* 1 = 5.9014 loss)
I0411 13:43:40.912837  3145 sgd_solver.cpp:106] Iteration 86, lr = 1e-05
I0411 13:43:41.289301  3145 solver.cpp:240] Iteration 87, loss = 5.89186
I0411 13:43:41.289335  3145 solver.cpp:256]     Train net output #0: loss = 5.89186 (* 1 = 5.89186 loss)
I0411 13:43:41.289343  3145 sgd_solver.cpp:106] Iteration 87, lr = 1e-05
I0411 13:43:41.670359  3145 solver.cpp:240] Iteration 88, loss = 5.98672
I0411 13:43:41.670392  3145 solver.cpp:256]     Train net output #0: loss = 5.98672 (* 1 = 5.98672 loss)
I0411 13:43:41.670399  3145 sgd_solver.cpp:106] Iteration 88, lr = 1e-05
I0411 13:43:42.047893  3145 solver.cpp:240] Iteration 89, loss = 5.86472
I0411 13:43:42.047931  3145 solver.cpp:256]     Train net output #0: loss = 5.86472 (* 1 = 5.86472 loss)
I0411 13:43:42.047940  3145 sgd_solver.cpp:106] Iteration 89, lr = 1e-05
I0411 13:43:42.423269  3145 solver.cpp:240] Iteration 90, loss = 6.04215
I0411 13:43:42.423302  3145 solver.cpp:256]     Train net output #0: loss = 6.04215 (* 1 = 6.04215 loss)
I0411 13:43:42.423310  3145 sgd_solver.cpp:106] Iteration 90, lr = 1e-05
I0411 13:43:42.802582  3145 solver.cpp:240] Iteration 91, loss = 6.02684
I0411 13:43:42.802614  3145 solver.cpp:256]     Train net output #0: loss = 6.02684 (* 1 = 6.02684 loss)
I0411 13:43:42.802621  3145 sgd_solver.cpp:106] Iteration 91, lr = 1e-05
I0411 13:43:43.180907  3145 solver.cpp:240] Iteration 92, loss = 6.04094
I0411 13:43:43.180945  3145 solver.cpp:256]     Train net output #0: loss = 6.04094 (* 1 = 6.04094 loss)
I0411 13:43:43.180955  3145 sgd_solver.cpp:106] Iteration 92, lr = 1e-05
I0411 13:43:43.558223  3145 solver.cpp:240] Iteration 93, loss = 6.00168
I0411 13:43:43.558255  3145 solver.cpp:256]     Train net output #0: loss = 6.00168 (* 1 = 6.00168 loss)
I0411 13:43:43.558264  3145 sgd_solver.cpp:106] Iteration 93, lr = 1e-05
I0411 13:43:43.935617  3145 solver.cpp:240] Iteration 94, loss = 5.9557
I0411 13:43:43.935650  3145 solver.cpp:256]     Train net output #0: loss = 5.9557 (* 1 = 5.9557 loss)
I0411 13:43:43.935658  3145 sgd_solver.cpp:106] Iteration 94, lr = 1e-05
I0411 13:43:44.316381  3145 solver.cpp:240] Iteration 95, loss = 5.96017
I0411 13:43:44.316421  3145 solver.cpp:256]     Train net output #0: loss = 5.96017 (* 1 = 5.96017 loss)
I0411 13:43:44.316428  3145 sgd_solver.cpp:106] Iteration 95, lr = 1e-05
I0411 13:43:44.695106  3145 solver.cpp:240] Iteration 96, loss = 5.96882
I0411 13:43:44.695142  3145 solver.cpp:256]     Train net output #0: loss = 5.96882 (* 1 = 5.96882 loss)
I0411 13:43:44.695173  3145 sgd_solver.cpp:106] Iteration 96, lr = 1e-05
I0411 13:43:45.070575  3145 solver.cpp:240] Iteration 97, loss = 6.0082
I0411 13:43:45.070608  3145 solver.cpp:256]     Train net output #0: loss = 6.0082 (* 1 = 6.0082 loss)
I0411 13:43:45.070616  3145 sgd_solver.cpp:106] Iteration 97, lr = 1e-05
I0411 13:43:45.448097  3145 solver.cpp:240] Iteration 98, loss = 6.02647
I0411 13:43:45.448132  3145 solver.cpp:256]     Train net output #0: loss = 6.02647 (* 1 = 6.02647 loss)
I0411 13:43:45.448139  3145 sgd_solver.cpp:106] Iteration 98, lr = 1e-05
I0411 13:43:45.831540  3145 solver.cpp:240] Iteration 99, loss = 5.95378
I0411 13:43:45.831574  3145 solver.cpp:256]     Train net output #0: loss = 5.95378 (* 1 = 5.95378 loss)
I0411 13:43:45.831581  3145 sgd_solver.cpp:106] Iteration 99, lr = 1e-05
I0411 13:43:45.831902  3145 solver.cpp:349] Iteration 100, Testing net (#0)
I0411 13:43:47.144822  3145 solver.cpp:416]     Test net output #0: accuracy_1 = 0.000366211
I0411 13:43:47.144855  3145 solver.cpp:416]     Test net output #1: accuracy_5 = 0.0563965
I0411 13:43:47.144863  3145 solver.cpp:416]     Test net output #2: loss = 5.1191 (* 1 = 5.1191 loss)
I0411 13:43:47.274473  3145 solver.cpp:240] Iteration 100, loss = 5.97854
I0411 13:43:47.274507  3145 solver.cpp:256]     Train net output #0: loss = 5.97854 (* 1 = 5.97854 loss)
I0411 13:43:47.274514  3145 sgd_solver.cpp:106] Iteration 100, lr = 1e-05
