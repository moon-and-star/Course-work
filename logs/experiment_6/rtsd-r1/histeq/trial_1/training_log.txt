I0409 02:43:22.942322  1681 caffe.cpp:217] Using GPUs 1
I0409 02:43:23.246646  1681 caffe.cpp:222] GPU 1: GeForce GTX 1070
I0409 02:43:24.076565  1681 solver.cpp:60] Initializing solver from parameters: 
train_net: "./Prototxt/experiment_6/rtsd-r1/histeq/trial_1/train.prototxt"
test_net: "./Prototxt/experiment_6/rtsd-r1/histeq/trial_1/test.prototxt"
test_iter: 74
test_interval: 249
base_lr: 0.0001
display: 1
max_iter: 24900
lr_policy: "step"
gamma: 0.5
momentum: 0.9
weight_decay: 0.0005
stepsize: 4980
snapshot: 2490
snapshot_prefix: "./snapshots/experiment_6/rtsd-r1/histeq/trial_1/snap"
solver_mode: GPU
device_id: 1
train_state {
  level: 0
  stage: ""
}
iter_size: 1
type: "Adam"
I0409 02:43:24.076725  1681 solver.cpp:93] Creating training net from train_net file: ./Prototxt/experiment_6/rtsd-r1/histeq/trial_1/train.prototxt
I0409 02:43:24.077085  1681 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_1
I0409 02:43:24.077098  1681 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_5
I0409 02:43:24.077278  1681 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: false
    crop_size: 48
    mean_value: 122
    mean_value: 115
    mean_value: 128
  }
  data_param {
    source: "../local_data/lmdb/rtsd-r1/histeq/train/lmdb"
    batch_size: 1024
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 7
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_prescale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
  }
}
layer {
  name: "conv1_sTanH"
  type: "TanH"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_postscale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 150
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_prescale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
  }
}
layer {
  name: "conv2_sTanH"
  type: "TanH"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_postscale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 250
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_prescale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
  }
}
layer {
  name: "conv3_sTanH"
  type: "TanH"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_postscale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc4_300"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc4_300"
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fc4_prescale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
  }
}
layer {
  name: "fc4_sTanH"
  type: "TanH"
  bottom: "fc4_300"
  top: "fc4_300"
}
layer {
  name: "fc4_postscale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
  }
}
layer {
  name: "drop4"
  type: "Dropout"
  bottom: "fc4_300"
  top: "fc4_300"
  dropout_param {
    dropout_ratio: 0.4
  }
}
layer {
  name: "fc5_67"
  type: "InnerProduct"
  bottom: "fc4_300"
  top: "fc5_classes"
  inner_product_param {
    num_output: 67
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc5_classes"
  bottom: "label"
  top: "loss"
}
I0409 02:43:24.077400  1681 layer_factory.hpp:77] Creating layer data
I0409 02:43:24.078677  1681 net.cpp:100] Creating Layer data
I0409 02:43:24.078698  1681 net.cpp:408] data -> data
I0409 02:43:24.078725  1681 net.cpp:408] data -> label
I0409 02:43:24.080266  1779 db_lmdb.cpp:35] Opened lmdb ../local_data/lmdb/rtsd-r1/histeq/train/lmdb
I0409 02:43:24.101830  1681 data_layer.cpp:41] output data size: 1024,3,48,48
I0409 02:43:24.176069  1681 net.cpp:150] Setting up data
I0409 02:43:24.176106  1681 net.cpp:157] Top shape: 1024 3 48 48 (7077888)
I0409 02:43:24.176113  1681 net.cpp:157] Top shape: 1024 (1024)
I0409 02:43:24.176118  1681 net.cpp:165] Memory required for data: 28315648
I0409 02:43:24.176129  1681 layer_factory.hpp:77] Creating layer conv1
I0409 02:43:24.176156  1681 net.cpp:100] Creating Layer conv1
I0409 02:43:24.176164  1681 net.cpp:434] conv1 <- data
I0409 02:43:24.176180  1681 net.cpp:408] conv1 -> conv1
I0409 02:43:24.494441  1681 net.cpp:150] Setting up conv1
I0409 02:43:24.494470  1681 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0409 02:43:24.494474  1681 net.cpp:165] Memory required for data: 750850048
I0409 02:43:24.494496  1681 layer_factory.hpp:77] Creating layer conv1_prescale
I0409 02:43:24.494513  1681 net.cpp:100] Creating Layer conv1_prescale
I0409 02:43:24.494518  1681 net.cpp:434] conv1_prescale <- conv1
I0409 02:43:24.494536  1681 net.cpp:395] conv1_prescale -> conv1 (in-place)
I0409 02:43:24.494643  1681 net.cpp:150] Setting up conv1_prescale
I0409 02:43:24.494652  1681 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0409 02:43:24.494655  1681 net.cpp:165] Memory required for data: 1473384448
I0409 02:43:24.494663  1681 layer_factory.hpp:77] Creating layer conv1_sTanH
I0409 02:43:24.494670  1681 net.cpp:100] Creating Layer conv1_sTanH
I0409 02:43:24.494673  1681 net.cpp:434] conv1_sTanH <- conv1
I0409 02:43:24.494678  1681 net.cpp:395] conv1_sTanH -> conv1 (in-place)
I0409 02:43:24.494868  1681 net.cpp:150] Setting up conv1_sTanH
I0409 02:43:24.494880  1681 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0409 02:43:24.494884  1681 net.cpp:165] Memory required for data: 2195918848
I0409 02:43:24.494887  1681 layer_factory.hpp:77] Creating layer conv1_postscale
I0409 02:43:24.494895  1681 net.cpp:100] Creating Layer conv1_postscale
I0409 02:43:24.494900  1681 net.cpp:434] conv1_postscale <- conv1
I0409 02:43:24.494905  1681 net.cpp:395] conv1_postscale -> conv1 (in-place)
I0409 02:43:24.495000  1681 net.cpp:150] Setting up conv1_postscale
I0409 02:43:24.495008  1681 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0409 02:43:24.495012  1681 net.cpp:165] Memory required for data: 2918453248
I0409 02:43:24.495017  1681 layer_factory.hpp:77] Creating layer pool1
I0409 02:43:24.495023  1681 net.cpp:100] Creating Layer pool1
I0409 02:43:24.495025  1681 net.cpp:434] pool1 <- conv1
I0409 02:43:24.495030  1681 net.cpp:408] pool1 -> pool1
I0409 02:43:24.495079  1681 net.cpp:150] Setting up pool1
I0409 02:43:24.495086  1681 net.cpp:157] Top shape: 1024 100 21 21 (45158400)
I0409 02:43:24.495090  1681 net.cpp:165] Memory required for data: 3099086848
I0409 02:43:24.495110  1681 layer_factory.hpp:77] Creating layer conv2
I0409 02:43:24.495121  1681 net.cpp:100] Creating Layer conv2
I0409 02:43:24.495126  1681 net.cpp:434] conv2 <- pool1
I0409 02:43:24.495132  1681 net.cpp:408] conv2 -> conv2
I0409 02:43:24.499356  1681 net.cpp:150] Setting up conv2
I0409 02:43:24.499373  1681 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0409 02:43:24.499390  1681 net.cpp:165] Memory required for data: 3298152448
I0409 02:43:24.499402  1681 layer_factory.hpp:77] Creating layer conv2_prescale
I0409 02:43:24.499413  1681 net.cpp:100] Creating Layer conv2_prescale
I0409 02:43:24.499416  1681 net.cpp:434] conv2_prescale <- conv2
I0409 02:43:24.499423  1681 net.cpp:395] conv2_prescale -> conv2 (in-place)
I0409 02:43:24.499529  1681 net.cpp:150] Setting up conv2_prescale
I0409 02:43:24.499538  1681 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0409 02:43:24.499541  1681 net.cpp:165] Memory required for data: 3497218048
I0409 02:43:24.499546  1681 layer_factory.hpp:77] Creating layer conv2_sTanH
I0409 02:43:24.499552  1681 net.cpp:100] Creating Layer conv2_sTanH
I0409 02:43:24.499555  1681 net.cpp:434] conv2_sTanH <- conv2
I0409 02:43:24.499559  1681 net.cpp:395] conv2_sTanH -> conv2 (in-place)
I0409 02:43:24.500337  1681 net.cpp:150] Setting up conv2_sTanH
I0409 02:43:24.500352  1681 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0409 02:43:24.500356  1681 net.cpp:165] Memory required for data: 3696283648
I0409 02:43:24.500371  1681 layer_factory.hpp:77] Creating layer conv2_postscale
I0409 02:43:24.500380  1681 net.cpp:100] Creating Layer conv2_postscale
I0409 02:43:24.500385  1681 net.cpp:434] conv2_postscale <- conv2
I0409 02:43:24.500391  1681 net.cpp:395] conv2_postscale -> conv2 (in-place)
I0409 02:43:24.500485  1681 net.cpp:150] Setting up conv2_postscale
I0409 02:43:24.500494  1681 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0409 02:43:24.500496  1681 net.cpp:165] Memory required for data: 3895349248
I0409 02:43:24.500501  1681 layer_factory.hpp:77] Creating layer pool2
I0409 02:43:24.500507  1681 net.cpp:100] Creating Layer pool2
I0409 02:43:24.500510  1681 net.cpp:434] pool2 <- conv2
I0409 02:43:24.500514  1681 net.cpp:408] pool2 -> pool2
I0409 02:43:24.500553  1681 net.cpp:150] Setting up pool2
I0409 02:43:24.500560  1681 net.cpp:157] Top shape: 1024 150 9 9 (12441600)
I0409 02:43:24.500563  1681 net.cpp:165] Memory required for data: 3945115648
I0409 02:43:24.500566  1681 layer_factory.hpp:77] Creating layer conv3
I0409 02:43:24.500574  1681 net.cpp:100] Creating Layer conv3
I0409 02:43:24.500578  1681 net.cpp:434] conv3 <- pool2
I0409 02:43:24.500583  1681 net.cpp:408] conv3 -> conv3
I0409 02:43:24.505964  1681 net.cpp:150] Setting up conv3
I0409 02:43:24.505980  1681 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0409 02:43:24.505983  1681 net.cpp:165] Memory required for data: 3981979648
I0409 02:43:24.506005  1681 layer_factory.hpp:77] Creating layer conv3_prescale
I0409 02:43:24.506013  1681 net.cpp:100] Creating Layer conv3_prescale
I0409 02:43:24.506018  1681 net.cpp:434] conv3_prescale <- conv3
I0409 02:43:24.506023  1681 net.cpp:395] conv3_prescale -> conv3 (in-place)
I0409 02:43:24.506114  1681 net.cpp:150] Setting up conv3_prescale
I0409 02:43:24.506124  1681 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0409 02:43:24.506125  1681 net.cpp:165] Memory required for data: 4018843648
I0409 02:43:24.506130  1681 layer_factory.hpp:77] Creating layer conv3_sTanH
I0409 02:43:24.506135  1681 net.cpp:100] Creating Layer conv3_sTanH
I0409 02:43:24.506139  1681 net.cpp:434] conv3_sTanH <- conv3
I0409 02:43:24.506142  1681 net.cpp:395] conv3_sTanH -> conv3 (in-place)
I0409 02:43:24.506892  1681 net.cpp:150] Setting up conv3_sTanH
I0409 02:43:24.506907  1681 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0409 02:43:24.506912  1681 net.cpp:165] Memory required for data: 4055707648
I0409 02:43:24.506927  1681 layer_factory.hpp:77] Creating layer conv3_postscale
I0409 02:43:24.506934  1681 net.cpp:100] Creating Layer conv3_postscale
I0409 02:43:24.506952  1681 net.cpp:434] conv3_postscale <- conv3
I0409 02:43:24.506958  1681 net.cpp:395] conv3_postscale -> conv3 (in-place)
I0409 02:43:24.507061  1681 net.cpp:150] Setting up conv3_postscale
I0409 02:43:24.507071  1681 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0409 02:43:24.507073  1681 net.cpp:165] Memory required for data: 4092571648
I0409 02:43:24.507078  1681 layer_factory.hpp:77] Creating layer pool3
I0409 02:43:24.507086  1681 net.cpp:100] Creating Layer pool3
I0409 02:43:24.507091  1681 net.cpp:434] pool3 <- conv3
I0409 02:43:24.507095  1681 net.cpp:408] pool3 -> pool3
I0409 02:43:24.507136  1681 net.cpp:150] Setting up pool3
I0409 02:43:24.507144  1681 net.cpp:157] Top shape: 1024 250 3 3 (2304000)
I0409 02:43:24.507148  1681 net.cpp:165] Memory required for data: 4101787648
I0409 02:43:24.507150  1681 layer_factory.hpp:77] Creating layer fc4_300
I0409 02:43:24.507159  1681 net.cpp:100] Creating Layer fc4_300
I0409 02:43:24.507164  1681 net.cpp:434] fc4_300 <- pool3
I0409 02:43:24.507169  1681 net.cpp:408] fc4_300 -> fc4_300
I0409 02:43:24.512572  1681 net.cpp:150] Setting up fc4_300
I0409 02:43:24.512588  1681 net.cpp:157] Top shape: 1024 300 (307200)
I0409 02:43:24.512590  1681 net.cpp:165] Memory required for data: 4103016448
I0409 02:43:24.512598  1681 layer_factory.hpp:77] Creating layer fc4_prescale
I0409 02:43:24.512605  1681 net.cpp:100] Creating Layer fc4_prescale
I0409 02:43:24.512610  1681 net.cpp:434] fc4_prescale <- fc4_300
I0409 02:43:24.512615  1681 net.cpp:395] fc4_prescale -> fc4_300 (in-place)
I0409 02:43:24.512702  1681 net.cpp:150] Setting up fc4_prescale
I0409 02:43:24.512712  1681 net.cpp:157] Top shape: 1024 300 (307200)
I0409 02:43:24.512714  1681 net.cpp:165] Memory required for data: 4104245248
I0409 02:43:24.512718  1681 layer_factory.hpp:77] Creating layer fc4_sTanH
I0409 02:43:24.512723  1681 net.cpp:100] Creating Layer fc4_sTanH
I0409 02:43:24.512727  1681 net.cpp:434] fc4_sTanH <- fc4_300
I0409 02:43:24.512730  1681 net.cpp:395] fc4_sTanH -> fc4_300 (in-place)
I0409 02:43:24.512924  1681 net.cpp:150] Setting up fc4_sTanH
I0409 02:43:24.512935  1681 net.cpp:157] Top shape: 1024 300 (307200)
I0409 02:43:24.512938  1681 net.cpp:165] Memory required for data: 4105474048
I0409 02:43:24.512941  1681 layer_factory.hpp:77] Creating layer fc4_postscale
I0409 02:43:24.512949  1681 net.cpp:100] Creating Layer fc4_postscale
I0409 02:43:24.512953  1681 net.cpp:434] fc4_postscale <- fc4_300
I0409 02:43:24.512959  1681 net.cpp:395] fc4_postscale -> fc4_300 (in-place)
I0409 02:43:24.513056  1681 net.cpp:150] Setting up fc4_postscale
I0409 02:43:24.513063  1681 net.cpp:157] Top shape: 1024 300 (307200)
I0409 02:43:24.513067  1681 net.cpp:165] Memory required for data: 4106702848
I0409 02:43:24.513070  1681 layer_factory.hpp:77] Creating layer drop4
I0409 02:43:24.513078  1681 net.cpp:100] Creating Layer drop4
I0409 02:43:24.513082  1681 net.cpp:434] drop4 <- fc4_300
I0409 02:43:24.513087  1681 net.cpp:395] drop4 -> fc4_300 (in-place)
I0409 02:43:24.513115  1681 net.cpp:150] Setting up drop4
I0409 02:43:24.513123  1681 net.cpp:157] Top shape: 1024 300 (307200)
I0409 02:43:24.513125  1681 net.cpp:165] Memory required for data: 4107931648
I0409 02:43:24.513129  1681 layer_factory.hpp:77] Creating layer fc5_67
I0409 02:43:24.513134  1681 net.cpp:100] Creating Layer fc5_67
I0409 02:43:24.513137  1681 net.cpp:434] fc5_67 <- fc4_300
I0409 02:43:24.513144  1681 net.cpp:408] fc5_67 -> fc5_classes
I0409 02:43:24.514372  1681 net.cpp:150] Setting up fc5_67
I0409 02:43:24.514386  1681 net.cpp:157] Top shape: 1024 67 (68608)
I0409 02:43:24.514391  1681 net.cpp:165] Memory required for data: 4108206080
I0409 02:43:24.514405  1681 layer_factory.hpp:77] Creating layer loss
I0409 02:43:24.514412  1681 net.cpp:100] Creating Layer loss
I0409 02:43:24.514417  1681 net.cpp:434] loss <- fc5_classes
I0409 02:43:24.514421  1681 net.cpp:434] loss <- label
I0409 02:43:24.514427  1681 net.cpp:408] loss -> loss
I0409 02:43:24.514441  1681 layer_factory.hpp:77] Creating layer loss
I0409 02:43:24.514775  1681 net.cpp:150] Setting up loss
I0409 02:43:24.514798  1681 net.cpp:157] Top shape: (1)
I0409 02:43:24.514803  1681 net.cpp:160]     with loss weight 1
I0409 02:43:24.514827  1681 net.cpp:165] Memory required for data: 4108206084
I0409 02:43:24.514830  1681 net.cpp:226] loss needs backward computation.
I0409 02:43:24.514837  1681 net.cpp:226] fc5_67 needs backward computation.
I0409 02:43:24.514840  1681 net.cpp:226] drop4 needs backward computation.
I0409 02:43:24.514843  1681 net.cpp:226] fc4_postscale needs backward computation.
I0409 02:43:24.514847  1681 net.cpp:226] fc4_sTanH needs backward computation.
I0409 02:43:24.514848  1681 net.cpp:226] fc4_prescale needs backward computation.
I0409 02:43:24.514852  1681 net.cpp:226] fc4_300 needs backward computation.
I0409 02:43:24.514854  1681 net.cpp:226] pool3 needs backward computation.
I0409 02:43:24.514858  1681 net.cpp:226] conv3_postscale needs backward computation.
I0409 02:43:24.514860  1681 net.cpp:226] conv3_sTanH needs backward computation.
I0409 02:43:24.514863  1681 net.cpp:226] conv3_prescale needs backward computation.
I0409 02:43:24.514865  1681 net.cpp:226] conv3 needs backward computation.
I0409 02:43:24.514868  1681 net.cpp:226] pool2 needs backward computation.
I0409 02:43:24.514871  1681 net.cpp:226] conv2_postscale needs backward computation.
I0409 02:43:24.514874  1681 net.cpp:226] conv2_sTanH needs backward computation.
I0409 02:43:24.514878  1681 net.cpp:226] conv2_prescale needs backward computation.
I0409 02:43:24.514879  1681 net.cpp:226] conv2 needs backward computation.
I0409 02:43:24.514883  1681 net.cpp:226] pool1 needs backward computation.
I0409 02:43:24.514885  1681 net.cpp:226] conv1_postscale needs backward computation.
I0409 02:43:24.514889  1681 net.cpp:226] conv1_sTanH needs backward computation.
I0409 02:43:24.514891  1681 net.cpp:226] conv1_prescale needs backward computation.
I0409 02:43:24.514894  1681 net.cpp:226] conv1 needs backward computation.
I0409 02:43:24.514897  1681 net.cpp:228] data does not need backward computation.
I0409 02:43:24.514900  1681 net.cpp:270] This network produces output loss
I0409 02:43:24.514917  1681 net.cpp:283] Network initialization done.
I0409 02:43:24.515180  1681 solver.cpp:193] Creating test net (#0) specified by test_net file: ./Prototxt/experiment_6/rtsd-r1/histeq/trial_1/test.prototxt
I0409 02:43:24.515359  1681 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 48
    mean_value: 117
    mean_value: 114
    mean_value: 133
  }
  data_param {
    source: "../local_data/lmdb/rtsd-r1/histeq/test/lmdb"
    batch_size: 1024
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 7
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_prescale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
  }
}
layer {
  name: "conv1_sTanH"
  type: "TanH"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_postscale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 150
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_prescale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
  }
}
layer {
  name: "conv2_sTanH"
  type: "TanH"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_postscale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 250
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_prescale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
  }
}
layer {
  name: "conv3_sTanH"
  type: "TanH"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_postscale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc4_300"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc4_300"
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fc4_prescale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
  }
}
layer {
  name: "fc4_sTanH"
  type: "TanH"
  bottom: "fc4_300"
  top: "fc4_300"
}
layer {
  name: "fc4_postscale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
  }
}
layer {
  name: "drop4"
  type: "Dropout"
  bottom: "fc4_300"
  top: "fc4_300"
  dropout_param {
    dropout_ratio: 0.4
  }
}
layer {
  name: "fc5_67"
  type: "InnerProduct"
  bottom: "fc4_300"
  top: "fc5_classes"
  inner_product_param {
    num_output: 67
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc5_classes"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy_1"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_1"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 1
  }
}
layer {
  name: "accuracy_5"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0409 02:43:24.515480  1681 layer_factory.hpp:77] Creating layer data
I0409 02:43:24.516185  1681 net.cpp:100] Creating Layer data
I0409 02:43:24.516199  1681 net.cpp:408] data -> data
I0409 02:43:24.516208  1681 net.cpp:408] data -> label
I0409 02:43:24.517367  1816 db_lmdb.cpp:35] Opened lmdb ../local_data/lmdb/rtsd-r1/histeq/test/lmdb
I0409 02:43:24.517530  1681 data_layer.cpp:41] output data size: 1024,3,48,48
I0409 02:43:24.566371  1681 net.cpp:150] Setting up data
I0409 02:43:24.566400  1681 net.cpp:157] Top shape: 1024 3 48 48 (7077888)
I0409 02:43:24.566404  1681 net.cpp:157] Top shape: 1024 (1024)
I0409 02:43:24.566407  1681 net.cpp:165] Memory required for data: 28315648
I0409 02:43:24.566413  1681 layer_factory.hpp:77] Creating layer label_data_1_split
I0409 02:43:24.566429  1681 net.cpp:100] Creating Layer label_data_1_split
I0409 02:43:24.566435  1681 net.cpp:434] label_data_1_split <- label
I0409 02:43:24.566443  1681 net.cpp:408] label_data_1_split -> label_data_1_split_0
I0409 02:43:24.566455  1681 net.cpp:408] label_data_1_split -> label_data_1_split_1
I0409 02:43:24.566463  1681 net.cpp:408] label_data_1_split -> label_data_1_split_2
I0409 02:43:24.566599  1681 net.cpp:150] Setting up label_data_1_split
I0409 02:43:24.566608  1681 net.cpp:157] Top shape: 1024 (1024)
I0409 02:43:24.566612  1681 net.cpp:157] Top shape: 1024 (1024)
I0409 02:43:24.566615  1681 net.cpp:157] Top shape: 1024 (1024)
I0409 02:43:24.566618  1681 net.cpp:165] Memory required for data: 28327936
I0409 02:43:24.566642  1681 layer_factory.hpp:77] Creating layer conv1
I0409 02:43:24.566656  1681 net.cpp:100] Creating Layer conv1
I0409 02:43:24.566661  1681 net.cpp:434] conv1 <- data
I0409 02:43:24.566668  1681 net.cpp:408] conv1 -> conv1
I0409 02:43:24.571892  1681 net.cpp:150] Setting up conv1
I0409 02:43:24.571910  1681 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0409 02:43:24.571916  1681 net.cpp:165] Memory required for data: 750862336
I0409 02:43:24.571928  1681 layer_factory.hpp:77] Creating layer conv1_prescale
I0409 02:43:24.571939  1681 net.cpp:100] Creating Layer conv1_prescale
I0409 02:43:24.571944  1681 net.cpp:434] conv1_prescale <- conv1
I0409 02:43:24.571950  1681 net.cpp:395] conv1_prescale -> conv1 (in-place)
I0409 02:43:24.572060  1681 net.cpp:150] Setting up conv1_prescale
I0409 02:43:24.572072  1681 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0409 02:43:24.572075  1681 net.cpp:165] Memory required for data: 1473396736
I0409 02:43:24.572082  1681 layer_factory.hpp:77] Creating layer conv1_sTanH
I0409 02:43:24.572089  1681 net.cpp:100] Creating Layer conv1_sTanH
I0409 02:43:24.572094  1681 net.cpp:434] conv1_sTanH <- conv1
I0409 02:43:24.572098  1681 net.cpp:395] conv1_sTanH -> conv1 (in-place)
I0409 02:43:24.572298  1681 net.cpp:150] Setting up conv1_sTanH
I0409 02:43:24.572309  1681 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0409 02:43:24.572314  1681 net.cpp:165] Memory required for data: 2195931136
I0409 02:43:24.572316  1681 layer_factory.hpp:77] Creating layer conv1_postscale
I0409 02:43:24.572324  1681 net.cpp:100] Creating Layer conv1_postscale
I0409 02:43:24.572326  1681 net.cpp:434] conv1_postscale <- conv1
I0409 02:43:24.572335  1681 net.cpp:395] conv1_postscale -> conv1 (in-place)
I0409 02:43:24.572453  1681 net.cpp:150] Setting up conv1_postscale
I0409 02:43:24.572461  1681 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0409 02:43:24.572464  1681 net.cpp:165] Memory required for data: 2918465536
I0409 02:43:24.572468  1681 layer_factory.hpp:77] Creating layer pool1
I0409 02:43:24.572475  1681 net.cpp:100] Creating Layer pool1
I0409 02:43:24.572480  1681 net.cpp:434] pool1 <- conv1
I0409 02:43:24.572486  1681 net.cpp:408] pool1 -> pool1
I0409 02:43:24.572530  1681 net.cpp:150] Setting up pool1
I0409 02:43:24.572538  1681 net.cpp:157] Top shape: 1024 100 21 21 (45158400)
I0409 02:43:24.572541  1681 net.cpp:165] Memory required for data: 3099099136
I0409 02:43:24.572545  1681 layer_factory.hpp:77] Creating layer conv2
I0409 02:43:24.572553  1681 net.cpp:100] Creating Layer conv2
I0409 02:43:24.572558  1681 net.cpp:434] conv2 <- pool1
I0409 02:43:24.572563  1681 net.cpp:408] conv2 -> conv2
I0409 02:43:24.580621  1681 net.cpp:150] Setting up conv2
I0409 02:43:24.580641  1681 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0409 02:43:24.580646  1681 net.cpp:165] Memory required for data: 3298164736
I0409 02:43:24.580658  1681 layer_factory.hpp:77] Creating layer conv2_prescale
I0409 02:43:24.580672  1681 net.cpp:100] Creating Layer conv2_prescale
I0409 02:43:24.580677  1681 net.cpp:434] conv2_prescale <- conv2
I0409 02:43:24.580683  1681 net.cpp:395] conv2_prescale -> conv2 (in-place)
I0409 02:43:24.580799  1681 net.cpp:150] Setting up conv2_prescale
I0409 02:43:24.580808  1681 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0409 02:43:24.580811  1681 net.cpp:165] Memory required for data: 3497230336
I0409 02:43:24.580816  1681 layer_factory.hpp:77] Creating layer conv2_sTanH
I0409 02:43:24.580822  1681 net.cpp:100] Creating Layer conv2_sTanH
I0409 02:43:24.580824  1681 net.cpp:434] conv2_sTanH <- conv2
I0409 02:43:24.580832  1681 net.cpp:395] conv2_sTanH -> conv2 (in-place)
I0409 02:43:24.581748  1681 net.cpp:150] Setting up conv2_sTanH
I0409 02:43:24.581765  1681 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0409 02:43:24.581770  1681 net.cpp:165] Memory required for data: 3696295936
I0409 02:43:24.581773  1681 layer_factory.hpp:77] Creating layer conv2_postscale
I0409 02:43:24.581782  1681 net.cpp:100] Creating Layer conv2_postscale
I0409 02:43:24.581800  1681 net.cpp:434] conv2_postscale <- conv2
I0409 02:43:24.581807  1681 net.cpp:395] conv2_postscale -> conv2 (in-place)
I0409 02:43:24.581918  1681 net.cpp:150] Setting up conv2_postscale
I0409 02:43:24.581926  1681 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0409 02:43:24.581929  1681 net.cpp:165] Memory required for data: 3895361536
I0409 02:43:24.581934  1681 layer_factory.hpp:77] Creating layer pool2
I0409 02:43:24.581943  1681 net.cpp:100] Creating Layer pool2
I0409 02:43:24.581950  1681 net.cpp:434] pool2 <- conv2
I0409 02:43:24.581955  1681 net.cpp:408] pool2 -> pool2
I0409 02:43:24.582005  1681 net.cpp:150] Setting up pool2
I0409 02:43:24.582013  1681 net.cpp:157] Top shape: 1024 150 9 9 (12441600)
I0409 02:43:24.582016  1681 net.cpp:165] Memory required for data: 3945127936
I0409 02:43:24.582020  1681 layer_factory.hpp:77] Creating layer conv3
I0409 02:43:24.582031  1681 net.cpp:100] Creating Layer conv3
I0409 02:43:24.582036  1681 net.cpp:434] conv3 <- pool2
I0409 02:43:24.582042  1681 net.cpp:408] conv3 -> conv3
I0409 02:43:24.588037  1681 net.cpp:150] Setting up conv3
I0409 02:43:24.588054  1681 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0409 02:43:24.588058  1681 net.cpp:165] Memory required for data: 3981991936
I0409 02:43:24.588069  1681 layer_factory.hpp:77] Creating layer conv3_prescale
I0409 02:43:24.588078  1681 net.cpp:100] Creating Layer conv3_prescale
I0409 02:43:24.588083  1681 net.cpp:434] conv3_prescale <- conv3
I0409 02:43:24.588090  1681 net.cpp:395] conv3_prescale -> conv3 (in-place)
I0409 02:43:24.588198  1681 net.cpp:150] Setting up conv3_prescale
I0409 02:43:24.588208  1681 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0409 02:43:24.588212  1681 net.cpp:165] Memory required for data: 4018855936
I0409 02:43:24.588215  1681 layer_factory.hpp:77] Creating layer conv3_sTanH
I0409 02:43:24.588222  1681 net.cpp:100] Creating Layer conv3_sTanH
I0409 02:43:24.588227  1681 net.cpp:434] conv3_sTanH <- conv3
I0409 02:43:24.588230  1681 net.cpp:395] conv3_sTanH -> conv3 (in-place)
I0409 02:43:24.590458  1681 net.cpp:150] Setting up conv3_sTanH
I0409 02:43:24.590474  1681 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0409 02:43:24.590479  1681 net.cpp:165] Memory required for data: 4055719936
I0409 02:43:24.590483  1681 layer_factory.hpp:77] Creating layer conv3_postscale
I0409 02:43:24.590490  1681 net.cpp:100] Creating Layer conv3_postscale
I0409 02:43:24.590493  1681 net.cpp:434] conv3_postscale <- conv3
I0409 02:43:24.590502  1681 net.cpp:395] conv3_postscale -> conv3 (in-place)
I0409 02:43:24.590607  1681 net.cpp:150] Setting up conv3_postscale
I0409 02:43:24.590615  1681 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0409 02:43:24.590618  1681 net.cpp:165] Memory required for data: 4092583936
I0409 02:43:24.590623  1681 layer_factory.hpp:77] Creating layer pool3
I0409 02:43:24.590632  1681 net.cpp:100] Creating Layer pool3
I0409 02:43:24.590637  1681 net.cpp:434] pool3 <- conv3
I0409 02:43:24.590642  1681 net.cpp:408] pool3 -> pool3
I0409 02:43:24.590683  1681 net.cpp:150] Setting up pool3
I0409 02:43:24.590692  1681 net.cpp:157] Top shape: 1024 250 3 3 (2304000)
I0409 02:43:24.590694  1681 net.cpp:165] Memory required for data: 4101799936
I0409 02:43:24.590698  1681 layer_factory.hpp:77] Creating layer fc4_300
I0409 02:43:24.590705  1681 net.cpp:100] Creating Layer fc4_300
I0409 02:43:24.590709  1681 net.cpp:434] fc4_300 <- pool3
I0409 02:43:24.590715  1681 net.cpp:408] fc4_300 -> fc4_300
I0409 02:43:24.596243  1681 net.cpp:150] Setting up fc4_300
I0409 02:43:24.596261  1681 net.cpp:157] Top shape: 1024 300 (307200)
I0409 02:43:24.596264  1681 net.cpp:165] Memory required for data: 4103028736
I0409 02:43:24.596271  1681 layer_factory.hpp:77] Creating layer fc4_prescale
I0409 02:43:24.596281  1681 net.cpp:100] Creating Layer fc4_prescale
I0409 02:43:24.596285  1681 net.cpp:434] fc4_prescale <- fc4_300
I0409 02:43:24.596290  1681 net.cpp:395] fc4_prescale -> fc4_300 (in-place)
I0409 02:43:24.596385  1681 net.cpp:150] Setting up fc4_prescale
I0409 02:43:24.596410  1681 net.cpp:157] Top shape: 1024 300 (307200)
I0409 02:43:24.596415  1681 net.cpp:165] Memory required for data: 4104257536
I0409 02:43:24.596420  1681 layer_factory.hpp:77] Creating layer fc4_sTanH
I0409 02:43:24.596424  1681 net.cpp:100] Creating Layer fc4_sTanH
I0409 02:43:24.596427  1681 net.cpp:434] fc4_sTanH <- fc4_300
I0409 02:43:24.596433  1681 net.cpp:395] fc4_sTanH -> fc4_300 (in-place)
I0409 02:43:24.596632  1681 net.cpp:150] Setting up fc4_sTanH
I0409 02:43:24.596642  1681 net.cpp:157] Top shape: 1024 300 (307200)
I0409 02:43:24.596645  1681 net.cpp:165] Memory required for data: 4105486336
I0409 02:43:24.596648  1681 layer_factory.hpp:77] Creating layer fc4_postscale
I0409 02:43:24.596657  1681 net.cpp:100] Creating Layer fc4_postscale
I0409 02:43:24.596662  1681 net.cpp:434] fc4_postscale <- fc4_300
I0409 02:43:24.596668  1681 net.cpp:395] fc4_postscale -> fc4_300 (in-place)
I0409 02:43:24.596771  1681 net.cpp:150] Setting up fc4_postscale
I0409 02:43:24.596779  1681 net.cpp:157] Top shape: 1024 300 (307200)
I0409 02:43:24.596782  1681 net.cpp:165] Memory required for data: 4106715136
I0409 02:43:24.596786  1681 layer_factory.hpp:77] Creating layer drop4
I0409 02:43:24.596796  1681 net.cpp:100] Creating Layer drop4
I0409 02:43:24.596799  1681 net.cpp:434] drop4 <- fc4_300
I0409 02:43:24.596804  1681 net.cpp:395] drop4 -> fc4_300 (in-place)
I0409 02:43:24.596832  1681 net.cpp:150] Setting up drop4
I0409 02:43:24.596838  1681 net.cpp:157] Top shape: 1024 300 (307200)
I0409 02:43:24.596842  1681 net.cpp:165] Memory required for data: 4107943936
I0409 02:43:24.596844  1681 layer_factory.hpp:77] Creating layer fc5_67
I0409 02:43:24.596849  1681 net.cpp:100] Creating Layer fc5_67
I0409 02:43:24.596854  1681 net.cpp:434] fc5_67 <- fc4_300
I0409 02:43:24.596860  1681 net.cpp:408] fc5_67 -> fc5_classes
I0409 02:43:24.597105  1681 net.cpp:150] Setting up fc5_67
I0409 02:43:24.597115  1681 net.cpp:157] Top shape: 1024 67 (68608)
I0409 02:43:24.597118  1681 net.cpp:165] Memory required for data: 4108218368
I0409 02:43:24.597128  1681 layer_factory.hpp:77] Creating layer fc5_classes_fc5_67_0_split
I0409 02:43:24.597136  1681 net.cpp:100] Creating Layer fc5_classes_fc5_67_0_split
I0409 02:43:24.597141  1681 net.cpp:434] fc5_classes_fc5_67_0_split <- fc5_classes
I0409 02:43:24.597149  1681 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_0
I0409 02:43:24.597160  1681 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_1
I0409 02:43:24.597169  1681 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_2
I0409 02:43:24.597225  1681 net.cpp:150] Setting up fc5_classes_fc5_67_0_split
I0409 02:43:24.597232  1681 net.cpp:157] Top shape: 1024 67 (68608)
I0409 02:43:24.597235  1681 net.cpp:157] Top shape: 1024 67 (68608)
I0409 02:43:24.597239  1681 net.cpp:157] Top shape: 1024 67 (68608)
I0409 02:43:24.597241  1681 net.cpp:165] Memory required for data: 4109041664
I0409 02:43:24.597244  1681 layer_factory.hpp:77] Creating layer loss
I0409 02:43:24.597250  1681 net.cpp:100] Creating Layer loss
I0409 02:43:24.597254  1681 net.cpp:434] loss <- fc5_classes_fc5_67_0_split_0
I0409 02:43:24.597257  1681 net.cpp:434] loss <- label_data_1_split_0
I0409 02:43:24.597262  1681 net.cpp:408] loss -> loss
I0409 02:43:24.597273  1681 layer_factory.hpp:77] Creating layer loss
I0409 02:43:24.597611  1681 net.cpp:150] Setting up loss
I0409 02:43:24.597625  1681 net.cpp:157] Top shape: (1)
I0409 02:43:24.597630  1681 net.cpp:160]     with loss weight 1
I0409 02:43:24.597641  1681 net.cpp:165] Memory required for data: 4109041668
I0409 02:43:24.597645  1681 layer_factory.hpp:77] Creating layer accuracy_1
I0409 02:43:24.597653  1681 net.cpp:100] Creating Layer accuracy_1
I0409 02:43:24.597658  1681 net.cpp:434] accuracy_1 <- fc5_classes_fc5_67_0_split_1
I0409 02:43:24.597663  1681 net.cpp:434] accuracy_1 <- label_data_1_split_1
I0409 02:43:24.597671  1681 net.cpp:408] accuracy_1 -> accuracy_1
I0409 02:43:24.597681  1681 net.cpp:150] Setting up accuracy_1
I0409 02:43:24.597697  1681 net.cpp:157] Top shape: (1)
I0409 02:43:24.597700  1681 net.cpp:165] Memory required for data: 4109041672
I0409 02:43:24.597703  1681 layer_factory.hpp:77] Creating layer accuracy_5
I0409 02:43:24.597709  1681 net.cpp:100] Creating Layer accuracy_5
I0409 02:43:24.597712  1681 net.cpp:434] accuracy_5 <- fc5_classes_fc5_67_0_split_2
I0409 02:43:24.597719  1681 net.cpp:434] accuracy_5 <- label_data_1_split_2
I0409 02:43:24.597725  1681 net.cpp:408] accuracy_5 -> accuracy_5
I0409 02:43:24.597734  1681 net.cpp:150] Setting up accuracy_5
I0409 02:43:24.597739  1681 net.cpp:157] Top shape: (1)
I0409 02:43:24.597743  1681 net.cpp:165] Memory required for data: 4109041676
I0409 02:43:24.597745  1681 net.cpp:228] accuracy_5 does not need backward computation.
I0409 02:43:24.597749  1681 net.cpp:228] accuracy_1 does not need backward computation.
I0409 02:43:24.597757  1681 net.cpp:226] loss needs backward computation.
I0409 02:43:24.597761  1681 net.cpp:226] fc5_classes_fc5_67_0_split needs backward computation.
I0409 02:43:24.597764  1681 net.cpp:226] fc5_67 needs backward computation.
I0409 02:43:24.597767  1681 net.cpp:226] drop4 needs backward computation.
I0409 02:43:24.597770  1681 net.cpp:226] fc4_postscale needs backward computation.
I0409 02:43:24.597772  1681 net.cpp:226] fc4_sTanH needs backward computation.
I0409 02:43:24.597775  1681 net.cpp:226] fc4_prescale needs backward computation.
I0409 02:43:24.597777  1681 net.cpp:226] fc4_300 needs backward computation.
I0409 02:43:24.597780  1681 net.cpp:226] pool3 needs backward computation.
I0409 02:43:24.597784  1681 net.cpp:226] conv3_postscale needs backward computation.
I0409 02:43:24.597787  1681 net.cpp:226] conv3_sTanH needs backward computation.
I0409 02:43:24.597790  1681 net.cpp:226] conv3_prescale needs backward computation.
I0409 02:43:24.597792  1681 net.cpp:226] conv3 needs backward computation.
I0409 02:43:24.597795  1681 net.cpp:226] pool2 needs backward computation.
I0409 02:43:24.597800  1681 net.cpp:226] conv2_postscale needs backward computation.
I0409 02:43:24.597801  1681 net.cpp:226] conv2_sTanH needs backward computation.
I0409 02:43:24.597805  1681 net.cpp:226] conv2_prescale needs backward computation.
I0409 02:43:24.597806  1681 net.cpp:226] conv2 needs backward computation.
I0409 02:43:24.597810  1681 net.cpp:226] pool1 needs backward computation.
I0409 02:43:24.597813  1681 net.cpp:226] conv1_postscale needs backward computation.
I0409 02:43:24.597826  1681 net.cpp:226] conv1_sTanH needs backward computation.
I0409 02:43:24.597829  1681 net.cpp:226] conv1_prescale needs backward computation.
I0409 02:43:24.597831  1681 net.cpp:226] conv1 needs backward computation.
I0409 02:43:24.597836  1681 net.cpp:228] label_data_1_split does not need backward computation.
I0409 02:43:24.597839  1681 net.cpp:228] data does not need backward computation.
I0409 02:43:24.597842  1681 net.cpp:270] This network produces output accuracy_1
I0409 02:43:24.597846  1681 net.cpp:270] This network produces output accuracy_5
I0409 02:43:24.597848  1681 net.cpp:270] This network produces output loss
I0409 02:43:24.597868  1681 net.cpp:283] Network initialization done.
I0409 02:43:24.597944  1681 solver.cpp:72] Solver scaffolding done.
I0409 02:43:24.598845  1681 caffe.cpp:251] Starting Optimization
I0409 02:43:24.598852  1681 solver.cpp:291] Solving 
I0409 02:43:24.598856  1681 solver.cpp:292] Learning Rate Policy: step
I0409 02:43:24.613204  1681 solver.cpp:349] Iteration 0, Testing net (#0)
I0409 02:43:34.349177  1681 solver.cpp:416]     Test net output #0: accuracy_1 = 0.0474029
I0409 02:43:34.349207  1681 solver.cpp:416]     Test net output #1: accuracy_5 = 0.116105
I0409 02:43:34.349217  1681 solver.cpp:416]     Test net output #2: loss = 4.72102 (* 1 = 4.72102 loss)
I0409 02:43:34.500993  1681 solver.cpp:240] Iteration 0, loss = 5.18902
I0409 02:43:34.501025  1681 solver.cpp:256]     Train net output #0: loss = 5.18902 (* 1 = 5.18902 loss)
I0409 02:43:34.501039  1681 sgd_solver.cpp:106] Iteration 0, lr = 0.0001
I0409 02:43:34.877336  1681 solver.cpp:240] Iteration 1, loss = 5.33161
I0409 02:43:34.877390  1681 solver.cpp:256]     Train net output #0: loss = 5.33161 (* 1 = 5.33161 loss)
I0409 02:43:34.877398  1681 sgd_solver.cpp:106] Iteration 1, lr = 0.0001
I0409 02:43:35.253132  1681 solver.cpp:240] Iteration 2, loss = 5.61488
I0409 02:43:35.253165  1681 solver.cpp:256]     Train net output #0: loss = 5.61488 (* 1 = 5.61488 loss)
I0409 02:43:35.253173  1681 sgd_solver.cpp:106] Iteration 2, lr = 0.0001
I0409 02:43:35.627969  1681 solver.cpp:240] Iteration 3, loss = 5.38817
I0409 02:43:35.628006  1681 solver.cpp:256]     Train net output #0: loss = 5.38817 (* 1 = 5.38817 loss)
I0409 02:43:35.628015  1681 sgd_solver.cpp:106] Iteration 3, lr = 0.0001
I0409 02:43:36.000655  1681 solver.cpp:240] Iteration 4, loss = 5.24481
I0409 02:43:36.000686  1681 solver.cpp:256]     Train net output #0: loss = 5.24481 (* 1 = 5.24481 loss)
I0409 02:43:36.000694  1681 sgd_solver.cpp:106] Iteration 4, lr = 0.0001
I0409 02:43:36.375138  1681 solver.cpp:240] Iteration 5, loss = 5.6494
I0409 02:43:36.375169  1681 solver.cpp:256]     Train net output #0: loss = 5.6494 (* 1 = 5.6494 loss)
I0409 02:43:36.375176  1681 sgd_solver.cpp:106] Iteration 5, lr = 0.0001
I0409 02:43:36.744503  1681 solver.cpp:240] Iteration 6, loss = 5.53603
I0409 02:43:36.744534  1681 solver.cpp:256]     Train net output #0: loss = 5.53603 (* 1 = 5.53603 loss)
I0409 02:43:36.744541  1681 sgd_solver.cpp:106] Iteration 6, lr = 0.0001
I0409 02:43:37.120910  1681 solver.cpp:240] Iteration 7, loss = 5.87493
I0409 02:43:37.120954  1681 solver.cpp:256]     Train net output #0: loss = 5.87493 (* 1 = 5.87493 loss)
I0409 02:43:37.120962  1681 sgd_solver.cpp:106] Iteration 7, lr = 0.0001
I0409 02:43:37.494966  1681 solver.cpp:240] Iteration 8, loss = 5.99895
I0409 02:43:37.494997  1681 solver.cpp:256]     Train net output #0: loss = 5.99895 (* 1 = 5.99895 loss)
I0409 02:43:37.495007  1681 sgd_solver.cpp:106] Iteration 8, lr = 0.0001
I0409 02:43:37.867933  1681 solver.cpp:240] Iteration 9, loss = 5.55776
I0409 02:43:37.867964  1681 solver.cpp:256]     Train net output #0: loss = 5.55776 (* 1 = 5.55776 loss)
I0409 02:43:37.867971  1681 sgd_solver.cpp:106] Iteration 9, lr = 0.0001
I0409 02:43:38.236300  1681 solver.cpp:240] Iteration 10, loss = 7.91612
I0409 02:43:38.236335  1681 solver.cpp:256]     Train net output #0: loss = 7.91612 (* 1 = 7.91612 loss)
I0409 02:43:38.236342  1681 sgd_solver.cpp:106] Iteration 10, lr = 0.0001
I0409 02:43:38.610867  1681 solver.cpp:240] Iteration 11, loss = 6.77788
I0409 02:43:38.610898  1681 solver.cpp:256]     Train net output #0: loss = 6.77788 (* 1 = 6.77788 loss)
I0409 02:43:38.610905  1681 sgd_solver.cpp:106] Iteration 11, lr = 0.0001
I0409 02:43:38.985096  1681 solver.cpp:240] Iteration 12, loss = 7.16177
I0409 02:43:38.985139  1681 solver.cpp:256]     Train net output #0: loss = 7.16177 (* 1 = 7.16177 loss)
I0409 02:43:38.985147  1681 sgd_solver.cpp:106] Iteration 12, lr = 0.0001
I0409 02:43:39.359663  1681 solver.cpp:240] Iteration 13, loss = 8.14196
I0409 02:43:39.359702  1681 solver.cpp:256]     Train net output #0: loss = 8.14196 (* 1 = 8.14196 loss)
I0409 02:43:39.359724  1681 sgd_solver.cpp:106] Iteration 13, lr = 0.0001
I0409 02:43:39.735066  1681 solver.cpp:240] Iteration 14, loss = 7.75704
I0409 02:43:39.735097  1681 solver.cpp:256]     Train net output #0: loss = 7.75704 (* 1 = 7.75704 loss)
I0409 02:43:39.735105  1681 sgd_solver.cpp:106] Iteration 14, lr = 0.0001
I0409 02:43:40.111007  1681 solver.cpp:240] Iteration 15, loss = 7.55452
I0409 02:43:40.111050  1681 solver.cpp:256]     Train net output #0: loss = 7.55452 (* 1 = 7.55452 loss)
I0409 02:43:40.111058  1681 sgd_solver.cpp:106] Iteration 15, lr = 0.0001
I0409 02:43:40.485867  1681 solver.cpp:240] Iteration 16, loss = 7.14174
I0409 02:43:40.485909  1681 solver.cpp:256]     Train net output #0: loss = 7.14174 (* 1 = 7.14174 loss)
I0409 02:43:40.485918  1681 sgd_solver.cpp:106] Iteration 16, lr = 0.0001
I0409 02:43:40.860011  1681 solver.cpp:240] Iteration 17, loss = 7.99881
I0409 02:43:40.860041  1681 solver.cpp:256]     Train net output #0: loss = 7.99881 (* 1 = 7.99881 loss)
I0409 02:43:40.860074  1681 sgd_solver.cpp:106] Iteration 17, lr = 0.0001
I0409 02:43:41.235074  1681 solver.cpp:240] Iteration 18, loss = 10.4267
I0409 02:43:41.235110  1681 solver.cpp:256]     Train net output #0: loss = 10.4267 (* 1 = 10.4267 loss)
I0409 02:43:41.235118  1681 sgd_solver.cpp:106] Iteration 18, lr = 0.0001
I0409 02:43:41.610065  1681 solver.cpp:240] Iteration 19, loss = 11.6046
I0409 02:43:41.610098  1681 solver.cpp:256]     Train net output #0: loss = 11.6046 (* 1 = 11.6046 loss)
I0409 02:43:41.610107  1681 sgd_solver.cpp:106] Iteration 19, lr = 0.0001
I0409 02:43:41.984920  1681 solver.cpp:240] Iteration 20, loss = 10.6703
I0409 02:43:41.984952  1681 solver.cpp:256]     Train net output #0: loss = 10.6703 (* 1 = 10.6703 loss)
I0409 02:43:41.984961  1681 sgd_solver.cpp:106] Iteration 20, lr = 0.0001
I0409 02:43:42.358137  1681 solver.cpp:240] Iteration 21, loss = 13.0161
I0409 02:43:42.358184  1681 solver.cpp:256]     Train net output #0: loss = 13.0161 (* 1 = 13.0161 loss)
I0409 02:43:42.358193  1681 sgd_solver.cpp:106] Iteration 21, lr = 0.0001
I0409 02:43:42.729619  1681 solver.cpp:240] Iteration 22, loss = 12.9109
I0409 02:43:42.729662  1681 solver.cpp:256]     Train net output #0: loss = 12.9109 (* 1 = 12.9109 loss)
I0409 02:43:42.729671  1681 sgd_solver.cpp:106] Iteration 22, lr = 0.0001
I0409 02:43:43.100461  1681 solver.cpp:240] Iteration 23, loss = 13.2902
I0409 02:43:43.100507  1681 solver.cpp:256]     Train net output #0: loss = 13.2902 (* 1 = 13.2902 loss)
I0409 02:43:43.100514  1681 sgd_solver.cpp:106] Iteration 23, lr = 0.0001
I0409 02:43:43.477808  1681 solver.cpp:240] Iteration 24, loss = 13.6329
I0409 02:43:43.477843  1681 solver.cpp:256]     Train net output #0: loss = 13.6329 (* 1 = 13.6329 loss)
I0409 02:43:43.477850  1681 sgd_solver.cpp:106] Iteration 24, lr = 0.0001
I0409 02:43:43.851670  1681 solver.cpp:240] Iteration 25, loss = 13.5752
I0409 02:43:43.851713  1681 solver.cpp:256]     Train net output #0: loss = 13.5752 (* 1 = 13.5752 loss)
I0409 02:43:43.851722  1681 sgd_solver.cpp:106] Iteration 25, lr = 0.0001
I0409 02:43:44.223565  1681 solver.cpp:240] Iteration 26, loss = 14.7938
I0409 02:43:44.223608  1681 solver.cpp:256]     Train net output #0: loss = 14.7938 (* 1 = 14.7938 loss)
I0409 02:43:44.223620  1681 sgd_solver.cpp:106] Iteration 26, lr = 0.0001
I0409 02:43:44.601907  1681 solver.cpp:240] Iteration 27, loss = 15.536
I0409 02:43:44.601949  1681 solver.cpp:256]     Train net output #0: loss = 15.536 (* 1 = 15.536 loss)
I0409 02:43:44.601956  1681 sgd_solver.cpp:106] Iteration 27, lr = 0.0001
I0409 02:43:44.979046  1681 solver.cpp:240] Iteration 28, loss = 15.5307
I0409 02:43:44.979079  1681 solver.cpp:256]     Train net output #0: loss = 15.5307 (* 1 = 15.5307 loss)
I0409 02:43:44.979087  1681 sgd_solver.cpp:106] Iteration 28, lr = 0.0001
I0409 02:43:45.355337  1681 solver.cpp:240] Iteration 29, loss = 16.5735
I0409 02:43:45.355370  1681 solver.cpp:256]     Train net output #0: loss = 16.5735 (* 1 = 16.5735 loss)
I0409 02:43:45.355378  1681 sgd_solver.cpp:106] Iteration 29, lr = 0.0001
I0409 02:43:45.728670  1681 solver.cpp:240] Iteration 30, loss = 16.0054
I0409 02:43:45.728714  1681 solver.cpp:256]     Train net output #0: loss = 16.0054 (* 1 = 16.0054 loss)
I0409 02:43:45.728723  1681 sgd_solver.cpp:106] Iteration 30, lr = 0.0001
I0409 02:43:46.111317  1681 solver.cpp:240] Iteration 31, loss = 18.0024
I0409 02:43:46.111361  1681 solver.cpp:256]     Train net output #0: loss = 18.0024 (* 1 = 18.0024 loss)
I0409 02:43:46.111368  1681 sgd_solver.cpp:106] Iteration 31, lr = 0.0001
I0409 02:43:46.488942  1681 solver.cpp:240] Iteration 32, loss = 15.2352
I0409 02:43:46.488986  1681 solver.cpp:256]     Train net output #0: loss = 15.2352 (* 1 = 15.2352 loss)
I0409 02:43:46.488994  1681 sgd_solver.cpp:106] Iteration 32, lr = 0.0001
I0409 02:43:46.864821  1681 solver.cpp:240] Iteration 33, loss = 17.492
I0409 02:43:46.864852  1681 solver.cpp:256]     Train net output #0: loss = 17.492 (* 1 = 17.492 loss)
I0409 02:43:46.864902  1681 sgd_solver.cpp:106] Iteration 33, lr = 0.0001
I0409 02:43:47.240301  1681 solver.cpp:240] Iteration 34, loss = 16.7484
I0409 02:43:47.240334  1681 solver.cpp:256]     Train net output #0: loss = 16.7484 (* 1 = 16.7484 loss)
I0409 02:43:47.240341  1681 sgd_solver.cpp:106] Iteration 34, lr = 0.0001
I0409 02:43:47.616441  1681 solver.cpp:240] Iteration 35, loss = 18.4426
I0409 02:43:47.616474  1681 solver.cpp:256]     Train net output #0: loss = 18.4426 (* 1 = 18.4426 loss)
I0409 02:43:47.616482  1681 sgd_solver.cpp:106] Iteration 35, lr = 0.0001
I0409 02:43:47.994326  1681 solver.cpp:240] Iteration 36, loss = 18.4142
I0409 02:43:47.994359  1681 solver.cpp:256]     Train net output #0: loss = 18.4142 (* 1 = 18.4142 loss)
I0409 02:43:47.994366  1681 sgd_solver.cpp:106] Iteration 36, lr = 0.0001
I0409 02:43:48.371430  1681 solver.cpp:240] Iteration 37, loss = 18.3633
I0409 02:43:48.371470  1681 solver.cpp:256]     Train net output #0: loss = 18.3633 (* 1 = 18.3633 loss)
I0409 02:43:48.371480  1681 sgd_solver.cpp:106] Iteration 37, lr = 0.0001
I0409 02:43:48.745833  1681 solver.cpp:240] Iteration 38, loss = 17.3616
I0409 02:43:48.745867  1681 solver.cpp:256]     Train net output #0: loss = 17.3616 (* 1 = 17.3616 loss)
I0409 02:43:48.745874  1681 sgd_solver.cpp:106] Iteration 38, lr = 0.0001
I0409 02:43:49.121465  1681 solver.cpp:240] Iteration 39, loss = 18.3884
I0409 02:43:49.121497  1681 solver.cpp:256]     Train net output #0: loss = 18.3884 (* 1 = 18.3884 loss)
I0409 02:43:49.121505  1681 sgd_solver.cpp:106] Iteration 39, lr = 0.0001
I0409 02:43:49.499979  1681 solver.cpp:240] Iteration 40, loss = 17.3904
I0409 02:43:49.500010  1681 solver.cpp:256]     Train net output #0: loss = 17.3904 (* 1 = 17.3904 loss)
I0409 02:43:49.500018  1681 sgd_solver.cpp:106] Iteration 40, lr = 0.0001
I0409 02:43:49.874506  1681 solver.cpp:240] Iteration 41, loss = 15.3638
I0409 02:43:49.874550  1681 solver.cpp:256]     Train net output #0: loss = 15.3638 (* 1 = 15.3638 loss)
I0409 02:43:49.874558  1681 sgd_solver.cpp:106] Iteration 41, lr = 0.0001
I0409 02:43:50.249660  1681 solver.cpp:240] Iteration 42, loss = 8.84647
I0409 02:43:50.249703  1681 solver.cpp:256]     Train net output #0: loss = 8.84647 (* 1 = 8.84647 loss)
I0409 02:43:50.249711  1681 sgd_solver.cpp:106] Iteration 42, lr = 0.0001
I0409 02:43:50.625742  1681 solver.cpp:240] Iteration 43, loss = 9.03231
I0409 02:43:50.625787  1681 solver.cpp:256]     Train net output #0: loss = 9.03231 (* 1 = 9.03231 loss)
I0409 02:43:50.625794  1681 sgd_solver.cpp:106] Iteration 43, lr = 0.0001
I0409 02:43:51.001549  1681 solver.cpp:240] Iteration 44, loss = 15.8075
I0409 02:43:51.001583  1681 solver.cpp:256]     Train net output #0: loss = 15.8075 (* 1 = 15.8075 loss)
I0409 02:43:51.001591  1681 sgd_solver.cpp:106] Iteration 44, lr = 0.0001
I0409 02:43:51.377185  1681 solver.cpp:240] Iteration 45, loss = 18.0393
I0409 02:43:51.377218  1681 solver.cpp:256]     Train net output #0: loss = 18.0393 (* 1 = 18.0393 loss)
I0409 02:43:51.377225  1681 sgd_solver.cpp:106] Iteration 45, lr = 0.0001
I0409 02:43:51.752272  1681 solver.cpp:240] Iteration 46, loss = 15.1217
I0409 02:43:51.752313  1681 solver.cpp:256]     Train net output #0: loss = 15.1217 (* 1 = 15.1217 loss)
I0409 02:43:51.752321  1681 sgd_solver.cpp:106] Iteration 46, lr = 0.0001
I0409 02:43:52.128515  1681 solver.cpp:240] Iteration 47, loss = 14.4109
I0409 02:43:52.128559  1681 solver.cpp:256]     Train net output #0: loss = 14.4109 (* 1 = 14.4109 loss)
I0409 02:43:52.128567  1681 sgd_solver.cpp:106] Iteration 47, lr = 0.0001
I0409 02:43:52.507323  1681 solver.cpp:240] Iteration 48, loss = 15.0024
I0409 02:43:52.507367  1681 solver.cpp:256]     Train net output #0: loss = 15.0024 (* 1 = 15.0024 loss)
I0409 02:43:52.507375  1681 sgd_solver.cpp:106] Iteration 48, lr = 0.0001
I0409 02:43:52.882860  1681 solver.cpp:240] Iteration 49, loss = 14.5815
I0409 02:43:52.882894  1681 solver.cpp:256]     Train net output #0: loss = 14.5815 (* 1 = 14.5815 loss)
I0409 02:43:52.882901  1681 sgd_solver.cpp:106] Iteration 49, lr = 0.0001
I0409 02:43:53.257994  1681 solver.cpp:240] Iteration 50, loss = 18.5999
I0409 02:43:53.258491  1681 solver.cpp:256]     Train net output #0: loss = 18.5999 (* 1 = 18.5999 loss)
I0409 02:43:53.258502  1681 sgd_solver.cpp:106] Iteration 50, lr = 0.0001
I0409 02:43:53.635164  1681 solver.cpp:240] Iteration 51, loss = 14.3732
I0409 02:43:53.635210  1681 solver.cpp:256]     Train net output #0: loss = 14.3732 (* 1 = 14.3732 loss)
I0409 02:43:53.635217  1681 sgd_solver.cpp:106] Iteration 51, lr = 0.0001
I0409 02:43:54.009974  1681 solver.cpp:240] Iteration 52, loss = 13.0735
I0409 02:43:54.010007  1681 solver.cpp:256]     Train net output #0: loss = 13.0735 (* 1 = 13.0735 loss)
I0409 02:43:54.010015  1681 sgd_solver.cpp:106] Iteration 52, lr = 0.0001
I0409 02:43:54.385602  1681 solver.cpp:240] Iteration 53, loss = 15.3913
I0409 02:43:54.385634  1681 solver.cpp:256]     Train net output #0: loss = 15.3913 (* 1 = 15.3913 loss)
I0409 02:43:54.385643  1681 sgd_solver.cpp:106] Iteration 53, lr = 0.0001
I0409 02:43:54.759291  1681 solver.cpp:240] Iteration 54, loss = 12.2934
I0409 02:43:54.759325  1681 solver.cpp:256]     Train net output #0: loss = 12.2934 (* 1 = 12.2934 loss)
I0409 02:43:54.759332  1681 sgd_solver.cpp:106] Iteration 54, lr = 0.0001
I0409 02:43:55.134481  1681 solver.cpp:240] Iteration 55, loss = 10.6331
I0409 02:43:55.134513  1681 solver.cpp:256]     Train net output #0: loss = 10.6331 (* 1 = 10.6331 loss)
I0409 02:43:55.134521  1681 sgd_solver.cpp:106] Iteration 55, lr = 0.0001
I0409 02:43:55.512125  1681 solver.cpp:240] Iteration 56, loss = 14.7261
I0409 02:43:55.512162  1681 solver.cpp:256]     Train net output #0: loss = 14.7261 (* 1 = 14.7261 loss)
I0409 02:43:55.512171  1681 sgd_solver.cpp:106] Iteration 56, lr = 0.0001
I0409 02:43:55.887117  1681 solver.cpp:240] Iteration 57, loss = 14.3748
I0409 02:43:55.887159  1681 solver.cpp:256]     Train net output #0: loss = 14.3748 (* 1 = 14.3748 loss)
I0409 02:43:55.887166  1681 sgd_solver.cpp:106] Iteration 57, lr = 0.0001
I0409 02:43:56.262020  1681 solver.cpp:240] Iteration 58, loss = 12.5578
I0409 02:43:56.262068  1681 solver.cpp:256]     Train net output #0: loss = 12.5578 (* 1 = 12.5578 loss)
I0409 02:43:56.262075  1681 sgd_solver.cpp:106] Iteration 58, lr = 0.0001
I0409 02:43:56.639844  1681 solver.cpp:240] Iteration 59, loss = 18.2583
I0409 02:43:56.639889  1681 solver.cpp:256]     Train net output #0: loss = 18.2583 (* 1 = 18.2583 loss)
I0409 02:43:56.639899  1681 sgd_solver.cpp:106] Iteration 59, lr = 0.0001
I0409 02:43:57.014663  1681 solver.cpp:240] Iteration 60, loss = 19.6228
I0409 02:43:57.014694  1681 solver.cpp:256]     Train net output #0: loss = 19.6228 (* 1 = 19.6228 loss)
I0409 02:43:57.014703  1681 sgd_solver.cpp:106] Iteration 60, lr = 0.0001
I0409 02:43:57.390158  1681 solver.cpp:240] Iteration 61, loss = 18.3216
I0409 02:43:57.390202  1681 solver.cpp:256]     Train net output #0: loss = 18.3216 (* 1 = 18.3216 loss)
I0409 02:43:57.390209  1681 sgd_solver.cpp:106] Iteration 61, lr = 0.0001
I0409 02:43:57.763674  1681 solver.cpp:240] Iteration 62, loss = 14.6384
I0409 02:43:57.763715  1681 solver.cpp:256]     Train net output #0: loss = 14.6384 (* 1 = 14.6384 loss)
I0409 02:43:57.763723  1681 sgd_solver.cpp:106] Iteration 62, lr = 0.0001
I0409 02:43:58.141297  1681 solver.cpp:240] Iteration 63, loss = 15.2077
I0409 02:43:58.141341  1681 solver.cpp:256]     Train net output #0: loss = 15.2077 (* 1 = 15.2077 loss)
I0409 02:43:58.141350  1681 sgd_solver.cpp:106] Iteration 63, lr = 0.0001
I0409 02:43:58.517657  1681 solver.cpp:240] Iteration 64, loss = 18.342
I0409 02:43:58.517690  1681 solver.cpp:256]     Train net output #0: loss = 18.342 (* 1 = 18.342 loss)
I0409 02:43:58.517699  1681 sgd_solver.cpp:106] Iteration 64, lr = 0.0001
I0409 02:43:58.893610  1681 solver.cpp:240] Iteration 65, loss = 17.7747
I0409 02:43:58.893643  1681 solver.cpp:256]     Train net output #0: loss = 17.7747 (* 1 = 17.7747 loss)
I0409 02:43:58.893652  1681 sgd_solver.cpp:106] Iteration 65, lr = 0.0001
I0409 02:43:59.267670  1681 solver.cpp:240] Iteration 66, loss = 18.374
I0409 02:43:59.267702  1681 solver.cpp:256]     Train net output #0: loss = 18.374 (* 1 = 18.374 loss)
I0409 02:43:59.267734  1681 sgd_solver.cpp:106] Iteration 66, lr = 0.0001
I0409 02:43:59.646607  1681 solver.cpp:240] Iteration 67, loss = 17.9805
I0409 02:43:59.646651  1681 solver.cpp:256]     Train net output #0: loss = 17.9805 (* 1 = 17.9805 loss)
I0409 02:43:59.646658  1681 sgd_solver.cpp:106] Iteration 67, lr = 0.0001
I0409 02:44:00.028506  1681 solver.cpp:240] Iteration 68, loss = 19.0065
I0409 02:44:00.028548  1681 solver.cpp:256]     Train net output #0: loss = 19.0065 (* 1 = 19.0065 loss)
I0409 02:44:00.028555  1681 sgd_solver.cpp:106] Iteration 68, lr = 0.0001
I0409 02:44:00.406276  1681 solver.cpp:240] Iteration 69, loss = 19.1612
I0409 02:44:00.406309  1681 solver.cpp:256]     Train net output #0: loss = 19.1612 (* 1 = 19.1612 loss)
I0409 02:44:00.406317  1681 sgd_solver.cpp:106] Iteration 69, lr = 0.0001
I0409 02:44:00.783336  1681 solver.cpp:240] Iteration 70, loss = 17.0163
I0409 02:44:00.783370  1681 solver.cpp:256]     Train net output #0: loss = 17.0163 (* 1 = 17.0163 loss)
I0409 02:44:00.783377  1681 sgd_solver.cpp:106] Iteration 70, lr = 0.0001
I0409 02:44:01.157095  1681 solver.cpp:240] Iteration 71, loss = 18.3853
I0409 02:44:01.157129  1681 solver.cpp:256]     Train net output #0: loss = 18.3853 (* 1 = 18.3853 loss)
I0409 02:44:01.157136  1681 sgd_solver.cpp:106] Iteration 71, lr = 0.0001
I0409 02:44:01.533658  1681 solver.cpp:240] Iteration 72, loss = 18.6099
I0409 02:44:01.533691  1681 solver.cpp:256]     Train net output #0: loss = 18.6099 (* 1 = 18.6099 loss)
I0409 02:44:01.533699  1681 sgd_solver.cpp:106] Iteration 72, lr = 0.0001
I0409 02:44:01.910033  1681 solver.cpp:240] Iteration 73, loss = 17.0835
I0409 02:44:01.910065  1681 solver.cpp:256]     Train net output #0: loss = 17.0835 (* 1 = 17.0835 loss)
I0409 02:44:01.910074  1681 sgd_solver.cpp:106] Iteration 73, lr = 0.0001
I0409 02:44:02.287436  1681 solver.cpp:240] Iteration 74, loss = 17.562
I0409 02:44:02.287467  1681 solver.cpp:256]     Train net output #0: loss = 17.562 (* 1 = 17.562 loss)
I0409 02:44:02.287475  1681 sgd_solver.cpp:106] Iteration 74, lr = 0.0001
I0409 02:44:02.662112  1681 solver.cpp:240] Iteration 75, loss = 18.0626
I0409 02:44:02.662144  1681 solver.cpp:256]     Train net output #0: loss = 18.0626 (* 1 = 18.0626 loss)
I0409 02:44:02.662153  1681 sgd_solver.cpp:106] Iteration 75, lr = 0.0001
I0409 02:44:03.039747  1681 solver.cpp:240] Iteration 76, loss = 17.3867
I0409 02:44:03.039791  1681 solver.cpp:256]     Train net output #0: loss = 17.3867 (* 1 = 17.3867 loss)
I0409 02:44:03.039799  1681 sgd_solver.cpp:106] Iteration 76, lr = 0.0001
I0409 02:44:03.416620  1681 solver.cpp:240] Iteration 77, loss = 16.0416
I0409 02:44:03.416667  1681 solver.cpp:256]     Train net output #0: loss = 16.0416 (* 1 = 16.0416 loss)
I0409 02:44:03.416676  1681 sgd_solver.cpp:106] Iteration 77, lr = 0.0001
I0409 02:44:03.794509  1681 solver.cpp:240] Iteration 78, loss = 12.485
I0409 02:44:03.794553  1681 solver.cpp:256]     Train net output #0: loss = 12.485 (* 1 = 12.485 loss)
I0409 02:44:03.794564  1681 sgd_solver.cpp:106] Iteration 78, lr = 0.0001
I0409 02:44:04.170065  1681 solver.cpp:240] Iteration 79, loss = 13.4599
I0409 02:44:04.170096  1681 solver.cpp:256]     Train net output #0: loss = 13.4599 (* 1 = 13.4599 loss)
I0409 02:44:04.170104  1681 sgd_solver.cpp:106] Iteration 79, lr = 0.0001
I0409 02:44:04.552063  1681 solver.cpp:240] Iteration 80, loss = 7.97335
I0409 02:44:04.552093  1681 solver.cpp:256]     Train net output #0: loss = 7.97335 (* 1 = 7.97335 loss)
I0409 02:44:04.552100  1681 sgd_solver.cpp:106] Iteration 80, lr = 0.0001
