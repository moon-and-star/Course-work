I0506 00:34:41.130347 25403 caffe.cpp:217] Using GPUs 0
I0506 00:34:41.438308 25403 caffe.cpp:222] GPU 0: GeForce GTX 1070
I0506 00:34:44.471668 25403 solver.cpp:60] Initializing solver from parameters: 
train_net: "./Prototxt/experiment_13/RTSD/orig/trial_1/train.prototxt"
test_net: "./Prototxt/experiment_13/RTSD/orig/trial_1/test.prototxt"
test_iter: 34
test_interval: 169
base_lr: 1e-06
display: 1
max_iter: 16900
lr_policy: "step"
gamma: 0.5
momentum: 0.9
weight_decay: 0.0005
stepsize: 10140
snapshot: 1690
snapshot_prefix: "./snapshots/experiment_13/RTSD/orig/trial_1/snap"
solver_mode: GPU
device_id: 0
train_state {
  level: 0
  stage: ""
}
iter_size: 1
type: "Adam"
I0506 00:34:44.471814 25403 solver.cpp:93] Creating training net from train_net file: ./Prototxt/experiment_13/RTSD/orig/trial_1/train.prototxt
I0506 00:34:44.472299 25403 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.0039215689
    mirror: false
    crop_size: 48
    mean_value: 119
    mean_value: 113
    mean_value: 113
  }
  data_param {
    source: "../local_data/lmdb/RTSD/orig/train/lmdb"
    batch_size: 512
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 7
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_prescale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv1_sTanH"
  type: "TanH"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_postscale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 150
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_prescale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv2_sTanH"
  type: "TanH"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_postscale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 250
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_prescale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv3_sTanH"
  type: "TanH"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_postscale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc4_300"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc4_300"
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fc4_prescale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "fc4_sTanH"
  type: "TanH"
  bottom: "fc4_300"
  top: "fc4_300"
}
layer {
  name: "fc4_postscale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "fc5_116"
  type: "InnerProduct"
  bottom: "fc4_300"
  top: "fc5_classes"
  inner_product_param {
    num_output: 116
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "softmax"
  type: "Softmax"
  bottom: "fc5_classes"
  top: "softmax"
}
layer {
  name: "loss"
  type: "MultinomialLogisticLoss"
  bottom: "softmax"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy_1"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_1"
  accuracy_param {
    top_k: 1
  }
}
layer {
  name: "accuracy_5"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_5"
  accuracy_param {
    top_k: 5
  }
}
layer {
  name: "silence"
  type: "Silence"
  bottom: "accuracy_1"
  bottom: "accuracy_5"
}
I0506 00:34:44.472453 25403 layer_factory.hpp:77] Creating layer data
I0506 00:34:44.473474 25403 net.cpp:100] Creating Layer data
I0506 00:34:44.473500 25403 net.cpp:408] data -> data
I0506 00:34:44.473533 25403 net.cpp:408] data -> label
I0506 00:34:44.481463 25686 db_lmdb.cpp:35] Opened lmdb ../local_data/lmdb/RTSD/orig/train/lmdb
I0506 00:34:44.513420 25403 data_layer.cpp:41] output data size: 512,3,48,48
I0506 00:34:44.673264 25403 net.cpp:150] Setting up data
I0506 00:34:44.673295 25403 net.cpp:157] Top shape: 512 3 48 48 (3538944)
I0506 00:34:44.673300 25403 net.cpp:157] Top shape: 512 (512)
I0506 00:34:44.673303 25403 net.cpp:165] Memory required for data: 14157824
I0506 00:34:44.673316 25403 layer_factory.hpp:77] Creating layer label_data_1_split
I0506 00:34:44.673331 25403 net.cpp:100] Creating Layer label_data_1_split
I0506 00:34:44.673338 25403 net.cpp:434] label_data_1_split <- label
I0506 00:34:44.673352 25403 net.cpp:408] label_data_1_split -> label_data_1_split_0
I0506 00:34:44.673363 25403 net.cpp:408] label_data_1_split -> label_data_1_split_1
I0506 00:34:44.673372 25403 net.cpp:408] label_data_1_split -> label_data_1_split_2
I0506 00:34:44.673471 25403 net.cpp:150] Setting up label_data_1_split
I0506 00:34:44.673480 25403 net.cpp:157] Top shape: 512 (512)
I0506 00:34:44.673485 25403 net.cpp:157] Top shape: 512 (512)
I0506 00:34:44.673490 25403 net.cpp:157] Top shape: 512 (512)
I0506 00:34:44.673491 25403 net.cpp:165] Memory required for data: 14163968
I0506 00:34:44.673496 25403 layer_factory.hpp:77] Creating layer conv1
I0506 00:34:44.673512 25403 net.cpp:100] Creating Layer conv1
I0506 00:34:44.673517 25403 net.cpp:434] conv1 <- data
I0506 00:34:44.673523 25403 net.cpp:408] conv1 -> conv1
I0506 00:34:46.582938 25403 net.cpp:150] Setting up conv1
I0506 00:34:46.582967 25403 net.cpp:157] Top shape: 512 100 42 42 (90316800)
I0506 00:34:46.582972 25403 net.cpp:165] Memory required for data: 375431168
I0506 00:34:46.582994 25403 layer_factory.hpp:77] Creating layer conv1_prescale
I0506 00:34:46.583011 25403 net.cpp:100] Creating Layer conv1_prescale
I0506 00:34:46.583017 25403 net.cpp:434] conv1_prescale <- conv1
I0506 00:34:46.583029 25403 net.cpp:395] conv1_prescale -> conv1 (in-place)
I0506 00:34:46.583151 25403 net.cpp:150] Setting up conv1_prescale
I0506 00:34:46.583160 25403 net.cpp:157] Top shape: 512 100 42 42 (90316800)
I0506 00:34:46.583165 25403 net.cpp:165] Memory required for data: 736698368
I0506 00:34:46.583173 25403 layer_factory.hpp:77] Creating layer conv1_sTanH
I0506 00:34:46.583184 25403 net.cpp:100] Creating Layer conv1_sTanH
I0506 00:34:46.583189 25403 net.cpp:434] conv1_sTanH <- conv1
I0506 00:34:46.583194 25403 net.cpp:395] conv1_sTanH -> conv1 (in-place)
I0506 00:34:46.583405 25403 net.cpp:150] Setting up conv1_sTanH
I0506 00:34:46.583417 25403 net.cpp:157] Top shape: 512 100 42 42 (90316800)
I0506 00:34:46.583442 25403 net.cpp:165] Memory required for data: 1097965568
I0506 00:34:46.583447 25403 layer_factory.hpp:77] Creating layer conv1_postscale
I0506 00:34:46.583456 25403 net.cpp:100] Creating Layer conv1_postscale
I0506 00:34:46.583461 25403 net.cpp:434] conv1_postscale <- conv1
I0506 00:34:46.583467 25403 net.cpp:395] conv1_postscale -> conv1 (in-place)
I0506 00:34:46.583571 25403 net.cpp:150] Setting up conv1_postscale
I0506 00:34:46.583580 25403 net.cpp:157] Top shape: 512 100 42 42 (90316800)
I0506 00:34:46.583583 25403 net.cpp:165] Memory required for data: 1459232768
I0506 00:34:46.583588 25403 layer_factory.hpp:77] Creating layer pool1
I0506 00:34:46.583600 25403 net.cpp:100] Creating Layer pool1
I0506 00:34:46.583603 25403 net.cpp:434] pool1 <- conv1
I0506 00:34:46.583608 25403 net.cpp:408] pool1 -> pool1
I0506 00:34:46.583660 25403 net.cpp:150] Setting up pool1
I0506 00:34:46.583668 25403 net.cpp:157] Top shape: 512 100 21 21 (22579200)
I0506 00:34:46.583672 25403 net.cpp:165] Memory required for data: 1549549568
I0506 00:34:46.583674 25403 layer_factory.hpp:77] Creating layer conv2
I0506 00:34:46.583685 25403 net.cpp:100] Creating Layer conv2
I0506 00:34:46.583689 25403 net.cpp:434] conv2 <- pool1
I0506 00:34:46.583695 25403 net.cpp:408] conv2 -> conv2
I0506 00:34:46.608692 25403 net.cpp:150] Setting up conv2
I0506 00:34:46.608711 25403 net.cpp:157] Top shape: 512 150 18 18 (24883200)
I0506 00:34:46.608716 25403 net.cpp:165] Memory required for data: 1649082368
I0506 00:34:46.608728 25403 layer_factory.hpp:77] Creating layer conv2_prescale
I0506 00:34:46.608741 25403 net.cpp:100] Creating Layer conv2_prescale
I0506 00:34:46.608745 25403 net.cpp:434] conv2_prescale <- conv2
I0506 00:34:46.608752 25403 net.cpp:395] conv2_prescale -> conv2 (in-place)
I0506 00:34:46.608873 25403 net.cpp:150] Setting up conv2_prescale
I0506 00:34:46.608882 25403 net.cpp:157] Top shape: 512 150 18 18 (24883200)
I0506 00:34:46.608886 25403 net.cpp:165] Memory required for data: 1748615168
I0506 00:34:46.608891 25403 layer_factory.hpp:77] Creating layer conv2_sTanH
I0506 00:34:46.608898 25403 net.cpp:100] Creating Layer conv2_sTanH
I0506 00:34:46.608902 25403 net.cpp:434] conv2_sTanH <- conv2
I0506 00:34:46.608906 25403 net.cpp:395] conv2_sTanH -> conv2 (in-place)
I0506 00:34:46.611439 25403 net.cpp:150] Setting up conv2_sTanH
I0506 00:34:46.611456 25403 net.cpp:157] Top shape: 512 150 18 18 (24883200)
I0506 00:34:46.611459 25403 net.cpp:165] Memory required for data: 1848147968
I0506 00:34:46.611464 25403 layer_factory.hpp:77] Creating layer conv2_postscale
I0506 00:34:46.611470 25403 net.cpp:100] Creating Layer conv2_postscale
I0506 00:34:46.611477 25403 net.cpp:434] conv2_postscale <- conv2
I0506 00:34:46.611484 25403 net.cpp:395] conv2_postscale -> conv2 (in-place)
I0506 00:34:46.611590 25403 net.cpp:150] Setting up conv2_postscale
I0506 00:34:46.611599 25403 net.cpp:157] Top shape: 512 150 18 18 (24883200)
I0506 00:34:46.611603 25403 net.cpp:165] Memory required for data: 1947680768
I0506 00:34:46.611608 25403 layer_factory.hpp:77] Creating layer pool2
I0506 00:34:46.611616 25403 net.cpp:100] Creating Layer pool2
I0506 00:34:46.611620 25403 net.cpp:434] pool2 <- conv2
I0506 00:34:46.611627 25403 net.cpp:408] pool2 -> pool2
I0506 00:34:46.611672 25403 net.cpp:150] Setting up pool2
I0506 00:34:46.611681 25403 net.cpp:157] Top shape: 512 150 9 9 (6220800)
I0506 00:34:46.611685 25403 net.cpp:165] Memory required for data: 1972563968
I0506 00:34:46.611690 25403 layer_factory.hpp:77] Creating layer conv3
I0506 00:34:46.611698 25403 net.cpp:100] Creating Layer conv3
I0506 00:34:46.611703 25403 net.cpp:434] conv3 <- pool2
I0506 00:34:46.611708 25403 net.cpp:408] conv3 -> conv3
I0506 00:34:46.617722 25403 net.cpp:150] Setting up conv3
I0506 00:34:46.617739 25403 net.cpp:157] Top shape: 512 250 6 6 (4608000)
I0506 00:34:46.617743 25403 net.cpp:165] Memory required for data: 1990995968
I0506 00:34:46.617753 25403 layer_factory.hpp:77] Creating layer conv3_prescale
I0506 00:34:46.617764 25403 net.cpp:100] Creating Layer conv3_prescale
I0506 00:34:46.617789 25403 net.cpp:434] conv3_prescale <- conv3
I0506 00:34:46.617797 25403 net.cpp:395] conv3_prescale -> conv3 (in-place)
I0506 00:34:46.617900 25403 net.cpp:150] Setting up conv3_prescale
I0506 00:34:46.617910 25403 net.cpp:157] Top shape: 512 250 6 6 (4608000)
I0506 00:34:46.617914 25403 net.cpp:165] Memory required for data: 2009427968
I0506 00:34:46.617919 25403 layer_factory.hpp:77] Creating layer conv3_sTanH
I0506 00:34:46.617926 25403 net.cpp:100] Creating Layer conv3_sTanH
I0506 00:34:46.617931 25403 net.cpp:434] conv3_sTanH <- conv3
I0506 00:34:46.617935 25403 net.cpp:395] conv3_sTanH -> conv3 (in-place)
I0506 00:34:46.619443 25403 net.cpp:150] Setting up conv3_sTanH
I0506 00:34:46.619462 25403 net.cpp:157] Top shape: 512 250 6 6 (4608000)
I0506 00:34:46.619467 25403 net.cpp:165] Memory required for data: 2027859968
I0506 00:34:46.619469 25403 layer_factory.hpp:77] Creating layer conv3_postscale
I0506 00:34:46.619478 25403 net.cpp:100] Creating Layer conv3_postscale
I0506 00:34:46.619482 25403 net.cpp:434] conv3_postscale <- conv3
I0506 00:34:46.619491 25403 net.cpp:395] conv3_postscale -> conv3 (in-place)
I0506 00:34:46.619596 25403 net.cpp:150] Setting up conv3_postscale
I0506 00:34:46.619606 25403 net.cpp:157] Top shape: 512 250 6 6 (4608000)
I0506 00:34:46.619611 25403 net.cpp:165] Memory required for data: 2046291968
I0506 00:34:46.619616 25403 layer_factory.hpp:77] Creating layer pool3
I0506 00:34:46.619626 25403 net.cpp:100] Creating Layer pool3
I0506 00:34:46.619630 25403 net.cpp:434] pool3 <- conv3
I0506 00:34:46.619637 25403 net.cpp:408] pool3 -> pool3
I0506 00:34:46.619678 25403 net.cpp:150] Setting up pool3
I0506 00:34:46.619685 25403 net.cpp:157] Top shape: 512 250 3 3 (1152000)
I0506 00:34:46.619689 25403 net.cpp:165] Memory required for data: 2050899968
I0506 00:34:46.619693 25403 layer_factory.hpp:77] Creating layer fc4_300
I0506 00:34:46.619700 25403 net.cpp:100] Creating Layer fc4_300
I0506 00:34:46.619705 25403 net.cpp:434] fc4_300 <- pool3
I0506 00:34:46.619711 25403 net.cpp:408] fc4_300 -> fc4_300
I0506 00:34:46.626960 25403 net.cpp:150] Setting up fc4_300
I0506 00:34:46.626977 25403 net.cpp:157] Top shape: 512 300 (153600)
I0506 00:34:46.626983 25403 net.cpp:165] Memory required for data: 2051514368
I0506 00:34:46.626991 25403 layer_factory.hpp:77] Creating layer fc4_prescale
I0506 00:34:46.627003 25403 net.cpp:100] Creating Layer fc4_prescale
I0506 00:34:46.627008 25403 net.cpp:434] fc4_prescale <- fc4_300
I0506 00:34:46.627014 25403 net.cpp:395] fc4_prescale -> fc4_300 (in-place)
I0506 00:34:46.627110 25403 net.cpp:150] Setting up fc4_prescale
I0506 00:34:46.627120 25403 net.cpp:157] Top shape: 512 300 (153600)
I0506 00:34:46.627125 25403 net.cpp:165] Memory required for data: 2052128768
I0506 00:34:46.627130 25403 layer_factory.hpp:77] Creating layer fc4_sTanH
I0506 00:34:46.627135 25403 net.cpp:100] Creating Layer fc4_sTanH
I0506 00:34:46.627140 25403 net.cpp:434] fc4_sTanH <- fc4_300
I0506 00:34:46.627148 25403 net.cpp:395] fc4_sTanH -> fc4_300 (in-place)
I0506 00:34:46.627349 25403 net.cpp:150] Setting up fc4_sTanH
I0506 00:34:46.627362 25403 net.cpp:157] Top shape: 512 300 (153600)
I0506 00:34:46.627367 25403 net.cpp:165] Memory required for data: 2052743168
I0506 00:34:46.627370 25403 layer_factory.hpp:77] Creating layer fc4_postscale
I0506 00:34:46.627377 25403 net.cpp:100] Creating Layer fc4_postscale
I0506 00:34:46.627382 25403 net.cpp:434] fc4_postscale <- fc4_300
I0506 00:34:46.627388 25403 net.cpp:395] fc4_postscale -> fc4_300 (in-place)
I0506 00:34:46.627492 25403 net.cpp:150] Setting up fc4_postscale
I0506 00:34:46.627501 25403 net.cpp:157] Top shape: 512 300 (153600)
I0506 00:34:46.627503 25403 net.cpp:165] Memory required for data: 2053357568
I0506 00:34:46.627508 25403 layer_factory.hpp:77] Creating layer fc5_116
I0506 00:34:46.627516 25403 net.cpp:100] Creating Layer fc5_116
I0506 00:34:46.627521 25403 net.cpp:434] fc5_116 <- fc4_300
I0506 00:34:46.627526 25403 net.cpp:408] fc5_116 -> fc5_classes
I0506 00:34:46.629220 25403 net.cpp:150] Setting up fc5_116
I0506 00:34:46.629251 25403 net.cpp:157] Top shape: 512 116 (59392)
I0506 00:34:46.629254 25403 net.cpp:165] Memory required for data: 2053595136
I0506 00:34:46.629266 25403 layer_factory.hpp:77] Creating layer fc5_classes_fc5_116_0_split
I0506 00:34:46.629277 25403 net.cpp:100] Creating Layer fc5_classes_fc5_116_0_split
I0506 00:34:46.629283 25403 net.cpp:434] fc5_classes_fc5_116_0_split <- fc5_classes
I0506 00:34:46.629289 25403 net.cpp:408] fc5_classes_fc5_116_0_split -> fc5_classes_fc5_116_0_split_0
I0506 00:34:46.629298 25403 net.cpp:408] fc5_classes_fc5_116_0_split -> fc5_classes_fc5_116_0_split_1
I0506 00:34:46.629305 25403 net.cpp:408] fc5_classes_fc5_116_0_split -> fc5_classes_fc5_116_0_split_2
I0506 00:34:46.629364 25403 net.cpp:150] Setting up fc5_classes_fc5_116_0_split
I0506 00:34:46.629371 25403 net.cpp:157] Top shape: 512 116 (59392)
I0506 00:34:46.629374 25403 net.cpp:157] Top shape: 512 116 (59392)
I0506 00:34:46.629377 25403 net.cpp:157] Top shape: 512 116 (59392)
I0506 00:34:46.629380 25403 net.cpp:165] Memory required for data: 2054307840
I0506 00:34:46.629384 25403 layer_factory.hpp:77] Creating layer softmax
I0506 00:34:46.629390 25403 net.cpp:100] Creating Layer softmax
I0506 00:34:46.629395 25403 net.cpp:434] softmax <- fc5_classes_fc5_116_0_split_0
I0506 00:34:46.629400 25403 net.cpp:408] softmax -> softmax
I0506 00:34:46.629665 25403 net.cpp:150] Setting up softmax
I0506 00:34:46.629680 25403 net.cpp:157] Top shape: 512 116 (59392)
I0506 00:34:46.629683 25403 net.cpp:165] Memory required for data: 2054545408
I0506 00:34:46.629686 25403 layer_factory.hpp:77] Creating layer loss
I0506 00:34:46.629693 25403 net.cpp:100] Creating Layer loss
I0506 00:34:46.629696 25403 net.cpp:434] loss <- softmax
I0506 00:34:46.629700 25403 net.cpp:434] loss <- label_data_1_split_0
I0506 00:34:46.629707 25403 net.cpp:408] loss -> loss
I0506 00:34:46.629737 25403 net.cpp:150] Setting up loss
I0506 00:34:46.629745 25403 net.cpp:157] Top shape: (1)
I0506 00:34:46.629747 25403 net.cpp:160]     with loss weight 1
I0506 00:34:46.629770 25403 net.cpp:165] Memory required for data: 2054545412
I0506 00:34:46.629775 25403 layer_factory.hpp:77] Creating layer accuracy_1
I0506 00:34:46.629783 25403 net.cpp:100] Creating Layer accuracy_1
I0506 00:34:46.629788 25403 net.cpp:434] accuracy_1 <- fc5_classes_fc5_116_0_split_1
I0506 00:34:46.629792 25403 net.cpp:434] accuracy_1 <- label_data_1_split_1
I0506 00:34:46.629797 25403 net.cpp:408] accuracy_1 -> accuracy_1
I0506 00:34:46.629811 25403 net.cpp:150] Setting up accuracy_1
I0506 00:34:46.629817 25403 net.cpp:157] Top shape: (1)
I0506 00:34:46.629818 25403 net.cpp:165] Memory required for data: 2054545416
I0506 00:34:46.629822 25403 layer_factory.hpp:77] Creating layer accuracy_5
I0506 00:34:46.629827 25403 net.cpp:100] Creating Layer accuracy_5
I0506 00:34:46.629829 25403 net.cpp:434] accuracy_5 <- fc5_classes_fc5_116_0_split_2
I0506 00:34:46.629833 25403 net.cpp:434] accuracy_5 <- label_data_1_split_2
I0506 00:34:46.629838 25403 net.cpp:408] accuracy_5 -> accuracy_5
I0506 00:34:46.629845 25403 net.cpp:150] Setting up accuracy_5
I0506 00:34:46.629851 25403 net.cpp:157] Top shape: (1)
I0506 00:34:46.629853 25403 net.cpp:165] Memory required for data: 2054545420
I0506 00:34:46.629856 25403 layer_factory.hpp:77] Creating layer silence
I0506 00:34:46.629863 25403 net.cpp:100] Creating Layer silence
I0506 00:34:46.629866 25403 net.cpp:434] silence <- accuracy_1
I0506 00:34:46.629870 25403 net.cpp:434] silence <- accuracy_5
I0506 00:34:46.629874 25403 net.cpp:150] Setting up silence
I0506 00:34:46.629878 25403 net.cpp:165] Memory required for data: 2054545420
I0506 00:34:46.629881 25403 net.cpp:228] silence does not need backward computation.
I0506 00:34:46.629890 25403 net.cpp:228] accuracy_5 does not need backward computation.
I0506 00:34:46.629894 25403 net.cpp:228] accuracy_1 does not need backward computation.
I0506 00:34:46.629899 25403 net.cpp:226] loss needs backward computation.
I0506 00:34:46.629901 25403 net.cpp:226] softmax needs backward computation.
I0506 00:34:46.629917 25403 net.cpp:226] fc5_classes_fc5_116_0_split needs backward computation.
I0506 00:34:46.629921 25403 net.cpp:226] fc5_116 needs backward computation.
I0506 00:34:46.629925 25403 net.cpp:226] fc4_postscale needs backward computation.
I0506 00:34:46.629930 25403 net.cpp:226] fc4_sTanH needs backward computation.
I0506 00:34:46.629932 25403 net.cpp:226] fc4_prescale needs backward computation.
I0506 00:34:46.629935 25403 net.cpp:226] fc4_300 needs backward computation.
I0506 00:34:46.629938 25403 net.cpp:226] pool3 needs backward computation.
I0506 00:34:46.629941 25403 net.cpp:226] conv3_postscale needs backward computation.
I0506 00:34:46.629945 25403 net.cpp:226] conv3_sTanH needs backward computation.
I0506 00:34:46.629947 25403 net.cpp:226] conv3_prescale needs backward computation.
I0506 00:34:46.629950 25403 net.cpp:226] conv3 needs backward computation.
I0506 00:34:46.629952 25403 net.cpp:226] pool2 needs backward computation.
I0506 00:34:46.629956 25403 net.cpp:226] conv2_postscale needs backward computation.
I0506 00:34:46.629958 25403 net.cpp:226] conv2_sTanH needs backward computation.
I0506 00:34:46.629961 25403 net.cpp:226] conv2_prescale needs backward computation.
I0506 00:34:46.629964 25403 net.cpp:226] conv2 needs backward computation.
I0506 00:34:46.629967 25403 net.cpp:226] pool1 needs backward computation.
I0506 00:34:46.629971 25403 net.cpp:226] conv1_postscale needs backward computation.
I0506 00:34:46.629973 25403 net.cpp:226] conv1_sTanH needs backward computation.
I0506 00:34:46.629976 25403 net.cpp:226] conv1_prescale needs backward computation.
I0506 00:34:46.629979 25403 net.cpp:226] conv1 needs backward computation.
I0506 00:34:46.629983 25403 net.cpp:228] label_data_1_split does not need backward computation.
I0506 00:34:46.629992 25403 net.cpp:228] data does not need backward computation.
I0506 00:34:46.629997 25403 net.cpp:270] This network produces output loss
I0506 00:34:46.630018 25403 net.cpp:283] Network initialization done.
I0506 00:34:46.630311 25403 solver.cpp:193] Creating test net (#0) specified by test_net file: ./Prototxt/experiment_13/RTSD/orig/trial_1/test.prototxt
I0506 00:34:46.630494 25403 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.0039215689
    mirror: false
    crop_size: 48
    mean_value: 121
    mean_value: 117
    mean_value: 120
  }
  data_param {
    source: "../local_data/lmdb/RTSD/orig/test/lmdb"
    batch_size: 512
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 7
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_prescale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv1_sTanH"
  type: "TanH"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_postscale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 150
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_prescale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv2_sTanH"
  type: "TanH"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_postscale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 250
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_prescale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv3_sTanH"
  type: "TanH"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_postscale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc4_300"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc4_300"
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fc4_prescale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "fc4_sTanH"
  type: "TanH"
  bottom: "fc4_300"
  top: "fc4_300"
}
layer {
  name: "fc4_postscale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "fc5_116"
  type: "InnerProduct"
  bottom: "fc4_300"
  top: "fc5_classes"
  inner_product_param {
    num_output: 116
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "softmax"
  type: "Softmax"
  bottom: "fc5_classes"
  top: "softmax"
}
layer {
  name: "loss"
  type: "MultinomialLogisticLoss"
  bottom: "softmax"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy_1"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_1"
  accuracy_param {
    top_k: 1
  }
}
layer {
  name: "accuracy_5"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_5"
  accuracy_param {
    top_k: 5
  }
}
I0506 00:34:46.630610 25403 layer_factory.hpp:77] Creating layer data
I0506 00:34:46.630970 25403 net.cpp:100] Creating Layer data
I0506 00:34:46.630982 25403 net.cpp:408] data -> data
I0506 00:34:46.630991 25403 net.cpp:408] data -> label
I0506 00:34:46.647047 25836 db_lmdb.cpp:35] Opened lmdb ../local_data/lmdb/RTSD/orig/test/lmdb
I0506 00:34:46.647308 25403 data_layer.cpp:41] output data size: 512,3,48,48
I0506 00:34:46.693267 25403 net.cpp:150] Setting up data
I0506 00:34:46.693300 25403 net.cpp:157] Top shape: 512 3 48 48 (3538944)
I0506 00:34:46.693305 25403 net.cpp:157] Top shape: 512 (512)
I0506 00:34:46.693310 25403 net.cpp:165] Memory required for data: 14157824
I0506 00:34:46.693316 25403 layer_factory.hpp:77] Creating layer label_data_1_split
I0506 00:34:46.693334 25403 net.cpp:100] Creating Layer label_data_1_split
I0506 00:34:46.693339 25403 net.cpp:434] label_data_1_split <- label
I0506 00:34:46.693348 25403 net.cpp:408] label_data_1_split -> label_data_1_split_0
I0506 00:34:46.693361 25403 net.cpp:408] label_data_1_split -> label_data_1_split_1
I0506 00:34:46.693369 25403 net.cpp:408] label_data_1_split -> label_data_1_split_2
I0506 00:34:46.693509 25403 net.cpp:150] Setting up label_data_1_split
I0506 00:34:46.693521 25403 net.cpp:157] Top shape: 512 (512)
I0506 00:34:46.693526 25403 net.cpp:157] Top shape: 512 (512)
I0506 00:34:46.693531 25403 net.cpp:157] Top shape: 512 (512)
I0506 00:34:46.693533 25403 net.cpp:165] Memory required for data: 14163968
I0506 00:34:46.693537 25403 layer_factory.hpp:77] Creating layer conv1
I0506 00:34:46.693577 25403 net.cpp:100] Creating Layer conv1
I0506 00:34:46.693583 25403 net.cpp:434] conv1 <- data
I0506 00:34:46.693590 25403 net.cpp:408] conv1 -> conv1
I0506 00:34:46.701189 25403 net.cpp:150] Setting up conv1
I0506 00:34:46.701215 25403 net.cpp:157] Top shape: 512 100 42 42 (90316800)
I0506 00:34:46.701218 25403 net.cpp:165] Memory required for data: 375431168
I0506 00:34:46.701231 25403 layer_factory.hpp:77] Creating layer conv1_prescale
I0506 00:34:46.701241 25403 net.cpp:100] Creating Layer conv1_prescale
I0506 00:34:46.701246 25403 net.cpp:434] conv1_prescale <- conv1
I0506 00:34:46.701252 25403 net.cpp:395] conv1_prescale -> conv1 (in-place)
I0506 00:34:46.701369 25403 net.cpp:150] Setting up conv1_prescale
I0506 00:34:46.701378 25403 net.cpp:157] Top shape: 512 100 42 42 (90316800)
I0506 00:34:46.701381 25403 net.cpp:165] Memory required for data: 736698368
I0506 00:34:46.701388 25403 layer_factory.hpp:77] Creating layer conv1_sTanH
I0506 00:34:46.701400 25403 net.cpp:100] Creating Layer conv1_sTanH
I0506 00:34:46.701405 25403 net.cpp:434] conv1_sTanH <- conv1
I0506 00:34:46.701409 25403 net.cpp:395] conv1_sTanH -> conv1 (in-place)
I0506 00:34:46.701611 25403 net.cpp:150] Setting up conv1_sTanH
I0506 00:34:46.701624 25403 net.cpp:157] Top shape: 512 100 42 42 (90316800)
I0506 00:34:46.701629 25403 net.cpp:165] Memory required for data: 1097965568
I0506 00:34:46.701632 25403 layer_factory.hpp:77] Creating layer conv1_postscale
I0506 00:34:46.701639 25403 net.cpp:100] Creating Layer conv1_postscale
I0506 00:34:46.701643 25403 net.cpp:434] conv1_postscale <- conv1
I0506 00:34:46.701650 25403 net.cpp:395] conv1_postscale -> conv1 (in-place)
I0506 00:34:46.701766 25403 net.cpp:150] Setting up conv1_postscale
I0506 00:34:46.701774 25403 net.cpp:157] Top shape: 512 100 42 42 (90316800)
I0506 00:34:46.701778 25403 net.cpp:165] Memory required for data: 1459232768
I0506 00:34:46.701783 25403 layer_factory.hpp:77] Creating layer pool1
I0506 00:34:46.701792 25403 net.cpp:100] Creating Layer pool1
I0506 00:34:46.701797 25403 net.cpp:434] pool1 <- conv1
I0506 00:34:46.701803 25403 net.cpp:408] pool1 -> pool1
I0506 00:34:46.701848 25403 net.cpp:150] Setting up pool1
I0506 00:34:46.701856 25403 net.cpp:157] Top shape: 512 100 21 21 (22579200)
I0506 00:34:46.701859 25403 net.cpp:165] Memory required for data: 1549549568
I0506 00:34:46.701863 25403 layer_factory.hpp:77] Creating layer conv2
I0506 00:34:46.701872 25403 net.cpp:100] Creating Layer conv2
I0506 00:34:46.701875 25403 net.cpp:434] conv2 <- pool1
I0506 00:34:46.701882 25403 net.cpp:408] conv2 -> conv2
I0506 00:34:46.711792 25403 net.cpp:150] Setting up conv2
I0506 00:34:46.711812 25403 net.cpp:157] Top shape: 512 150 18 18 (24883200)
I0506 00:34:46.711817 25403 net.cpp:165] Memory required for data: 1649082368
I0506 00:34:46.711828 25403 layer_factory.hpp:77] Creating layer conv2_prescale
I0506 00:34:46.711840 25403 net.cpp:100] Creating Layer conv2_prescale
I0506 00:34:46.711843 25403 net.cpp:434] conv2_prescale <- conv2
I0506 00:34:46.711849 25403 net.cpp:395] conv2_prescale -> conv2 (in-place)
I0506 00:34:46.711990 25403 net.cpp:150] Setting up conv2_prescale
I0506 00:34:46.712002 25403 net.cpp:157] Top shape: 512 150 18 18 (24883200)
I0506 00:34:46.712005 25403 net.cpp:165] Memory required for data: 1748615168
I0506 00:34:46.712010 25403 layer_factory.hpp:77] Creating layer conv2_sTanH
I0506 00:34:46.712016 25403 net.cpp:100] Creating Layer conv2_sTanH
I0506 00:34:46.712021 25403 net.cpp:434] conv2_sTanH <- conv2
I0506 00:34:46.712028 25403 net.cpp:395] conv2_sTanH -> conv2 (in-place)
I0506 00:34:46.719588 25403 net.cpp:150] Setting up conv2_sTanH
I0506 00:34:46.719611 25403 net.cpp:157] Top shape: 512 150 18 18 (24883200)
I0506 00:34:46.719619 25403 net.cpp:165] Memory required for data: 1848147968
I0506 00:34:46.719624 25403 layer_factory.hpp:77] Creating layer conv2_postscale
I0506 00:34:46.719635 25403 net.cpp:100] Creating Layer conv2_postscale
I0506 00:34:46.719640 25403 net.cpp:434] conv2_postscale <- conv2
I0506 00:34:46.719666 25403 net.cpp:395] conv2_postscale -> conv2 (in-place)
I0506 00:34:46.720891 25403 net.cpp:150] Setting up conv2_postscale
I0506 00:34:46.720906 25403 net.cpp:157] Top shape: 512 150 18 18 (24883200)
I0506 00:34:46.720909 25403 net.cpp:165] Memory required for data: 1947680768
I0506 00:34:46.720916 25403 layer_factory.hpp:77] Creating layer pool2
I0506 00:34:46.720923 25403 net.cpp:100] Creating Layer pool2
I0506 00:34:46.720928 25403 net.cpp:434] pool2 <- conv2
I0506 00:34:46.720935 25403 net.cpp:408] pool2 -> pool2
I0506 00:34:46.720993 25403 net.cpp:150] Setting up pool2
I0506 00:34:46.721004 25403 net.cpp:157] Top shape: 512 150 9 9 (6220800)
I0506 00:34:46.721007 25403 net.cpp:165] Memory required for data: 1972563968
I0506 00:34:46.721010 25403 layer_factory.hpp:77] Creating layer conv3
I0506 00:34:46.721021 25403 net.cpp:100] Creating Layer conv3
I0506 00:34:46.721026 25403 net.cpp:434] conv3 <- pool2
I0506 00:34:46.721032 25403 net.cpp:408] conv3 -> conv3
I0506 00:34:46.733264 25403 net.cpp:150] Setting up conv3
I0506 00:34:46.733294 25403 net.cpp:157] Top shape: 512 250 6 6 (4608000)
I0506 00:34:46.733299 25403 net.cpp:165] Memory required for data: 1990995968
I0506 00:34:46.733312 25403 layer_factory.hpp:77] Creating layer conv3_prescale
I0506 00:34:46.733325 25403 net.cpp:100] Creating Layer conv3_prescale
I0506 00:34:46.733330 25403 net.cpp:434] conv3_prescale <- conv3
I0506 00:34:46.733337 25403 net.cpp:395] conv3_prescale -> conv3 (in-place)
I0506 00:34:46.733472 25403 net.cpp:150] Setting up conv3_prescale
I0506 00:34:46.733489 25403 net.cpp:157] Top shape: 512 250 6 6 (4608000)
I0506 00:34:46.733495 25403 net.cpp:165] Memory required for data: 2009427968
I0506 00:34:46.733501 25403 layer_factory.hpp:77] Creating layer conv3_sTanH
I0506 00:34:46.733508 25403 net.cpp:100] Creating Layer conv3_sTanH
I0506 00:34:46.733512 25403 net.cpp:434] conv3_sTanH <- conv3
I0506 00:34:46.733520 25403 net.cpp:395] conv3_sTanH -> conv3 (in-place)
I0506 00:34:46.734740 25403 net.cpp:150] Setting up conv3_sTanH
I0506 00:34:46.734756 25403 net.cpp:157] Top shape: 512 250 6 6 (4608000)
I0506 00:34:46.734761 25403 net.cpp:165] Memory required for data: 2027859968
I0506 00:34:46.734766 25403 layer_factory.hpp:77] Creating layer conv3_postscale
I0506 00:34:46.734776 25403 net.cpp:100] Creating Layer conv3_postscale
I0506 00:34:46.734781 25403 net.cpp:434] conv3_postscale <- conv3
I0506 00:34:46.734786 25403 net.cpp:395] conv3_postscale -> conv3 (in-place)
I0506 00:34:46.734900 25403 net.cpp:150] Setting up conv3_postscale
I0506 00:34:46.734910 25403 net.cpp:157] Top shape: 512 250 6 6 (4608000)
I0506 00:34:46.734915 25403 net.cpp:165] Memory required for data: 2046291968
I0506 00:34:46.734920 25403 layer_factory.hpp:77] Creating layer pool3
I0506 00:34:46.734930 25403 net.cpp:100] Creating Layer pool3
I0506 00:34:46.734935 25403 net.cpp:434] pool3 <- conv3
I0506 00:34:46.734941 25403 net.cpp:408] pool3 -> pool3
I0506 00:34:46.734988 25403 net.cpp:150] Setting up pool3
I0506 00:34:46.734997 25403 net.cpp:157] Top shape: 512 250 3 3 (1152000)
I0506 00:34:46.735002 25403 net.cpp:165] Memory required for data: 2050899968
I0506 00:34:46.735004 25403 layer_factory.hpp:77] Creating layer fc4_300
I0506 00:34:46.735011 25403 net.cpp:100] Creating Layer fc4_300
I0506 00:34:46.735015 25403 net.cpp:434] fc4_300 <- pool3
I0506 00:34:46.735023 25403 net.cpp:408] fc4_300 -> fc4_300
I0506 00:34:46.741441 25403 net.cpp:150] Setting up fc4_300
I0506 00:34:46.741461 25403 net.cpp:157] Top shape: 512 300 (153600)
I0506 00:34:46.741464 25403 net.cpp:165] Memory required for data: 2051514368
I0506 00:34:46.741472 25403 layer_factory.hpp:77] Creating layer fc4_prescale
I0506 00:34:46.741480 25403 net.cpp:100] Creating Layer fc4_prescale
I0506 00:34:46.741484 25403 net.cpp:434] fc4_prescale <- fc4_300
I0506 00:34:46.741492 25403 net.cpp:395] fc4_prescale -> fc4_300 (in-place)
I0506 00:34:46.741619 25403 net.cpp:150] Setting up fc4_prescale
I0506 00:34:46.741631 25403 net.cpp:157] Top shape: 512 300 (153600)
I0506 00:34:46.741636 25403 net.cpp:165] Memory required for data: 2052128768
I0506 00:34:46.741657 25403 layer_factory.hpp:77] Creating layer fc4_sTanH
I0506 00:34:46.741668 25403 net.cpp:100] Creating Layer fc4_sTanH
I0506 00:34:46.741673 25403 net.cpp:434] fc4_sTanH <- fc4_300
I0506 00:34:46.741678 25403 net.cpp:395] fc4_sTanH -> fc4_300 (in-place)
I0506 00:34:46.741924 25403 net.cpp:150] Setting up fc4_sTanH
I0506 00:34:46.742859 25403 net.cpp:157] Top shape: 512 300 (153600)
I0506 00:34:46.742871 25403 net.cpp:165] Memory required for data: 2052743168
I0506 00:34:46.742875 25403 layer_factory.hpp:77] Creating layer fc4_postscale
I0506 00:34:46.742887 25403 net.cpp:100] Creating Layer fc4_postscale
I0506 00:34:46.742890 25403 net.cpp:434] fc4_postscale <- fc4_300
I0506 00:34:46.742897 25403 net.cpp:395] fc4_postscale -> fc4_300 (in-place)
I0506 00:34:46.743015 25403 net.cpp:150] Setting up fc4_postscale
I0506 00:34:46.743024 25403 net.cpp:157] Top shape: 512 300 (153600)
I0506 00:34:46.743028 25403 net.cpp:165] Memory required for data: 2053357568
I0506 00:34:46.743039 25403 layer_factory.hpp:77] Creating layer fc5_116
I0506 00:34:46.743053 25403 net.cpp:100] Creating Layer fc5_116
I0506 00:34:46.743058 25403 net.cpp:434] fc5_116 <- fc4_300
I0506 00:34:46.743064 25403 net.cpp:408] fc5_116 -> fc5_classes
I0506 00:34:46.743424 25403 net.cpp:150] Setting up fc5_116
I0506 00:34:46.743434 25403 net.cpp:157] Top shape: 512 116 (59392)
I0506 00:34:46.743438 25403 net.cpp:165] Memory required for data: 2053595136
I0506 00:34:46.743448 25403 layer_factory.hpp:77] Creating layer fc5_classes_fc5_116_0_split
I0506 00:34:46.743455 25403 net.cpp:100] Creating Layer fc5_classes_fc5_116_0_split
I0506 00:34:46.743463 25403 net.cpp:434] fc5_classes_fc5_116_0_split <- fc5_classes
I0506 00:34:46.743470 25403 net.cpp:408] fc5_classes_fc5_116_0_split -> fc5_classes_fc5_116_0_split_0
I0506 00:34:46.743479 25403 net.cpp:408] fc5_classes_fc5_116_0_split -> fc5_classes_fc5_116_0_split_1
I0506 00:34:46.743485 25403 net.cpp:408] fc5_classes_fc5_116_0_split -> fc5_classes_fc5_116_0_split_2
I0506 00:34:46.743544 25403 net.cpp:150] Setting up fc5_classes_fc5_116_0_split
I0506 00:34:46.743552 25403 net.cpp:157] Top shape: 512 116 (59392)
I0506 00:34:46.743556 25403 net.cpp:157] Top shape: 512 116 (59392)
I0506 00:34:46.743558 25403 net.cpp:157] Top shape: 512 116 (59392)
I0506 00:34:46.743561 25403 net.cpp:165] Memory required for data: 2054307840
I0506 00:34:46.743564 25403 layer_factory.hpp:77] Creating layer softmax
I0506 00:34:46.743571 25403 net.cpp:100] Creating Layer softmax
I0506 00:34:46.743576 25403 net.cpp:434] softmax <- fc5_classes_fc5_116_0_split_0
I0506 00:34:46.743582 25403 net.cpp:408] softmax -> softmax
I0506 00:34:46.743852 25403 net.cpp:150] Setting up softmax
I0506 00:34:46.743870 25403 net.cpp:157] Top shape: 512 116 (59392)
I0506 00:34:46.743876 25403 net.cpp:165] Memory required for data: 2054545408
I0506 00:34:46.743896 25403 layer_factory.hpp:77] Creating layer loss
I0506 00:34:46.743904 25403 net.cpp:100] Creating Layer loss
I0506 00:34:46.743912 25403 net.cpp:434] loss <- softmax
I0506 00:34:46.743934 25403 net.cpp:434] loss <- label_data_1_split_0
I0506 00:34:46.743947 25403 net.cpp:408] loss -> loss
I0506 00:34:46.743998 25403 net.cpp:150] Setting up loss
I0506 00:34:46.744011 25403 net.cpp:157] Top shape: (1)
I0506 00:34:46.744017 25403 net.cpp:160]     with loss weight 1
I0506 00:34:46.744032 25403 net.cpp:165] Memory required for data: 2054545412
I0506 00:34:46.744038 25403 layer_factory.hpp:77] Creating layer accuracy_1
I0506 00:34:46.744050 25403 net.cpp:100] Creating Layer accuracy_1
I0506 00:34:46.744066 25403 net.cpp:434] accuracy_1 <- fc5_classes_fc5_116_0_split_1
I0506 00:34:46.744086 25403 net.cpp:434] accuracy_1 <- label_data_1_split_1
I0506 00:34:46.744107 25403 net.cpp:408] accuracy_1 -> accuracy_1
I0506 00:34:46.744123 25403 net.cpp:150] Setting up accuracy_1
I0506 00:34:46.744138 25403 net.cpp:157] Top shape: (1)
I0506 00:34:46.744148 25403 net.cpp:165] Memory required for data: 2054545416
I0506 00:34:46.744154 25403 layer_factory.hpp:77] Creating layer accuracy_5
I0506 00:34:46.744180 25403 net.cpp:100] Creating Layer accuracy_5
I0506 00:34:46.744189 25403 net.cpp:434] accuracy_5 <- fc5_classes_fc5_116_0_split_2
I0506 00:34:46.744194 25403 net.cpp:434] accuracy_5 <- label_data_1_split_2
I0506 00:34:46.744202 25403 net.cpp:408] accuracy_5 -> accuracy_5
I0506 00:34:46.744213 25403 net.cpp:150] Setting up accuracy_5
I0506 00:34:46.744220 25403 net.cpp:157] Top shape: (1)
I0506 00:34:46.744221 25403 net.cpp:165] Memory required for data: 2054545420
I0506 00:34:46.744225 25403 net.cpp:228] accuracy_5 does not need backward computation.
I0506 00:34:46.744228 25403 net.cpp:228] accuracy_1 does not need backward computation.
I0506 00:34:46.744233 25403 net.cpp:226] loss needs backward computation.
I0506 00:34:46.744236 25403 net.cpp:226] softmax needs backward computation.
I0506 00:34:46.744240 25403 net.cpp:226] fc5_classes_fc5_116_0_split needs backward computation.
I0506 00:34:46.744243 25403 net.cpp:226] fc5_116 needs backward computation.
I0506 00:34:46.744247 25403 net.cpp:226] fc4_postscale needs backward computation.
I0506 00:34:46.744249 25403 net.cpp:226] fc4_sTanH needs backward computation.
I0506 00:34:46.744252 25403 net.cpp:226] fc4_prescale needs backward computation.
I0506 00:34:46.744254 25403 net.cpp:226] fc4_300 needs backward computation.
I0506 00:34:46.744257 25403 net.cpp:226] pool3 needs backward computation.
I0506 00:34:46.744261 25403 net.cpp:226] conv3_postscale needs backward computation.
I0506 00:34:46.744263 25403 net.cpp:226] conv3_sTanH needs backward computation.
I0506 00:34:46.744266 25403 net.cpp:226] conv3_prescale needs backward computation.
I0506 00:34:46.744268 25403 net.cpp:226] conv3 needs backward computation.
I0506 00:34:46.744271 25403 net.cpp:226] pool2 needs backward computation.
I0506 00:34:46.744276 25403 net.cpp:226] conv2_postscale needs backward computation.
I0506 00:34:46.744278 25403 net.cpp:226] conv2_sTanH needs backward computation.
I0506 00:34:46.744282 25403 net.cpp:226] conv2_prescale needs backward computation.
I0506 00:34:46.744283 25403 net.cpp:226] conv2 needs backward computation.
I0506 00:34:46.744287 25403 net.cpp:226] pool1 needs backward computation.
I0506 00:34:46.744289 25403 net.cpp:226] conv1_postscale needs backward computation.
I0506 00:34:46.744293 25403 net.cpp:226] conv1_sTanH needs backward computation.
I0506 00:34:46.744297 25403 net.cpp:226] conv1_prescale needs backward computation.
I0506 00:34:46.744299 25403 net.cpp:226] conv1 needs backward computation.
I0506 00:34:46.744303 25403 net.cpp:228] label_data_1_split does not need backward computation.
I0506 00:34:46.744307 25403 net.cpp:228] data does not need backward computation.
I0506 00:34:46.744309 25403 net.cpp:270] This network produces output accuracy_1
I0506 00:34:46.744314 25403 net.cpp:270] This network produces output accuracy_5
I0506 00:34:46.744319 25403 net.cpp:270] This network produces output loss
I0506 00:34:46.744351 25403 net.cpp:283] Network initialization done.
I0506 00:34:46.744457 25403 solver.cpp:72] Solver scaffolding done.
I0506 00:34:46.745450 25403 caffe.cpp:251] Starting Optimization
I0506 00:34:46.745461 25403 solver.cpp:291] Solving 
I0506 00:34:46.745465 25403 solver.cpp:292] Learning Rate Policy: step
I0506 00:34:46.768667 25403 solver.cpp:349] Iteration 0, Testing net (#0)
I0506 00:34:46.776156 25403 net.cpp:693] Ignoring source layer silence
I0506 00:34:49.128276 25403 solver.cpp:416]     Test net output #0: accuracy_1 = 0.053596
I0506 00:34:49.128307 25403 solver.cpp:416]     Test net output #1: accuracy_5 = 0.120175
I0506 00:34:49.128322 25403 solver.cpp:416]     Test net output #2: loss = 4.68177 (* 1 = 4.68177 loss)
I0506 00:34:49.241802 25403 solver.cpp:240] Iteration 0, loss = 4.67689
I0506 00:34:49.241840 25403 solver.cpp:256]     Train net output #0: loss = 4.67689 (* 1 = 4.67689 loss)
I0506 00:34:49.241859 25403 sgd_solver.cpp:106] Iteration 0, lr = 1e-06
I0506 00:34:49.403888 25403 solver.cpp:240] Iteration 1, loss = 4.6873
I0506 00:34:49.403925 25403 solver.cpp:256]     Train net output #0: loss = 4.6873 (* 1 = 4.6873 loss)
I0506 00:34:49.403969 25403 sgd_solver.cpp:106] Iteration 1, lr = 1e-06
I0506 00:34:49.592228 25403 solver.cpp:240] Iteration 2, loss = 4.6551
I0506 00:34:49.592269 25403 solver.cpp:256]     Train net output #0: loss = 4.6551 (* 1 = 4.6551 loss)
I0506 00:34:49.592281 25403 sgd_solver.cpp:106] Iteration 2, lr = 1e-06
I0506 00:34:49.782171 25403 solver.cpp:240] Iteration 3, loss = 4.66871
I0506 00:34:49.782209 25403 solver.cpp:256]     Train net output #0: loss = 4.66871 (* 1 = 4.66871 loss)
I0506 00:34:49.782222 25403 sgd_solver.cpp:106] Iteration 3, lr = 1e-06
I0506 00:34:49.969650 25403 solver.cpp:240] Iteration 4, loss = 4.65443
I0506 00:34:49.969688 25403 solver.cpp:256]     Train net output #0: loss = 4.65443 (* 1 = 4.65443 loss)
I0506 00:34:49.969700 25403 sgd_solver.cpp:106] Iteration 4, lr = 1e-06
I0506 00:34:50.160125 25403 solver.cpp:240] Iteration 5, loss = 4.63409
I0506 00:34:50.160163 25403 solver.cpp:256]     Train net output #0: loss = 4.63409 (* 1 = 4.63409 loss)
I0506 00:34:50.160174 25403 sgd_solver.cpp:106] Iteration 5, lr = 1e-06
I0506 00:34:50.347214 25403 solver.cpp:240] Iteration 6, loss = 4.66498
I0506 00:34:50.347252 25403 solver.cpp:256]     Train net output #0: loss = 4.66498 (* 1 = 4.66498 loss)
I0506 00:34:50.347265 25403 sgd_solver.cpp:106] Iteration 6, lr = 1e-06
I0506 00:34:50.536286 25403 solver.cpp:240] Iteration 7, loss = 4.65083
I0506 00:34:50.536324 25403 solver.cpp:256]     Train net output #0: loss = 4.65083 (* 1 = 4.65083 loss)
I0506 00:34:50.536334 25403 sgd_solver.cpp:106] Iteration 7, lr = 1e-06
I0506 00:34:50.723104 25403 solver.cpp:240] Iteration 8, loss = 4.62717
I0506 00:34:50.723140 25403 solver.cpp:256]     Train net output #0: loss = 4.62717 (* 1 = 4.62717 loss)
I0506 00:34:50.723151 25403 sgd_solver.cpp:106] Iteration 8, lr = 1e-06
I0506 00:34:50.909570 25403 solver.cpp:240] Iteration 9, loss = 4.64483
I0506 00:34:50.909606 25403 solver.cpp:256]     Train net output #0: loss = 4.64483 (* 1 = 4.64483 loss)
I0506 00:34:50.909617 25403 sgd_solver.cpp:106] Iteration 9, lr = 1e-06
I0506 00:34:51.098145 25403 solver.cpp:240] Iteration 10, loss = 4.58815
I0506 00:34:51.098181 25403 solver.cpp:256]     Train net output #0: loss = 4.58815 (* 1 = 4.58815 loss)
I0506 00:34:51.098192 25403 sgd_solver.cpp:106] Iteration 10, lr = 1e-06
I0506 00:34:51.286254 25403 solver.cpp:240] Iteration 11, loss = 4.64552
I0506 00:34:51.286291 25403 solver.cpp:256]     Train net output #0: loss = 4.64552 (* 1 = 4.64552 loss)
I0506 00:34:51.286301 25403 sgd_solver.cpp:106] Iteration 11, lr = 1e-06
I0506 00:34:51.472612 25403 solver.cpp:240] Iteration 12, loss = 4.6246
I0506 00:34:51.472652 25403 solver.cpp:256]     Train net output #0: loss = 4.6246 (* 1 = 4.6246 loss)
I0506 00:34:51.472663 25403 sgd_solver.cpp:106] Iteration 12, lr = 1e-06
I0506 00:34:51.662327 25403 solver.cpp:240] Iteration 13, loss = 4.60253
I0506 00:34:51.662370 25403 solver.cpp:256]     Train net output #0: loss = 4.60253 (* 1 = 4.60253 loss)
I0506 00:34:51.662380 25403 sgd_solver.cpp:106] Iteration 13, lr = 1e-06
I0506 00:34:51.849616 25403 solver.cpp:240] Iteration 14, loss = 4.63257
I0506 00:34:51.849653 25403 solver.cpp:256]     Train net output #0: loss = 4.63257 (* 1 = 4.63257 loss)
I0506 00:34:51.849664 25403 sgd_solver.cpp:106] Iteration 14, lr = 1e-06
I0506 00:34:52.036717 25403 solver.cpp:240] Iteration 15, loss = 4.58946
I0506 00:34:52.036756 25403 solver.cpp:256]     Train net output #0: loss = 4.58946 (* 1 = 4.58946 loss)
I0506 00:34:52.036768 25403 sgd_solver.cpp:106] Iteration 15, lr = 1e-06
I0506 00:34:52.226584 25403 solver.cpp:240] Iteration 16, loss = 4.63892
I0506 00:34:52.226620 25403 solver.cpp:256]     Train net output #0: loss = 4.63892 (* 1 = 4.63892 loss)
I0506 00:34:52.226631 25403 sgd_solver.cpp:106] Iteration 16, lr = 1e-06
I0506 00:34:52.413090 25403 solver.cpp:240] Iteration 17, loss = 4.61708
I0506 00:34:52.413128 25403 solver.cpp:256]     Train net output #0: loss = 4.61708 (* 1 = 4.61708 loss)
I0506 00:34:52.413139 25403 sgd_solver.cpp:106] Iteration 17, lr = 1e-06
I0506 00:34:52.601683 25403 solver.cpp:240] Iteration 18, loss = 4.62137
I0506 00:34:52.601719 25403 solver.cpp:256]     Train net output #0: loss = 4.62137 (* 1 = 4.62137 loss)
I0506 00:34:52.601730 25403 sgd_solver.cpp:106] Iteration 18, lr = 1e-06
I0506 00:34:52.790243 25403 solver.cpp:240] Iteration 19, loss = 4.5829
I0506 00:34:52.790277 25403 solver.cpp:256]     Train net output #0: loss = 4.5829 (* 1 = 4.5829 loss)
I0506 00:34:52.790288 25403 sgd_solver.cpp:106] Iteration 19, lr = 1e-06
I0506 00:34:52.977885 25403 solver.cpp:240] Iteration 20, loss = 4.60107
I0506 00:34:52.977922 25403 solver.cpp:256]     Train net output #0: loss = 4.60107 (* 1 = 4.60107 loss)
I0506 00:34:52.977934 25403 sgd_solver.cpp:106] Iteration 20, lr = 1e-06
I0506 00:34:53.168264 25403 solver.cpp:240] Iteration 21, loss = 4.60114
I0506 00:34:53.168295 25403 solver.cpp:256]     Train net output #0: loss = 4.60114 (* 1 = 4.60114 loss)
I0506 00:34:53.168306 25403 sgd_solver.cpp:106] Iteration 21, lr = 1e-06
I0506 00:34:53.355540 25403 solver.cpp:240] Iteration 22, loss = 4.59855
I0506 00:34:53.355576 25403 solver.cpp:256]     Train net output #0: loss = 4.59855 (* 1 = 4.59855 loss)
I0506 00:34:53.355587 25403 sgd_solver.cpp:106] Iteration 22, lr = 1e-06
I0506 00:34:53.544826 25403 solver.cpp:240] Iteration 23, loss = 4.56392
I0506 00:34:53.544869 25403 solver.cpp:256]     Train net output #0: loss = 4.56392 (* 1 = 4.56392 loss)
I0506 00:34:53.544881 25403 sgd_solver.cpp:106] Iteration 23, lr = 1e-06
I0506 00:34:53.739955 25403 solver.cpp:240] Iteration 24, loss = 4.56246
I0506 00:34:53.740079 25403 solver.cpp:256]     Train net output #0: loss = 4.56246 (* 1 = 4.56246 loss)
I0506 00:34:53.740116 25403 sgd_solver.cpp:106] Iteration 24, lr = 1e-06
I0506 00:34:53.928738 25403 solver.cpp:240] Iteration 25, loss = 4.57025
I0506 00:34:53.928777 25403 solver.cpp:256]     Train net output #0: loss = 4.57025 (* 1 = 4.57025 loss)
I0506 00:34:53.928789 25403 sgd_solver.cpp:106] Iteration 25, lr = 1e-06
I0506 00:34:54.114758 25403 solver.cpp:240] Iteration 26, loss = 4.61864
I0506 00:34:54.114801 25403 solver.cpp:256]     Train net output #0: loss = 4.61864 (* 1 = 4.61864 loss)
I0506 00:34:54.114812 25403 sgd_solver.cpp:106] Iteration 26, lr = 1e-06
I0506 00:34:54.302889 25403 solver.cpp:240] Iteration 27, loss = 4.56835
I0506 00:34:54.302945 25403 solver.cpp:256]     Train net output #0: loss = 4.56835 (* 1 = 4.56835 loss)
I0506 00:34:54.302965 25403 sgd_solver.cpp:106] Iteration 27, lr = 1e-06
I0506 00:34:54.492867 25403 solver.cpp:240] Iteration 28, loss = 4.54209
I0506 00:34:54.492905 25403 solver.cpp:256]     Train net output #0: loss = 4.54209 (* 1 = 4.54209 loss)
I0506 00:34:54.492916 25403 sgd_solver.cpp:106] Iteration 28, lr = 1e-06
I0506 00:34:54.680960 25403 solver.cpp:240] Iteration 29, loss = 4.56357
I0506 00:34:54.681020 25403 solver.cpp:256]     Train net output #0: loss = 4.56357 (* 1 = 4.56357 loss)
I0506 00:34:54.681041 25403 sgd_solver.cpp:106] Iteration 29, lr = 1e-06
I0506 00:34:54.872133 25403 solver.cpp:240] Iteration 30, loss = 4.55922
I0506 00:34:54.872169 25403 solver.cpp:256]     Train net output #0: loss = 4.55922 (* 1 = 4.55922 loss)
I0506 00:34:54.872181 25403 sgd_solver.cpp:106] Iteration 30, lr = 1e-06
I0506 00:34:55.058933 25403 solver.cpp:240] Iteration 31, loss = 4.56992
I0506 00:34:55.058974 25403 solver.cpp:256]     Train net output #0: loss = 4.56992 (* 1 = 4.56992 loss)
I0506 00:34:55.058985 25403 sgd_solver.cpp:106] Iteration 31, lr = 1e-06
I0506 00:34:55.247666 25403 solver.cpp:240] Iteration 32, loss = 4.56083
I0506 00:34:55.247709 25403 solver.cpp:256]     Train net output #0: loss = 4.56083 (* 1 = 4.56083 loss)
I0506 00:34:55.247720 25403 sgd_solver.cpp:106] Iteration 32, lr = 1e-06
I0506 00:34:55.437091 25403 solver.cpp:240] Iteration 33, loss = 4.54776
I0506 00:34:55.437144 25403 solver.cpp:256]     Train net output #0: loss = 4.54776 (* 1 = 4.54776 loss)
I0506 00:34:55.437157 25403 sgd_solver.cpp:106] Iteration 33, lr = 1e-06
I0506 00:34:55.624513 25403 solver.cpp:240] Iteration 34, loss = 4.57921
I0506 00:34:55.624577 25403 solver.cpp:256]     Train net output #0: loss = 4.57921 (* 1 = 4.57921 loss)
I0506 00:34:55.624591 25403 sgd_solver.cpp:106] Iteration 34, lr = 1e-06
I0506 00:34:55.814432 25403 solver.cpp:240] Iteration 35, loss = 4.51772
I0506 00:34:55.814468 25403 solver.cpp:256]     Train net output #0: loss = 4.51772 (* 1 = 4.51772 loss)
I0506 00:34:55.814479 25403 sgd_solver.cpp:106] Iteration 35, lr = 1e-06
I0506 00:34:56.002341 25403 solver.cpp:240] Iteration 36, loss = 4.55288
I0506 00:34:56.002377 25403 solver.cpp:256]     Train net output #0: loss = 4.55288 (* 1 = 4.55288 loss)
I0506 00:34:56.002388 25403 sgd_solver.cpp:106] Iteration 36, lr = 1e-06
I0506 00:34:56.192404 25403 solver.cpp:240] Iteration 37, loss = 4.53203
I0506 00:34:56.192441 25403 solver.cpp:256]     Train net output #0: loss = 4.53203 (* 1 = 4.53203 loss)
I0506 00:34:56.192452 25403 sgd_solver.cpp:106] Iteration 37, lr = 1e-06
I0506 00:34:56.381193 25403 solver.cpp:240] Iteration 38, loss = 4.50686
I0506 00:34:56.381229 25403 solver.cpp:256]     Train net output #0: loss = 4.50686 (* 1 = 4.50686 loss)
I0506 00:34:56.381240 25403 sgd_solver.cpp:106] Iteration 38, lr = 1e-06
I0506 00:34:56.569521 25403 solver.cpp:240] Iteration 39, loss = 4.52409
I0506 00:34:56.569561 25403 solver.cpp:256]     Train net output #0: loss = 4.52409 (* 1 = 4.52409 loss)
I0506 00:34:56.569572 25403 sgd_solver.cpp:106] Iteration 39, lr = 1e-06
I0506 00:34:56.759383 25403 solver.cpp:240] Iteration 40, loss = 4.52026
I0506 00:34:56.759419 25403 solver.cpp:256]     Train net output #0: loss = 4.52026 (* 1 = 4.52026 loss)
I0506 00:34:56.759430 25403 sgd_solver.cpp:106] Iteration 40, lr = 1e-06
I0506 00:34:56.946410 25403 solver.cpp:240] Iteration 41, loss = 4.55909
I0506 00:34:56.946446 25403 solver.cpp:256]     Train net output #0: loss = 4.55909 (* 1 = 4.55909 loss)
I0506 00:34:56.946458 25403 sgd_solver.cpp:106] Iteration 41, lr = 1e-06
I0506 00:34:57.136462 25403 solver.cpp:240] Iteration 42, loss = 4.4757
I0506 00:34:57.136502 25403 solver.cpp:256]     Train net output #0: loss = 4.4757 (* 1 = 4.4757 loss)
I0506 00:34:57.136514 25403 sgd_solver.cpp:106] Iteration 42, lr = 1e-06
I0506 00:34:57.324515 25403 solver.cpp:240] Iteration 43, loss = 4.52453
I0506 00:34:57.324554 25403 solver.cpp:256]     Train net output #0: loss = 4.52453 (* 1 = 4.52453 loss)
I0506 00:34:57.324566 25403 sgd_solver.cpp:106] Iteration 43, lr = 1e-06
I0506 00:34:57.513234 25403 solver.cpp:240] Iteration 44, loss = 4.48524
I0506 00:34:57.513272 25403 solver.cpp:256]     Train net output #0: loss = 4.48524 (* 1 = 4.48524 loss)
I0506 00:34:57.513284 25403 sgd_solver.cpp:106] Iteration 44, lr = 1e-06
I0506 00:34:57.704170 25403 solver.cpp:240] Iteration 45, loss = 4.52795
I0506 00:34:57.704206 25403 solver.cpp:256]     Train net output #0: loss = 4.52795 (* 1 = 4.52795 loss)
I0506 00:34:57.704218 25403 sgd_solver.cpp:106] Iteration 45, lr = 1e-06
I0506 00:34:57.891777 25403 solver.cpp:240] Iteration 46, loss = 4.53545
I0506 00:34:57.891811 25403 solver.cpp:256]     Train net output #0: loss = 4.53545 (* 1 = 4.53545 loss)
I0506 00:34:57.891822 25403 sgd_solver.cpp:106] Iteration 46, lr = 1e-06
I0506 00:34:58.082038 25403 solver.cpp:240] Iteration 47, loss = 4.50358
I0506 00:34:58.082077 25403 solver.cpp:256]     Train net output #0: loss = 4.50358 (* 1 = 4.50358 loss)
I0506 00:34:58.082088 25403 sgd_solver.cpp:106] Iteration 47, lr = 1e-06
I0506 00:34:58.268913 25403 solver.cpp:240] Iteration 48, loss = 4.49907
I0506 00:34:58.268949 25403 solver.cpp:256]     Train net output #0: loss = 4.49907 (* 1 = 4.49907 loss)
I0506 00:34:58.268960 25403 sgd_solver.cpp:106] Iteration 48, lr = 1e-06
I0506 00:34:58.458071 25403 solver.cpp:240] Iteration 49, loss = 4.48276
I0506 00:34:58.458109 25403 solver.cpp:256]     Train net output #0: loss = 4.48276 (* 1 = 4.48276 loss)
I0506 00:34:58.458120 25403 sgd_solver.cpp:106] Iteration 49, lr = 1e-06
I0506 00:34:58.646064 25403 solver.cpp:240] Iteration 50, loss = 4.5105
I0506 00:34:58.646107 25403 solver.cpp:256]     Train net output #0: loss = 4.5105 (* 1 = 4.5105 loss)
I0506 00:34:58.646147 25403 sgd_solver.cpp:106] Iteration 50, lr = 1e-06
I0506 00:34:58.834934 25403 solver.cpp:240] Iteration 51, loss = 4.49649
I0506 00:34:58.834974 25403 solver.cpp:256]     Train net output #0: loss = 4.49649 (* 1 = 4.49649 loss)
I0506 00:34:58.834985 25403 sgd_solver.cpp:106] Iteration 51, lr = 1e-06
I0506 00:34:59.025431 25403 solver.cpp:240] Iteration 52, loss = 4.49052
I0506 00:34:59.025468 25403 solver.cpp:256]     Train net output #0: loss = 4.49052 (* 1 = 4.49052 loss)
I0506 00:34:59.025480 25403 sgd_solver.cpp:106] Iteration 52, lr = 1e-06
I0506 00:34:59.213372 25403 solver.cpp:240] Iteration 53, loss = 4.48825
I0506 00:34:59.213408 25403 solver.cpp:256]     Train net output #0: loss = 4.48825 (* 1 = 4.48825 loss)
I0506 00:34:59.213420 25403 sgd_solver.cpp:106] Iteration 53, lr = 1e-06
I0506 00:34:59.406273 25403 solver.cpp:240] Iteration 54, loss = 4.47184
I0506 00:34:59.406311 25403 solver.cpp:256]     Train net output #0: loss = 4.47184 (* 1 = 4.47184 loss)
I0506 00:34:59.406322 25403 sgd_solver.cpp:106] Iteration 54, lr = 1e-06
I0506 00:34:59.592795 25403 solver.cpp:240] Iteration 55, loss = 4.443
I0506 00:34:59.592833 25403 solver.cpp:256]     Train net output #0: loss = 4.443 (* 1 = 4.443 loss)
I0506 00:34:59.592845 25403 sgd_solver.cpp:106] Iteration 55, lr = 1e-06
I0506 00:34:59.782030 25403 solver.cpp:240] Iteration 56, loss = 4.44507
I0506 00:34:59.782069 25403 solver.cpp:256]     Train net output #0: loss = 4.44507 (* 1 = 4.44507 loss)
I0506 00:34:59.782079 25403 sgd_solver.cpp:106] Iteration 56, lr = 1e-06
I0506 00:34:59.969823 25403 solver.cpp:240] Iteration 57, loss = 4.44236
I0506 00:34:59.969861 25403 solver.cpp:256]     Train net output #0: loss = 4.44236 (* 1 = 4.44236 loss)
I0506 00:34:59.969871 25403 sgd_solver.cpp:106] Iteration 57, lr = 1e-06
I0506 00:35:00.158362 25403 solver.cpp:240] Iteration 58, loss = 4.42161
I0506 00:35:00.158397 25403 solver.cpp:256]     Train net output #0: loss = 4.42161 (* 1 = 4.42161 loss)
I0506 00:35:00.158409 25403 sgd_solver.cpp:106] Iteration 58, lr = 1e-06
I0506 00:35:00.348378 25403 solver.cpp:240] Iteration 59, loss = 4.4434
I0506 00:35:00.348419 25403 solver.cpp:256]     Train net output #0: loss = 4.4434 (* 1 = 4.4434 loss)
I0506 00:35:00.348430 25403 sgd_solver.cpp:106] Iteration 59, lr = 1e-06
I0506 00:35:00.536526 25403 solver.cpp:240] Iteration 60, loss = 4.4286
I0506 00:35:00.536564 25403 solver.cpp:256]     Train net output #0: loss = 4.4286 (* 1 = 4.4286 loss)
I0506 00:35:00.536576 25403 sgd_solver.cpp:106] Iteration 60, lr = 1e-06
I0506 00:35:00.727994 25403 solver.cpp:240] Iteration 61, loss = 4.43706
I0506 00:35:00.728031 25403 solver.cpp:256]     Train net output #0: loss = 4.43706 (* 1 = 4.43706 loss)
I0506 00:35:00.728044 25403 sgd_solver.cpp:106] Iteration 61, lr = 1e-06
I0506 00:35:00.915272 25403 solver.cpp:240] Iteration 62, loss = 4.41369
I0506 00:35:00.915311 25403 solver.cpp:256]     Train net output #0: loss = 4.41369 (* 1 = 4.41369 loss)
I0506 00:35:00.915323 25403 sgd_solver.cpp:106] Iteration 62, lr = 1e-06
I0506 00:35:01.104863 25403 solver.cpp:240] Iteration 63, loss = 4.44438
I0506 00:35:01.104902 25403 solver.cpp:256]     Train net output #0: loss = 4.44438 (* 1 = 4.44438 loss)
I0506 00:35:01.104914 25403 sgd_solver.cpp:106] Iteration 63, lr = 1e-06
I0506 00:35:01.293560 25403 solver.cpp:240] Iteration 64, loss = 4.48455
I0506 00:35:01.293596 25403 solver.cpp:256]     Train net output #0: loss = 4.48455 (* 1 = 4.48455 loss)
I0506 00:35:01.293608 25403 sgd_solver.cpp:106] Iteration 64, lr = 1e-06
I0506 00:35:01.481873 25403 solver.cpp:240] Iteration 65, loss = 4.44004
I0506 00:35:01.481910 25403 solver.cpp:256]     Train net output #0: loss = 4.44004 (* 1 = 4.44004 loss)
I0506 00:35:01.481920 25403 sgd_solver.cpp:106] Iteration 65, lr = 1e-06
I0506 00:35:01.671872 25403 solver.cpp:240] Iteration 66, loss = 4.4297
I0506 00:35:01.671926 25403 solver.cpp:256]     Train net output #0: loss = 4.4297 (* 1 = 4.4297 loss)
I0506 00:35:01.671938 25403 sgd_solver.cpp:106] Iteration 66, lr = 1e-06
I0506 00:35:01.859833 25403 solver.cpp:240] Iteration 67, loss = 4.38099
I0506 00:35:01.859869 25403 solver.cpp:256]     Train net output #0: loss = 4.38099 (* 1 = 4.38099 loss)
I0506 00:35:01.859886 25403 sgd_solver.cpp:106] Iteration 67, lr = 1e-06
I0506 00:35:02.049814 25403 solver.cpp:240] Iteration 68, loss = 4.42604
I0506 00:35:02.049847 25403 solver.cpp:256]     Train net output #0: loss = 4.42604 (* 1 = 4.42604 loss)
I0506 00:35:02.049854 25403 sgd_solver.cpp:106] Iteration 68, lr = 1e-06
I0506 00:35:02.237745 25403 solver.cpp:240] Iteration 69, loss = 4.46751
I0506 00:35:02.237779 25403 solver.cpp:256]     Train net output #0: loss = 4.46751 (* 1 = 4.46751 loss)
I0506 00:35:02.237788 25403 sgd_solver.cpp:106] Iteration 69, lr = 1e-06
I0506 00:35:02.426714 25403 solver.cpp:240] Iteration 70, loss = 4.42263
I0506 00:35:02.426750 25403 solver.cpp:256]     Train net output #0: loss = 4.42263 (* 1 = 4.42263 loss)
I0506 00:35:02.426759 25403 sgd_solver.cpp:106] Iteration 70, lr = 1e-06
I0506 00:35:02.615926 25403 solver.cpp:240] Iteration 71, loss = 4.43273
I0506 00:35:02.615960 25403 solver.cpp:256]     Train net output #0: loss = 4.43273 (* 1 = 4.43273 loss)
I0506 00:35:02.615968 25403 sgd_solver.cpp:106] Iteration 71, lr = 1e-06
I0506 00:35:02.803174 25403 solver.cpp:240] Iteration 72, loss = 4.40446
I0506 00:35:02.803205 25403 solver.cpp:256]     Train net output #0: loss = 4.40446 (* 1 = 4.40446 loss)
I0506 00:35:02.803212 25403 sgd_solver.cpp:106] Iteration 72, lr = 1e-06
I0506 00:35:02.994736 25403 solver.cpp:240] Iteration 73, loss = 4.46579
I0506 00:35:02.994781 25403 solver.cpp:256]     Train net output #0: loss = 4.46579 (* 1 = 4.46579 loss)
I0506 00:35:02.994792 25403 sgd_solver.cpp:106] Iteration 73, lr = 1e-06
I0506 00:35:03.182489 25403 solver.cpp:240] Iteration 74, loss = 4.36819
I0506 00:35:03.182524 25403 solver.cpp:256]     Train net output #0: loss = 4.36819 (* 1 = 4.36819 loss)
I0506 00:35:03.182533 25403 sgd_solver.cpp:106] Iteration 74, lr = 1e-06
I0506 00:35:03.371712 25403 solver.cpp:240] Iteration 75, loss = 4.3963
I0506 00:35:03.371747 25403 solver.cpp:256]     Train net output #0: loss = 4.3963 (* 1 = 4.3963 loss)
I0506 00:35:03.371755 25403 sgd_solver.cpp:106] Iteration 75, lr = 1e-06
I0506 00:35:03.561657 25403 solver.cpp:240] Iteration 76, loss = 4.3919
I0506 00:35:03.561692 25403 solver.cpp:256]     Train net output #0: loss = 4.3919 (* 1 = 4.3919 loss)
I0506 00:35:03.561699 25403 sgd_solver.cpp:106] Iteration 76, lr = 1e-06
I0506 00:35:03.749438 25403 solver.cpp:240] Iteration 77, loss = 4.40968
I0506 00:35:03.749469 25403 solver.cpp:256]     Train net output #0: loss = 4.40968 (* 1 = 4.40968 loss)
I0506 00:35:03.749477 25403 sgd_solver.cpp:106] Iteration 77, lr = 1e-06
I0506 00:35:03.941004 25403 solver.cpp:240] Iteration 78, loss = 4.41492
I0506 00:35:03.941045 25403 solver.cpp:256]     Train net output #0: loss = 4.41492 (* 1 = 4.41492 loss)
I0506 00:35:03.941056 25403 sgd_solver.cpp:106] Iteration 78, lr = 1e-06
I0506 00:35:04.127924 25403 solver.cpp:240] Iteration 79, loss = 4.32793
I0506 00:35:04.127961 25403 solver.cpp:256]     Train net output #0: loss = 4.32793 (* 1 = 4.32793 loss)
I0506 00:35:04.127969 25403 sgd_solver.cpp:106] Iteration 79, lr = 1e-06
I0506 00:35:04.319113 25403 solver.cpp:240] Iteration 80, loss = 4.37788
I0506 00:35:04.319144 25403 solver.cpp:256]     Train net output #0: loss = 4.37788 (* 1 = 4.37788 loss)
I0506 00:35:04.319152 25403 sgd_solver.cpp:106] Iteration 80, lr = 1e-06
I0506 00:35:04.506775 25403 solver.cpp:240] Iteration 81, loss = 4.38463
I0506 00:35:04.506808 25403 solver.cpp:256]     Train net output #0: loss = 4.38463 (* 1 = 4.38463 loss)
I0506 00:35:04.506816 25403 sgd_solver.cpp:106] Iteration 81, lr = 1e-06
I0506 00:35:04.696650 25403 solver.cpp:240] Iteration 82, loss = 4.35052
I0506 00:35:04.696683 25403 solver.cpp:256]     Train net output #0: loss = 4.35052 (* 1 = 4.35052 loss)
I0506 00:35:04.696691 25403 sgd_solver.cpp:106] Iteration 82, lr = 1e-06
I0506 00:35:04.885359 25403 solver.cpp:240] Iteration 83, loss = 4.36964
I0506 00:35:04.885423 25403 solver.cpp:256]     Train net output #0: loss = 4.36964 (* 1 = 4.36964 loss)
I0506 00:35:04.885432 25403 sgd_solver.cpp:106] Iteration 83, lr = 1e-06
I0506 00:35:05.074681 25403 solver.cpp:240] Iteration 84, loss = 4.35065
I0506 00:35:05.074717 25403 solver.cpp:256]     Train net output #0: loss = 4.35065 (* 1 = 4.35065 loss)
I0506 00:35:05.074723 25403 sgd_solver.cpp:106] Iteration 84, lr = 1e-06
I0506 00:35:05.264902 25403 solver.cpp:240] Iteration 85, loss = 4.36933
I0506 00:35:05.264935 25403 solver.cpp:256]     Train net output #0: loss = 4.36933 (* 1 = 4.36933 loss)
I0506 00:35:05.264942 25403 sgd_solver.cpp:106] Iteration 85, lr = 1e-06
I0506 00:35:05.452416 25403 solver.cpp:240] Iteration 86, loss = 4.31778
I0506 00:35:05.452450 25403 solver.cpp:256]     Train net output #0: loss = 4.31778 (* 1 = 4.31778 loss)
I0506 00:35:05.452456 25403 sgd_solver.cpp:106] Iteration 86, lr = 1e-06
I0506 00:35:05.643695 25403 solver.cpp:240] Iteration 87, loss = 4.35469
I0506 00:35:05.643729 25403 solver.cpp:256]     Train net output #0: loss = 4.35469 (* 1 = 4.35469 loss)
I0506 00:35:05.643738 25403 sgd_solver.cpp:106] Iteration 87, lr = 1e-06
I0506 00:35:05.830950 25403 solver.cpp:240] Iteration 88, loss = 4.37053
I0506 00:35:05.830984 25403 solver.cpp:256]     Train net output #0: loss = 4.37053 (* 1 = 4.37053 loss)
I0506 00:35:05.830992 25403 sgd_solver.cpp:106] Iteration 88, lr = 1e-06
I0506 00:35:06.021064 25403 solver.cpp:240] Iteration 89, loss = 4.31044
I0506 00:35:06.021100 25403 solver.cpp:256]     Train net output #0: loss = 4.31044 (* 1 = 4.31044 loss)
I0506 00:35:06.021107 25403 sgd_solver.cpp:106] Iteration 89, lr = 1e-06
I0506 00:35:06.209233 25403 solver.cpp:240] Iteration 90, loss = 4.28381
I0506 00:35:06.209266 25403 solver.cpp:256]     Train net output #0: loss = 4.28381 (* 1 = 4.28381 loss)
I0506 00:35:06.209273 25403 sgd_solver.cpp:106] Iteration 90, lr = 1e-06
I0506 00:35:06.399248 25403 solver.cpp:240] Iteration 91, loss = 4.32118
I0506 00:35:06.399282 25403 solver.cpp:256]     Train net output #0: loss = 4.32118 (* 1 = 4.32118 loss)
I0506 00:35:06.399291 25403 sgd_solver.cpp:106] Iteration 91, lr = 1e-06
I0506 00:35:06.589339 25403 solver.cpp:240] Iteration 92, loss = 4.31378
I0506 00:35:06.589373 25403 solver.cpp:256]     Train net output #0: loss = 4.31378 (* 1 = 4.31378 loss)
I0506 00:35:06.589380 25403 sgd_solver.cpp:106] Iteration 92, lr = 1e-06
I0506 00:35:06.778519 25403 solver.cpp:240] Iteration 93, loss = 4.31231
I0506 00:35:06.778554 25403 solver.cpp:256]     Train net output #0: loss = 4.31231 (* 1 = 4.31231 loss)
I0506 00:35:06.778563 25403 sgd_solver.cpp:106] Iteration 93, lr = 1e-06
I0506 00:35:06.968494 25403 solver.cpp:240] Iteration 94, loss = 4.31312
I0506 00:35:06.968529 25403 solver.cpp:256]     Train net output #0: loss = 4.31312 (* 1 = 4.31312 loss)
I0506 00:35:06.968538 25403 sgd_solver.cpp:106] Iteration 94, lr = 1e-06
I0506 00:35:07.155679 25403 solver.cpp:240] Iteration 95, loss = 4.34314
I0506 00:35:07.155714 25403 solver.cpp:256]     Train net output #0: loss = 4.34314 (* 1 = 4.34314 loss)
I0506 00:35:07.155721 25403 sgd_solver.cpp:106] Iteration 95, lr = 1e-06
I0506 00:35:07.345894 25403 solver.cpp:240] Iteration 96, loss = 4.27507
I0506 00:35:07.345928 25403 solver.cpp:256]     Train net output #0: loss = 4.27507 (* 1 = 4.27507 loss)
I0506 00:35:07.345937 25403 sgd_solver.cpp:106] Iteration 96, lr = 1e-06
I0506 00:35:07.533696 25403 solver.cpp:240] Iteration 97, loss = 4.31637
I0506 00:35:07.533730 25403 solver.cpp:256]     Train net output #0: loss = 4.31637 (* 1 = 4.31637 loss)
I0506 00:35:07.533737 25403 sgd_solver.cpp:106] Iteration 97, lr = 1e-06
I0506 00:35:07.722416 25403 solver.cpp:240] Iteration 98, loss = 4.27703
I0506 00:35:07.722448 25403 solver.cpp:256]     Train net output #0: loss = 4.27703 (* 1 = 4.27703 loss)
I0506 00:35:07.722456 25403 sgd_solver.cpp:106] Iteration 98, lr = 1e-06
I0506 00:35:07.913967 25403 solver.cpp:240] Iteration 99, loss = 4.31576
I0506 00:35:07.914003 25403 solver.cpp:256]     Train net output #0: loss = 4.31576 (* 1 = 4.31576 loss)
I0506 00:35:07.914034 25403 sgd_solver.cpp:106] Iteration 99, lr = 1e-06
I0506 00:35:08.101830 25403 solver.cpp:240] Iteration 100, loss = 4.32486
I0506 00:35:08.101867 25403 solver.cpp:256]     Train net output #0: loss = 4.32486 (* 1 = 4.32486 loss)
I0506 00:35:08.101876 25403 sgd_solver.cpp:106] Iteration 100, lr = 1e-06
I0506 00:35:08.293198 25403 solver.cpp:240] Iteration 101, loss = 4.32163
I0506 00:35:08.293234 25403 solver.cpp:256]     Train net output #0: loss = 4.32163 (* 1 = 4.32163 loss)
I0506 00:35:08.293242 25403 sgd_solver.cpp:106] Iteration 101, lr = 1e-06
I0506 00:35:08.480886 25403 solver.cpp:240] Iteration 102, loss = 4.27884
I0506 00:35:08.480934 25403 solver.cpp:256]     Train net output #0: loss = 4.27884 (* 1 = 4.27884 loss)
I0506 00:35:08.480945 25403 sgd_solver.cpp:106] Iteration 102, lr = 1e-06
I0506 00:35:08.672505 25403 solver.cpp:240] Iteration 103, loss = 4.29777
I0506 00:35:08.672539 25403 solver.cpp:256]     Train net output #0: loss = 4.29777 (* 1 = 4.29777 loss)
I0506 00:35:08.672547 25403 sgd_solver.cpp:106] Iteration 103, lr = 1e-06
I0506 00:35:08.860669 25403 solver.cpp:240] Iteration 104, loss = 4.2281
I0506 00:35:08.860704 25403 solver.cpp:256]     Train net output #0: loss = 4.2281 (* 1 = 4.2281 loss)
I0506 00:35:08.860713 25403 sgd_solver.cpp:106] Iteration 104, lr = 1e-06
I0506 00:35:09.051007 25403 solver.cpp:240] Iteration 105, loss = 4.26053
I0506 00:35:09.051040 25403 solver.cpp:256]     Train net output #0: loss = 4.26053 (* 1 = 4.26053 loss)
I0506 00:35:09.051048 25403 sgd_solver.cpp:106] Iteration 105, lr = 1e-06
I0506 00:35:09.239102 25403 solver.cpp:240] Iteration 106, loss = 4.28766
I0506 00:35:09.239136 25403 solver.cpp:256]     Train net output #0: loss = 4.28766 (* 1 = 4.28766 loss)
I0506 00:35:09.239143 25403 sgd_solver.cpp:106] Iteration 106, lr = 1e-06
I0506 00:35:09.429214 25403 solver.cpp:240] Iteration 107, loss = 4.32674
I0506 00:35:09.429249 25403 solver.cpp:256]     Train net output #0: loss = 4.32674 (* 1 = 4.32674 loss)
I0506 00:35:09.429256 25403 sgd_solver.cpp:106] Iteration 107, lr = 1e-06
I0506 00:35:09.617324 25403 solver.cpp:240] Iteration 108, loss = 4.30528
I0506 00:35:09.617359 25403 solver.cpp:256]     Train net output #0: loss = 4.30528 (* 1 = 4.30528 loss)
I0506 00:35:09.617367 25403 sgd_solver.cpp:106] Iteration 108, lr = 1e-06
I0506 00:35:09.805371 25403 solver.cpp:240] Iteration 109, loss = 4.22785
I0506 00:35:09.805408 25403 solver.cpp:256]     Train net output #0: loss = 4.22785 (* 1 = 4.22785 loss)
I0506 00:35:09.805415 25403 sgd_solver.cpp:106] Iteration 109, lr = 1e-06
I0506 00:35:09.997489 25403 solver.cpp:240] Iteration 110, loss = 4.26739
I0506 00:35:09.997524 25403 solver.cpp:256]     Train net output #0: loss = 4.26739 (* 1 = 4.26739 loss)
I0506 00:35:09.997532 25403 sgd_solver.cpp:106] Iteration 110, lr = 1e-06
I0506 00:35:10.184998 25403 solver.cpp:240] Iteration 111, loss = 4.30405
I0506 00:35:10.185027 25403 solver.cpp:256]     Train net output #0: loss = 4.30405 (* 1 = 4.30405 loss)
I0506 00:35:10.185035 25403 sgd_solver.cpp:106] Iteration 111, lr = 1e-06
I0506 00:35:10.375228 25403 solver.cpp:240] Iteration 112, loss = 4.25312
I0506 00:35:10.375263 25403 solver.cpp:256]     Train net output #0: loss = 4.25312 (* 1 = 4.25312 loss)
I0506 00:35:10.375270 25403 sgd_solver.cpp:106] Iteration 112, lr = 1e-06
I0506 00:35:10.563114 25403 solver.cpp:240] Iteration 113, loss = 4.30286
I0506 00:35:10.563153 25403 solver.cpp:256]     Train net output #0: loss = 4.30286 (* 1 = 4.30286 loss)
I0506 00:35:10.563161 25403 sgd_solver.cpp:106] Iteration 113, lr = 1e-06
I0506 00:35:10.753125 25403 solver.cpp:240] Iteration 114, loss = 4.24502
I0506 00:35:10.753166 25403 solver.cpp:256]     Train net output #0: loss = 4.24502 (* 1 = 4.24502 loss)
I0506 00:35:10.753173 25403 sgd_solver.cpp:106] Iteration 114, lr = 1e-06
I0506 00:35:10.943271 25403 solver.cpp:240] Iteration 115, loss = 4.30679
I0506 00:35:10.943305 25403 solver.cpp:256]     Train net output #0: loss = 4.30679 (* 1 = 4.30679 loss)
I0506 00:35:10.943311 25403 sgd_solver.cpp:106] Iteration 115, lr = 1e-06
I0506 00:35:11.132958 25403 solver.cpp:240] Iteration 116, loss = 4.30909
I0506 00:35:11.138095 25403 solver.cpp:256]     Train net output #0: loss = 4.30909 (* 1 = 4.30909 loss)
I0506 00:35:11.138108 25403 sgd_solver.cpp:106] Iteration 116, lr = 1e-06
I0506 00:35:11.321923 25403 solver.cpp:240] Iteration 117, loss = 4.28853
I0506 00:35:11.321957 25403 solver.cpp:256]     Train net output #0: loss = 4.28853 (* 1 = 4.28853 loss)
I0506 00:35:11.321965 25403 sgd_solver.cpp:106] Iteration 117, lr = 1e-06
I0506 00:35:11.510030 25403 solver.cpp:240] Iteration 118, loss = 4.22682
I0506 00:35:11.510067 25403 solver.cpp:256]     Train net output #0: loss = 4.22682 (* 1 = 4.22682 loss)
I0506 00:35:11.510074 25403 sgd_solver.cpp:106] Iteration 118, lr = 1e-06
I0506 00:35:11.701442 25403 solver.cpp:240] Iteration 119, loss = 4.2279
I0506 00:35:11.701475 25403 solver.cpp:256]     Train net output #0: loss = 4.2279 (* 1 = 4.2279 loss)
I0506 00:35:11.701483 25403 sgd_solver.cpp:106] Iteration 119, lr = 1e-06
I0506 00:35:11.889065 25403 solver.cpp:240] Iteration 120, loss = 4.27413
I0506 00:35:11.889099 25403 solver.cpp:256]     Train net output #0: loss = 4.27413 (* 1 = 4.27413 loss)
I0506 00:35:11.889106 25403 sgd_solver.cpp:106] Iteration 120, lr = 1e-06
I0506 00:35:12.082090 25403 solver.cpp:240] Iteration 121, loss = 4.18474
I0506 00:35:12.082126 25403 solver.cpp:256]     Train net output #0: loss = 4.18474 (* 1 = 4.18474 loss)
I0506 00:35:12.082134 25403 sgd_solver.cpp:106] Iteration 121, lr = 1e-06
I0506 00:35:12.270963 25403 solver.cpp:240] Iteration 122, loss = 4.23898
I0506 00:35:12.270995 25403 solver.cpp:256]     Train net output #0: loss = 4.23898 (* 1 = 4.23898 loss)
I0506 00:35:12.271003 25403 sgd_solver.cpp:106] Iteration 122, lr = 1e-06
I0506 00:35:12.462369 25403 solver.cpp:240] Iteration 123, loss = 4.21513
I0506 00:35:12.462404 25403 solver.cpp:256]     Train net output #0: loss = 4.21513 (* 1 = 4.21513 loss)
I0506 00:35:12.462414 25403 sgd_solver.cpp:106] Iteration 123, lr = 1e-06
I0506 00:35:12.648949 25403 solver.cpp:240] Iteration 124, loss = 4.18933
I0506 00:35:12.648985 25403 solver.cpp:256]     Train net output #0: loss = 4.18933 (* 1 = 4.18933 loss)
I0506 00:35:12.648993 25403 sgd_solver.cpp:106] Iteration 124, lr = 1e-06
I0506 00:35:12.838690 25403 solver.cpp:240] Iteration 125, loss = 4.22299
I0506 00:35:12.838718 25403 solver.cpp:256]     Train net output #0: loss = 4.22299 (* 1 = 4.22299 loss)
I0506 00:35:12.838726 25403 sgd_solver.cpp:106] Iteration 125, lr = 1e-06
I0506 00:35:13.026669 25403 solver.cpp:240] Iteration 126, loss = 4.19488
I0506 00:35:13.026706 25403 solver.cpp:256]     Train net output #0: loss = 4.19488 (* 1 = 4.19488 loss)
I0506 00:35:13.026715 25403 sgd_solver.cpp:106] Iteration 126, lr = 1e-06
I0506 00:35:13.215553 25403 solver.cpp:240] Iteration 127, loss = 4.20701
I0506 00:35:13.215585 25403 solver.cpp:256]     Train net output #0: loss = 4.20701 (* 1 = 4.20701 loss)
I0506 00:35:13.215593 25403 sgd_solver.cpp:106] Iteration 127, lr = 1e-06
I0506 00:35:13.406177 25403 solver.cpp:240] Iteration 128, loss = 4.22413
I0506 00:35:13.406208 25403 solver.cpp:256]     Train net output #0: loss = 4.22413 (* 1 = 4.22413 loss)
I0506 00:35:13.406216 25403 sgd_solver.cpp:106] Iteration 128, lr = 1e-06
I0506 00:35:13.594674 25403 solver.cpp:240] Iteration 129, loss = 4.2252
I0506 00:35:13.594708 25403 solver.cpp:256]     Train net output #0: loss = 4.2252 (* 1 = 4.2252 loss)
I0506 00:35:13.594717 25403 sgd_solver.cpp:106] Iteration 129, lr = 1e-06
I0506 00:35:13.787824 25403 solver.cpp:240] Iteration 130, loss = 4.21866
I0506 00:35:13.787858 25403 solver.cpp:256]     Train net output #0: loss = 4.21866 (* 1 = 4.21866 loss)
I0506 00:35:13.787865 25403 sgd_solver.cpp:106] Iteration 130, lr = 1e-06
I0506 00:35:13.975459 25403 solver.cpp:240] Iteration 131, loss = 4.23038
I0506 00:35:13.975492 25403 solver.cpp:256]     Train net output #0: loss = 4.23038 (* 1 = 4.23038 loss)
I0506 00:35:13.975500 25403 sgd_solver.cpp:106] Iteration 131, lr = 1e-06
I0506 00:35:14.166718 25403 solver.cpp:240] Iteration 132, loss = 4.19066
I0506 00:35:14.166750 25403 solver.cpp:256]     Train net output #0: loss = 4.19066 (* 1 = 4.19066 loss)
I0506 00:35:14.166784 25403 sgd_solver.cpp:106] Iteration 132, lr = 1e-06
I0506 00:35:14.353808 25403 solver.cpp:240] Iteration 133, loss = 4.21475
I0506 00:35:14.353838 25403 solver.cpp:256]     Train net output #0: loss = 4.21475 (* 1 = 4.21475 loss)
I0506 00:35:14.353845 25403 sgd_solver.cpp:106] Iteration 133, lr = 1e-06
I0506 00:35:14.542495 25403 solver.cpp:240] Iteration 134, loss = 4.17743
I0506 00:35:14.542528 25403 solver.cpp:256]     Train net output #0: loss = 4.17743 (* 1 = 4.17743 loss)
I0506 00:35:14.542536 25403 sgd_solver.cpp:106] Iteration 134, lr = 1e-06
I0506 00:35:14.730679 25403 solver.cpp:240] Iteration 135, loss = 4.15869
I0506 00:35:14.730713 25403 solver.cpp:256]     Train net output #0: loss = 4.15869 (* 1 = 4.15869 loss)
I0506 00:35:14.730720 25403 sgd_solver.cpp:106] Iteration 135, lr = 1e-06
I0506 00:35:14.918402 25403 solver.cpp:240] Iteration 136, loss = 4.15954
I0506 00:35:14.918437 25403 solver.cpp:256]     Train net output #0: loss = 4.15954 (* 1 = 4.15954 loss)
I0506 00:35:14.918443 25403 sgd_solver.cpp:106] Iteration 136, lr = 1e-06
I0506 00:35:15.110164 25403 solver.cpp:240] Iteration 137, loss = 4.12641
I0506 00:35:15.110198 25403 solver.cpp:256]     Train net output #0: loss = 4.12641 (* 1 = 4.12641 loss)
I0506 00:35:15.110204 25403 sgd_solver.cpp:106] Iteration 137, lr = 1e-06
I0506 00:35:15.297312 25403 solver.cpp:240] Iteration 138, loss = 4.2012
I0506 00:35:15.297343 25403 solver.cpp:256]     Train net output #0: loss = 4.2012 (* 1 = 4.2012 loss)
I0506 00:35:15.297350 25403 sgd_solver.cpp:106] Iteration 138, lr = 1e-06
I0506 00:35:15.485216 25403 solver.cpp:240] Iteration 139, loss = 4.16627
I0506 00:35:15.485247 25403 solver.cpp:256]     Train net output #0: loss = 4.16627 (* 1 = 4.16627 loss)
I0506 00:35:15.485255 25403 sgd_solver.cpp:106] Iteration 139, lr = 1e-06
I0506 00:35:15.673156 25403 solver.cpp:240] Iteration 140, loss = 4.20864
I0506 00:35:15.673189 25403 solver.cpp:256]     Train net output #0: loss = 4.20864 (* 1 = 4.20864 loss)
I0506 00:35:15.673197 25403 sgd_solver.cpp:106] Iteration 140, lr = 1e-06
I0506 00:35:15.860157 25403 solver.cpp:240] Iteration 141, loss = 4.19108
I0506 00:35:15.860188 25403 solver.cpp:256]     Train net output #0: loss = 4.19108 (* 1 = 4.19108 loss)
I0506 00:35:15.860196 25403 sgd_solver.cpp:106] Iteration 141, lr = 1e-06
I0506 00:35:16.050720 25403 solver.cpp:240] Iteration 142, loss = 4.10188
I0506 00:35:16.050756 25403 solver.cpp:256]     Train net output #0: loss = 4.10188 (* 1 = 4.10188 loss)
I0506 00:35:16.050766 25403 sgd_solver.cpp:106] Iteration 142, lr = 1e-06
I0506 00:35:16.237583 25403 solver.cpp:240] Iteration 143, loss = 4.20348
I0506 00:35:16.237620 25403 solver.cpp:256]     Train net output #0: loss = 4.20348 (* 1 = 4.20348 loss)
I0506 00:35:16.237629 25403 sgd_solver.cpp:106] Iteration 143, lr = 1e-06
I0506 00:35:16.426092 25403 solver.cpp:240] Iteration 144, loss = 4.21196
I0506 00:35:16.426126 25403 solver.cpp:256]     Train net output #0: loss = 4.21196 (* 1 = 4.21196 loss)
I0506 00:35:16.426132 25403 sgd_solver.cpp:106] Iteration 144, lr = 1e-06
I0506 00:35:16.616559 25403 solver.cpp:240] Iteration 145, loss = 4.14054
I0506 00:35:16.616591 25403 solver.cpp:256]     Train net output #0: loss = 4.14054 (* 1 = 4.14054 loss)
I0506 00:35:16.616600 25403 sgd_solver.cpp:106] Iteration 145, lr = 1e-06
I0506 00:35:16.803427 25403 solver.cpp:240] Iteration 146, loss = 4.13848
I0506 00:35:16.803462 25403 solver.cpp:256]     Train net output #0: loss = 4.13848 (* 1 = 4.13848 loss)
I0506 00:35:16.803469 25403 sgd_solver.cpp:106] Iteration 146, lr = 1e-06
I0506 00:35:16.995594 25403 solver.cpp:240] Iteration 147, loss = 4.11287
I0506 00:35:16.995632 25403 solver.cpp:256]     Train net output #0: loss = 4.11287 (* 1 = 4.11287 loss)
I0506 00:35:16.995640 25403 sgd_solver.cpp:106] Iteration 147, lr = 1e-06
I0506 00:35:17.181145 25403 solver.cpp:240] Iteration 148, loss = 4.08398
I0506 00:35:17.181179 25403 solver.cpp:256]     Train net output #0: loss = 4.08398 (* 1 = 4.08398 loss)
I0506 00:35:17.181188 25403 sgd_solver.cpp:106] Iteration 148, lr = 1e-06
