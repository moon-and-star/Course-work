I0506 00:50:35.659943  5276 caffe.cpp:217] Using GPUs 0
I0506 00:50:35.948439  5276 caffe.cpp:222] GPU 0: GeForce GTX 1070
I0506 00:50:38.708530  5276 solver.cpp:60] Initializing solver from parameters: 
train_net: "./Prototxt/experiment_13/RTSD/orig/trial_1/train.prototxt"
test_net: "./Prototxt/experiment_13/RTSD/orig/trial_1/test.prototxt"
test_iter: 34
test_interval: 169
base_lr: 1e-06
display: 1
max_iter: 16900
lr_policy: "step"
gamma: 0.5
momentum: 0.9
weight_decay: 0.0005
stepsize: 1690
snapshot: 1690
snapshot_prefix: "./snapshots/experiment_13/RTSD/orig/trial_1/snap"
solver_mode: GPU
device_id: 0
train_state {
  level: 0
  stage: ""
}
iter_size: 1
type: "Adam"
I0506 00:50:38.708673  5276 solver.cpp:93] Creating training net from train_net file: ./Prototxt/experiment_13/RTSD/orig/trial_1/train.prototxt
I0506 00:50:38.709143  5276 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.0039215689
    mirror: false
    crop_size: 48
    mean_value: 119
    mean_value: 113
    mean_value: 113
  }
  data_param {
    source: "../local_data/lmdb/RTSD/orig/train/lmdb"
    batch_size: 512
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 7
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_prescale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv1_sTanH"
  type: "TanH"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_postscale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 150
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_prescale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv2_sTanH"
  type: "TanH"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_postscale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 250
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_prescale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv3_sTanH"
  type: "TanH"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_postscale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc4_300"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc4_300"
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fc4_prescale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "fc4_sTanH"
  type: "TanH"
  bottom: "fc4_300"
  top: "fc4_300"
}
layer {
  name: "fc4_postscale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "fc5_116"
  type: "InnerProduct"
  bottom: "fc4_300"
  top: "fc5_classes"
  inner_product_param {
    num_output: 116
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "softmax"
  type: "Softmax"
  bottom: "fc5_classes"
  top: "softmax"
}
layer {
  name: "loss"
  type: "MultinomialLogisticLoss"
  bottom: "softmax"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy_1"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_1"
  accuracy_param {
    top_k: 1
  }
}
layer {
  name: "accuracy_5"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_5"
  accuracy_param {
    top_k: 5
  }
}
layer {
  name: "silence"
  type: "Silence"
  bottom: "accuracy_1"
  bottom: "accuracy_5"
}
I0506 00:50:38.709300  5276 layer_factory.hpp:77] Creating layer data
I0506 00:50:38.710058  5276 net.cpp:100] Creating Layer data
I0506 00:50:38.710074  5276 net.cpp:408] data -> data
I0506 00:50:38.710100  5276 net.cpp:408] data -> label
I0506 00:50:38.722007  5533 db_lmdb.cpp:35] Opened lmdb ../local_data/lmdb/RTSD/orig/train/lmdb
I0506 00:50:38.739795  5276 data_layer.cpp:41] output data size: 512,3,48,48
I0506 00:50:38.827695  5276 net.cpp:150] Setting up data
I0506 00:50:38.827725  5276 net.cpp:157] Top shape: 512 3 48 48 (3538944)
I0506 00:50:38.827730  5276 net.cpp:157] Top shape: 512 (512)
I0506 00:50:38.827733  5276 net.cpp:165] Memory required for data: 14157824
I0506 00:50:38.827744  5276 layer_factory.hpp:77] Creating layer label_data_1_split
I0506 00:50:38.827759  5276 net.cpp:100] Creating Layer label_data_1_split
I0506 00:50:38.827766  5276 net.cpp:434] label_data_1_split <- label
I0506 00:50:38.827785  5276 net.cpp:408] label_data_1_split -> label_data_1_split_0
I0506 00:50:38.827796  5276 net.cpp:408] label_data_1_split -> label_data_1_split_1
I0506 00:50:38.827805  5276 net.cpp:408] label_data_1_split -> label_data_1_split_2
I0506 00:50:38.827872  5276 net.cpp:150] Setting up label_data_1_split
I0506 00:50:38.827891  5276 net.cpp:157] Top shape: 512 (512)
I0506 00:50:38.827898  5276 net.cpp:157] Top shape: 512 (512)
I0506 00:50:38.827901  5276 net.cpp:157] Top shape: 512 (512)
I0506 00:50:38.827903  5276 net.cpp:165] Memory required for data: 14163968
I0506 00:50:38.827906  5276 layer_factory.hpp:77] Creating layer conv1
I0506 00:50:38.827924  5276 net.cpp:100] Creating Layer conv1
I0506 00:50:38.827929  5276 net.cpp:434] conv1 <- data
I0506 00:50:38.827935  5276 net.cpp:408] conv1 -> conv1
I0506 00:50:40.126811  5276 net.cpp:150] Setting up conv1
I0506 00:50:40.126840  5276 net.cpp:157] Top shape: 512 100 42 42 (90316800)
I0506 00:50:40.126844  5276 net.cpp:165] Memory required for data: 375431168
I0506 00:50:40.126868  5276 layer_factory.hpp:77] Creating layer conv1_prescale
I0506 00:50:40.126883  5276 net.cpp:100] Creating Layer conv1_prescale
I0506 00:50:40.126888  5276 net.cpp:434] conv1_prescale <- conv1
I0506 00:50:40.126894  5276 net.cpp:395] conv1_prescale -> conv1 (in-place)
I0506 00:50:40.127005  5276 net.cpp:150] Setting up conv1_prescale
I0506 00:50:40.127014  5276 net.cpp:157] Top shape: 512 100 42 42 (90316800)
I0506 00:50:40.127017  5276 net.cpp:165] Memory required for data: 736698368
I0506 00:50:40.127023  5276 layer_factory.hpp:77] Creating layer conv1_sTanH
I0506 00:50:40.127032  5276 net.cpp:100] Creating Layer conv1_sTanH
I0506 00:50:40.127038  5276 net.cpp:434] conv1_sTanH <- conv1
I0506 00:50:40.127041  5276 net.cpp:395] conv1_sTanH -> conv1 (in-place)
I0506 00:50:40.127238  5276 net.cpp:150] Setting up conv1_sTanH
I0506 00:50:40.127250  5276 net.cpp:157] Top shape: 512 100 42 42 (90316800)
I0506 00:50:40.127276  5276 net.cpp:165] Memory required for data: 1097965568
I0506 00:50:40.127280  5276 layer_factory.hpp:77] Creating layer conv1_postscale
I0506 00:50:40.127287  5276 net.cpp:100] Creating Layer conv1_postscale
I0506 00:50:40.127290  5276 net.cpp:434] conv1_postscale <- conv1
I0506 00:50:40.127295  5276 net.cpp:395] conv1_postscale -> conv1 (in-place)
I0506 00:50:40.127391  5276 net.cpp:150] Setting up conv1_postscale
I0506 00:50:40.127399  5276 net.cpp:157] Top shape: 512 100 42 42 (90316800)
I0506 00:50:40.127401  5276 net.cpp:165] Memory required for data: 1459232768
I0506 00:50:40.127406  5276 layer_factory.hpp:77] Creating layer pool1
I0506 00:50:40.127413  5276 net.cpp:100] Creating Layer pool1
I0506 00:50:40.127418  5276 net.cpp:434] pool1 <- conv1
I0506 00:50:40.127423  5276 net.cpp:408] pool1 -> pool1
I0506 00:50:40.127470  5276 net.cpp:150] Setting up pool1
I0506 00:50:40.127478  5276 net.cpp:157] Top shape: 512 100 21 21 (22579200)
I0506 00:50:40.127480  5276 net.cpp:165] Memory required for data: 1549549568
I0506 00:50:40.127485  5276 layer_factory.hpp:77] Creating layer conv2
I0506 00:50:40.127495  5276 net.cpp:100] Creating Layer conv2
I0506 00:50:40.127498  5276 net.cpp:434] conv2 <- pool1
I0506 00:50:40.127503  5276 net.cpp:408] conv2 -> conv2
I0506 00:50:40.133769  5276 net.cpp:150] Setting up conv2
I0506 00:50:40.133790  5276 net.cpp:157] Top shape: 512 150 18 18 (24883200)
I0506 00:50:40.133795  5276 net.cpp:165] Memory required for data: 1649082368
I0506 00:50:40.133806  5276 layer_factory.hpp:77] Creating layer conv2_prescale
I0506 00:50:40.133819  5276 net.cpp:100] Creating Layer conv2_prescale
I0506 00:50:40.133826  5276 net.cpp:434] conv2_prescale <- conv2
I0506 00:50:40.133831  5276 net.cpp:395] conv2_prescale -> conv2 (in-place)
I0506 00:50:40.133954  5276 net.cpp:150] Setting up conv2_prescale
I0506 00:50:40.133962  5276 net.cpp:157] Top shape: 512 150 18 18 (24883200)
I0506 00:50:40.133966  5276 net.cpp:165] Memory required for data: 1748615168
I0506 00:50:40.133971  5276 layer_factory.hpp:77] Creating layer conv2_sTanH
I0506 00:50:40.133976  5276 net.cpp:100] Creating Layer conv2_sTanH
I0506 00:50:40.133981  5276 net.cpp:434] conv2_sTanH <- conv2
I0506 00:50:40.133987  5276 net.cpp:395] conv2_sTanH -> conv2 (in-place)
I0506 00:50:40.169469  5276 net.cpp:150] Setting up conv2_sTanH
I0506 00:50:40.169502  5276 net.cpp:157] Top shape: 512 150 18 18 (24883200)
I0506 00:50:40.169507  5276 net.cpp:165] Memory required for data: 1848147968
I0506 00:50:40.169512  5276 layer_factory.hpp:77] Creating layer conv2_postscale
I0506 00:50:40.169529  5276 net.cpp:100] Creating Layer conv2_postscale
I0506 00:50:40.169536  5276 net.cpp:434] conv2_postscale <- conv2
I0506 00:50:40.169543  5276 net.cpp:395] conv2_postscale -> conv2 (in-place)
I0506 00:50:40.169683  5276 net.cpp:150] Setting up conv2_postscale
I0506 00:50:40.169693  5276 net.cpp:157] Top shape: 512 150 18 18 (24883200)
I0506 00:50:40.169697  5276 net.cpp:165] Memory required for data: 1947680768
I0506 00:50:40.169703  5276 layer_factory.hpp:77] Creating layer pool2
I0506 00:50:40.169711  5276 net.cpp:100] Creating Layer pool2
I0506 00:50:40.169716  5276 net.cpp:434] pool2 <- conv2
I0506 00:50:40.169723  5276 net.cpp:408] pool2 -> pool2
I0506 00:50:40.169776  5276 net.cpp:150] Setting up pool2
I0506 00:50:40.169785  5276 net.cpp:157] Top shape: 512 150 9 9 (6220800)
I0506 00:50:40.169788  5276 net.cpp:165] Memory required for data: 1972563968
I0506 00:50:40.169796  5276 layer_factory.hpp:77] Creating layer conv3
I0506 00:50:40.169808  5276 net.cpp:100] Creating Layer conv3
I0506 00:50:40.169813  5276 net.cpp:434] conv3 <- pool2
I0506 00:50:40.169821  5276 net.cpp:408] conv3 -> conv3
I0506 00:50:40.179836  5276 net.cpp:150] Setting up conv3
I0506 00:50:40.179857  5276 net.cpp:157] Top shape: 512 250 6 6 (4608000)
I0506 00:50:40.179862  5276 net.cpp:165] Memory required for data: 1990995968
I0506 00:50:40.179873  5276 layer_factory.hpp:77] Creating layer conv3_prescale
I0506 00:50:40.179893  5276 net.cpp:100] Creating Layer conv3_prescale
I0506 00:50:40.179920  5276 net.cpp:434] conv3_prescale <- conv3
I0506 00:50:40.179927  5276 net.cpp:395] conv3_prescale -> conv3 (in-place)
I0506 00:50:40.180030  5276 net.cpp:150] Setting up conv3_prescale
I0506 00:50:40.180039  5276 net.cpp:157] Top shape: 512 250 6 6 (4608000)
I0506 00:50:40.180042  5276 net.cpp:165] Memory required for data: 2009427968
I0506 00:50:40.180047  5276 layer_factory.hpp:77] Creating layer conv3_sTanH
I0506 00:50:40.180053  5276 net.cpp:100] Creating Layer conv3_sTanH
I0506 00:50:40.180058  5276 net.cpp:434] conv3_sTanH <- conv3
I0506 00:50:40.180064  5276 net.cpp:395] conv3_sTanH -> conv3 (in-place)
I0506 00:50:40.181144  5276 net.cpp:150] Setting up conv3_sTanH
I0506 00:50:40.181160  5276 net.cpp:157] Top shape: 512 250 6 6 (4608000)
I0506 00:50:40.181164  5276 net.cpp:165] Memory required for data: 2027859968
I0506 00:50:40.181166  5276 layer_factory.hpp:77] Creating layer conv3_postscale
I0506 00:50:40.181176  5276 net.cpp:100] Creating Layer conv3_postscale
I0506 00:50:40.181181  5276 net.cpp:434] conv3_postscale <- conv3
I0506 00:50:40.181186  5276 net.cpp:395] conv3_postscale -> conv3 (in-place)
I0506 00:50:40.181288  5276 net.cpp:150] Setting up conv3_postscale
I0506 00:50:40.181296  5276 net.cpp:157] Top shape: 512 250 6 6 (4608000)
I0506 00:50:40.181299  5276 net.cpp:165] Memory required for data: 2046291968
I0506 00:50:40.181304  5276 layer_factory.hpp:77] Creating layer pool3
I0506 00:50:40.181315  5276 net.cpp:100] Creating Layer pool3
I0506 00:50:40.181320  5276 net.cpp:434] pool3 <- conv3
I0506 00:50:40.181325  5276 net.cpp:408] pool3 -> pool3
I0506 00:50:40.181367  5276 net.cpp:150] Setting up pool3
I0506 00:50:40.181375  5276 net.cpp:157] Top shape: 512 250 3 3 (1152000)
I0506 00:50:40.181378  5276 net.cpp:165] Memory required for data: 2050899968
I0506 00:50:40.181381  5276 layer_factory.hpp:77] Creating layer fc4_300
I0506 00:50:40.181387  5276 net.cpp:100] Creating Layer fc4_300
I0506 00:50:40.181392  5276 net.cpp:434] fc4_300 <- pool3
I0506 00:50:40.181398  5276 net.cpp:408] fc4_300 -> fc4_300
I0506 00:50:40.188043  5276 net.cpp:150] Setting up fc4_300
I0506 00:50:40.188060  5276 net.cpp:157] Top shape: 512 300 (153600)
I0506 00:50:40.188062  5276 net.cpp:165] Memory required for data: 2051514368
I0506 00:50:40.188069  5276 layer_factory.hpp:77] Creating layer fc4_prescale
I0506 00:50:40.188079  5276 net.cpp:100] Creating Layer fc4_prescale
I0506 00:50:40.188086  5276 net.cpp:434] fc4_prescale <- fc4_300
I0506 00:50:40.188091  5276 net.cpp:395] fc4_prescale -> fc4_300 (in-place)
I0506 00:50:40.188184  5276 net.cpp:150] Setting up fc4_prescale
I0506 00:50:40.188191  5276 net.cpp:157] Top shape: 512 300 (153600)
I0506 00:50:40.188194  5276 net.cpp:165] Memory required for data: 2052128768
I0506 00:50:40.188199  5276 layer_factory.hpp:77] Creating layer fc4_sTanH
I0506 00:50:40.188207  5276 net.cpp:100] Creating Layer fc4_sTanH
I0506 00:50:40.188211  5276 net.cpp:434] fc4_sTanH <- fc4_300
I0506 00:50:40.188215  5276 net.cpp:395] fc4_sTanH -> fc4_300 (in-place)
I0506 00:50:40.188410  5276 net.cpp:150] Setting up fc4_sTanH
I0506 00:50:40.188421  5276 net.cpp:157] Top shape: 512 300 (153600)
I0506 00:50:40.188424  5276 net.cpp:165] Memory required for data: 2052743168
I0506 00:50:40.188427  5276 layer_factory.hpp:77] Creating layer fc4_postscale
I0506 00:50:40.188437  5276 net.cpp:100] Creating Layer fc4_postscale
I0506 00:50:40.188442  5276 net.cpp:434] fc4_postscale <- fc4_300
I0506 00:50:40.188448  5276 net.cpp:395] fc4_postscale -> fc4_300 (in-place)
I0506 00:50:40.188549  5276 net.cpp:150] Setting up fc4_postscale
I0506 00:50:40.188556  5276 net.cpp:157] Top shape: 512 300 (153600)
I0506 00:50:40.188560  5276 net.cpp:165] Memory required for data: 2053357568
I0506 00:50:40.188565  5276 layer_factory.hpp:77] Creating layer fc5_116
I0506 00:50:40.188571  5276 net.cpp:100] Creating Layer fc5_116
I0506 00:50:40.188575  5276 net.cpp:434] fc5_116 <- fc4_300
I0506 00:50:40.188581  5276 net.cpp:408] fc5_116 -> fc5_classes
I0506 00:50:40.191084  5276 net.cpp:150] Setting up fc5_116
I0506 00:50:40.191112  5276 net.cpp:157] Top shape: 512 116 (59392)
I0506 00:50:40.191118  5276 net.cpp:165] Memory required for data: 2053595136
I0506 00:50:40.191129  5276 layer_factory.hpp:77] Creating layer fc5_classes_fc5_116_0_split
I0506 00:50:40.191138  5276 net.cpp:100] Creating Layer fc5_classes_fc5_116_0_split
I0506 00:50:40.191143  5276 net.cpp:434] fc5_classes_fc5_116_0_split <- fc5_classes
I0506 00:50:40.191149  5276 net.cpp:408] fc5_classes_fc5_116_0_split -> fc5_classes_fc5_116_0_split_0
I0506 00:50:40.191159  5276 net.cpp:408] fc5_classes_fc5_116_0_split -> fc5_classes_fc5_116_0_split_1
I0506 00:50:40.191166  5276 net.cpp:408] fc5_classes_fc5_116_0_split -> fc5_classes_fc5_116_0_split_2
I0506 00:50:40.191226  5276 net.cpp:150] Setting up fc5_classes_fc5_116_0_split
I0506 00:50:40.191234  5276 net.cpp:157] Top shape: 512 116 (59392)
I0506 00:50:40.191237  5276 net.cpp:157] Top shape: 512 116 (59392)
I0506 00:50:40.191241  5276 net.cpp:157] Top shape: 512 116 (59392)
I0506 00:50:40.191243  5276 net.cpp:165] Memory required for data: 2054307840
I0506 00:50:40.191246  5276 layer_factory.hpp:77] Creating layer softmax
I0506 00:50:40.191251  5276 net.cpp:100] Creating Layer softmax
I0506 00:50:40.191256  5276 net.cpp:434] softmax <- fc5_classes_fc5_116_0_split_0
I0506 00:50:40.191260  5276 net.cpp:408] softmax -> softmax
I0506 00:50:40.191515  5276 net.cpp:150] Setting up softmax
I0506 00:50:40.191527  5276 net.cpp:157] Top shape: 512 116 (59392)
I0506 00:50:40.191531  5276 net.cpp:165] Memory required for data: 2054545408
I0506 00:50:40.191534  5276 layer_factory.hpp:77] Creating layer loss
I0506 00:50:40.191541  5276 net.cpp:100] Creating Layer loss
I0506 00:50:40.191546  5276 net.cpp:434] loss <- softmax
I0506 00:50:40.191551  5276 net.cpp:434] loss <- label_data_1_split_0
I0506 00:50:40.191555  5276 net.cpp:408] loss -> loss
I0506 00:50:40.191586  5276 net.cpp:150] Setting up loss
I0506 00:50:40.191591  5276 net.cpp:157] Top shape: (1)
I0506 00:50:40.191594  5276 net.cpp:160]     with loss weight 1
I0506 00:50:40.191614  5276 net.cpp:165] Memory required for data: 2054545412
I0506 00:50:40.191618  5276 layer_factory.hpp:77] Creating layer accuracy_1
I0506 00:50:40.191625  5276 net.cpp:100] Creating Layer accuracy_1
I0506 00:50:40.191630  5276 net.cpp:434] accuracy_1 <- fc5_classes_fc5_116_0_split_1
I0506 00:50:40.191634  5276 net.cpp:434] accuracy_1 <- label_data_1_split_1
I0506 00:50:40.191643  5276 net.cpp:408] accuracy_1 -> accuracy_1
I0506 00:50:40.191653  5276 net.cpp:150] Setting up accuracy_1
I0506 00:50:40.191659  5276 net.cpp:157] Top shape: (1)
I0506 00:50:40.191661  5276 net.cpp:165] Memory required for data: 2054545416
I0506 00:50:40.191664  5276 layer_factory.hpp:77] Creating layer accuracy_5
I0506 00:50:40.191669  5276 net.cpp:100] Creating Layer accuracy_5
I0506 00:50:40.191673  5276 net.cpp:434] accuracy_5 <- fc5_classes_fc5_116_0_split_2
I0506 00:50:40.191676  5276 net.cpp:434] accuracy_5 <- label_data_1_split_2
I0506 00:50:40.191682  5276 net.cpp:408] accuracy_5 -> accuracy_5
I0506 00:50:40.191689  5276 net.cpp:150] Setting up accuracy_5
I0506 00:50:40.191694  5276 net.cpp:157] Top shape: (1)
I0506 00:50:40.191696  5276 net.cpp:165] Memory required for data: 2054545420
I0506 00:50:40.191699  5276 layer_factory.hpp:77] Creating layer silence
I0506 00:50:40.191704  5276 net.cpp:100] Creating Layer silence
I0506 00:50:40.191707  5276 net.cpp:434] silence <- accuracy_1
I0506 00:50:40.191710  5276 net.cpp:434] silence <- accuracy_5
I0506 00:50:40.191715  5276 net.cpp:150] Setting up silence
I0506 00:50:40.191717  5276 net.cpp:165] Memory required for data: 2054545420
I0506 00:50:40.191720  5276 net.cpp:228] silence does not need backward computation.
I0506 00:50:40.191730  5276 net.cpp:228] accuracy_5 does not need backward computation.
I0506 00:50:40.191732  5276 net.cpp:228] accuracy_1 does not need backward computation.
I0506 00:50:40.191736  5276 net.cpp:226] loss needs backward computation.
I0506 00:50:40.191741  5276 net.cpp:226] softmax needs backward computation.
I0506 00:50:40.191756  5276 net.cpp:226] fc5_classes_fc5_116_0_split needs backward computation.
I0506 00:50:40.191762  5276 net.cpp:226] fc5_116 needs backward computation.
I0506 00:50:40.191766  5276 net.cpp:226] fc4_postscale needs backward computation.
I0506 00:50:40.191768  5276 net.cpp:226] fc4_sTanH needs backward computation.
I0506 00:50:40.191771  5276 net.cpp:226] fc4_prescale needs backward computation.
I0506 00:50:40.191773  5276 net.cpp:226] fc4_300 needs backward computation.
I0506 00:50:40.191777  5276 net.cpp:226] pool3 needs backward computation.
I0506 00:50:40.191781  5276 net.cpp:226] conv3_postscale needs backward computation.
I0506 00:50:40.191783  5276 net.cpp:226] conv3_sTanH needs backward computation.
I0506 00:50:40.191787  5276 net.cpp:226] conv3_prescale needs backward computation.
I0506 00:50:40.191789  5276 net.cpp:226] conv3 needs backward computation.
I0506 00:50:40.191792  5276 net.cpp:226] pool2 needs backward computation.
I0506 00:50:40.191795  5276 net.cpp:226] conv2_postscale needs backward computation.
I0506 00:50:40.191798  5276 net.cpp:226] conv2_sTanH needs backward computation.
I0506 00:50:40.191802  5276 net.cpp:226] conv2_prescale needs backward computation.
I0506 00:50:40.191804  5276 net.cpp:226] conv2 needs backward computation.
I0506 00:50:40.191807  5276 net.cpp:226] pool1 needs backward computation.
I0506 00:50:40.191812  5276 net.cpp:226] conv1_postscale needs backward computation.
I0506 00:50:40.191813  5276 net.cpp:226] conv1_sTanH needs backward computation.
I0506 00:50:40.191817  5276 net.cpp:226] conv1_prescale needs backward computation.
I0506 00:50:40.191819  5276 net.cpp:226] conv1 needs backward computation.
I0506 00:50:40.191823  5276 net.cpp:228] label_data_1_split does not need backward computation.
I0506 00:50:40.191828  5276 net.cpp:228] data does not need backward computation.
I0506 00:50:40.191830  5276 net.cpp:270] This network produces output loss
I0506 00:50:40.191854  5276 net.cpp:283] Network initialization done.
I0506 00:50:40.192157  5276 solver.cpp:193] Creating test net (#0) specified by test_net file: ./Prototxt/experiment_13/RTSD/orig/trial_1/test.prototxt
I0506 00:50:40.192339  5276 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.0039215689
    mirror: false
    crop_size: 48
    mean_value: 121
    mean_value: 117
    mean_value: 120
  }
  data_param {
    source: "../local_data/lmdb/RTSD/orig/test/lmdb"
    batch_size: 512
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 7
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_prescale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv1_sTanH"
  type: "TanH"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_postscale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 150
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_prescale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv2_sTanH"
  type: "TanH"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_postscale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 250
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_prescale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv3_sTanH"
  type: "TanH"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_postscale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc4_300"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc4_300"
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fc4_prescale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "fc4_sTanH"
  type: "TanH"
  bottom: "fc4_300"
  top: "fc4_300"
}
layer {
  name: "fc4_postscale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "fc5_116"
  type: "InnerProduct"
  bottom: "fc4_300"
  top: "fc5_classes"
  inner_product_param {
    num_output: 116
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "softmax"
  type: "Softmax"
  bottom: "fc5_classes"
  top: "softmax"
}
layer {
  name: "loss"
  type: "MultinomialLogisticLoss"
  bottom: "softmax"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy_1"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_1"
  accuracy_param {
    top_k: 1
  }
}
layer {
  name: "accuracy_5"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_5"
  accuracy_param {
    top_k: 5
  }
}
I0506 00:50:40.192451  5276 layer_factory.hpp:77] Creating layer data
I0506 00:50:40.192816  5276 net.cpp:100] Creating Layer data
I0506 00:50:40.192829  5276 net.cpp:408] data -> data
I0506 00:50:40.192838  5276 net.cpp:408] data -> label
I0506 00:50:40.198907  5629 db_lmdb.cpp:35] Opened lmdb ../local_data/lmdb/RTSD/orig/test/lmdb
I0506 00:50:40.199072  5276 data_layer.cpp:41] output data size: 512,3,48,48
I0506 00:50:40.250682  5276 net.cpp:150] Setting up data
I0506 00:50:40.250711  5276 net.cpp:157] Top shape: 512 3 48 48 (3538944)
I0506 00:50:40.250716  5276 net.cpp:157] Top shape: 512 (512)
I0506 00:50:40.250720  5276 net.cpp:165] Memory required for data: 14157824
I0506 00:50:40.250725  5276 layer_factory.hpp:77] Creating layer label_data_1_split
I0506 00:50:40.250738  5276 net.cpp:100] Creating Layer label_data_1_split
I0506 00:50:40.250744  5276 net.cpp:434] label_data_1_split <- label
I0506 00:50:40.250752  5276 net.cpp:408] label_data_1_split -> label_data_1_split_0
I0506 00:50:40.250775  5276 net.cpp:408] label_data_1_split -> label_data_1_split_1
I0506 00:50:40.250783  5276 net.cpp:408] label_data_1_split -> label_data_1_split_2
I0506 00:50:40.250898  5276 net.cpp:150] Setting up label_data_1_split
I0506 00:50:40.250908  5276 net.cpp:157] Top shape: 512 (512)
I0506 00:50:40.250911  5276 net.cpp:157] Top shape: 512 (512)
I0506 00:50:40.250915  5276 net.cpp:157] Top shape: 512 (512)
I0506 00:50:40.250917  5276 net.cpp:165] Memory required for data: 14163968
I0506 00:50:40.250921  5276 layer_factory.hpp:77] Creating layer conv1
I0506 00:50:40.250954  5276 net.cpp:100] Creating Layer conv1
I0506 00:50:40.250959  5276 net.cpp:434] conv1 <- data
I0506 00:50:40.250965  5276 net.cpp:408] conv1 -> conv1
I0506 00:50:40.253820  5276 net.cpp:150] Setting up conv1
I0506 00:50:40.253839  5276 net.cpp:157] Top shape: 512 100 42 42 (90316800)
I0506 00:50:40.253844  5276 net.cpp:165] Memory required for data: 375431168
I0506 00:50:40.253855  5276 layer_factory.hpp:77] Creating layer conv1_prescale
I0506 00:50:40.253867  5276 net.cpp:100] Creating Layer conv1_prescale
I0506 00:50:40.253872  5276 net.cpp:434] conv1_prescale <- conv1
I0506 00:50:40.253877  5276 net.cpp:395] conv1_prescale -> conv1 (in-place)
I0506 00:50:40.253988  5276 net.cpp:150] Setting up conv1_prescale
I0506 00:50:40.254000  5276 net.cpp:157] Top shape: 512 100 42 42 (90316800)
I0506 00:50:40.254004  5276 net.cpp:165] Memory required for data: 736698368
I0506 00:50:40.254010  5276 layer_factory.hpp:77] Creating layer conv1_sTanH
I0506 00:50:40.254019  5276 net.cpp:100] Creating Layer conv1_sTanH
I0506 00:50:40.254024  5276 net.cpp:434] conv1_sTanH <- conv1
I0506 00:50:40.254029  5276 net.cpp:395] conv1_sTanH -> conv1 (in-place)
I0506 00:50:40.254223  5276 net.cpp:150] Setting up conv1_sTanH
I0506 00:50:40.254233  5276 net.cpp:157] Top shape: 512 100 42 42 (90316800)
I0506 00:50:40.254237  5276 net.cpp:165] Memory required for data: 1097965568
I0506 00:50:40.254240  5276 layer_factory.hpp:77] Creating layer conv1_postscale
I0506 00:50:40.254248  5276 net.cpp:100] Creating Layer conv1_postscale
I0506 00:50:40.254252  5276 net.cpp:434] conv1_postscale <- conv1
I0506 00:50:40.254257  5276 net.cpp:395] conv1_postscale -> conv1 (in-place)
I0506 00:50:40.254364  5276 net.cpp:150] Setting up conv1_postscale
I0506 00:50:40.254372  5276 net.cpp:157] Top shape: 512 100 42 42 (90316800)
I0506 00:50:40.254376  5276 net.cpp:165] Memory required for data: 1459232768
I0506 00:50:40.254381  5276 layer_factory.hpp:77] Creating layer pool1
I0506 00:50:40.254390  5276 net.cpp:100] Creating Layer pool1
I0506 00:50:40.254395  5276 net.cpp:434] pool1 <- conv1
I0506 00:50:40.254400  5276 net.cpp:408] pool1 -> pool1
I0506 00:50:40.254442  5276 net.cpp:150] Setting up pool1
I0506 00:50:40.254449  5276 net.cpp:157] Top shape: 512 100 21 21 (22579200)
I0506 00:50:40.254454  5276 net.cpp:165] Memory required for data: 1549549568
I0506 00:50:40.254457  5276 layer_factory.hpp:77] Creating layer conv2
I0506 00:50:40.254467  5276 net.cpp:100] Creating Layer conv2
I0506 00:50:40.254472  5276 net.cpp:434] conv2 <- pool1
I0506 00:50:40.254477  5276 net.cpp:408] conv2 -> conv2
I0506 00:50:40.277760  5276 net.cpp:150] Setting up conv2
I0506 00:50:40.277784  5276 net.cpp:157] Top shape: 512 150 18 18 (24883200)
I0506 00:50:40.277789  5276 net.cpp:165] Memory required for data: 1649082368
I0506 00:50:40.277808  5276 layer_factory.hpp:77] Creating layer conv2_prescale
I0506 00:50:40.277828  5276 net.cpp:100] Creating Layer conv2_prescale
I0506 00:50:40.277834  5276 net.cpp:434] conv2_prescale <- conv2
I0506 00:50:40.277843  5276 net.cpp:395] conv2_prescale -> conv2 (in-place)
I0506 00:50:40.277967  5276 net.cpp:150] Setting up conv2_prescale
I0506 00:50:40.277978  5276 net.cpp:157] Top shape: 512 150 18 18 (24883200)
I0506 00:50:40.277982  5276 net.cpp:165] Memory required for data: 1748615168
I0506 00:50:40.277987  5276 layer_factory.hpp:77] Creating layer conv2_sTanH
I0506 00:50:40.277993  5276 net.cpp:100] Creating Layer conv2_sTanH
I0506 00:50:40.277997  5276 net.cpp:434] conv2_sTanH <- conv2
I0506 00:50:40.278002  5276 net.cpp:395] conv2_sTanH -> conv2 (in-place)
I0506 00:50:40.281786  5276 net.cpp:150] Setting up conv2_sTanH
I0506 00:50:40.281805  5276 net.cpp:157] Top shape: 512 150 18 18 (24883200)
I0506 00:50:40.281808  5276 net.cpp:165] Memory required for data: 1848147968
I0506 00:50:40.281812  5276 layer_factory.hpp:77] Creating layer conv2_postscale
I0506 00:50:40.281821  5276 net.cpp:100] Creating Layer conv2_postscale
I0506 00:50:40.281826  5276 net.cpp:434] conv2_postscale <- conv2
I0506 00:50:40.281852  5276 net.cpp:395] conv2_postscale -> conv2 (in-place)
I0506 00:50:40.281967  5276 net.cpp:150] Setting up conv2_postscale
I0506 00:50:40.281977  5276 net.cpp:157] Top shape: 512 150 18 18 (24883200)
I0506 00:50:40.281980  5276 net.cpp:165] Memory required for data: 1947680768
I0506 00:50:40.281985  5276 layer_factory.hpp:77] Creating layer pool2
I0506 00:50:40.281992  5276 net.cpp:100] Creating Layer pool2
I0506 00:50:40.281996  5276 net.cpp:434] pool2 <- conv2
I0506 00:50:40.282001  5276 net.cpp:408] pool2 -> pool2
I0506 00:50:40.282054  5276 net.cpp:150] Setting up pool2
I0506 00:50:40.282064  5276 net.cpp:157] Top shape: 512 150 9 9 (6220800)
I0506 00:50:40.282071  5276 net.cpp:165] Memory required for data: 1972563968
I0506 00:50:40.282076  5276 layer_factory.hpp:77] Creating layer conv3
I0506 00:50:40.282094  5276 net.cpp:100] Creating Layer conv3
I0506 00:50:40.282099  5276 net.cpp:434] conv3 <- pool2
I0506 00:50:40.282106  5276 net.cpp:408] conv3 -> conv3
I0506 00:50:40.288815  5276 net.cpp:150] Setting up conv3
I0506 00:50:40.288833  5276 net.cpp:157] Top shape: 512 250 6 6 (4608000)
I0506 00:50:40.288837  5276 net.cpp:165] Memory required for data: 1990995968
I0506 00:50:40.288851  5276 layer_factory.hpp:77] Creating layer conv3_prescale
I0506 00:50:40.288863  5276 net.cpp:100] Creating Layer conv3_prescale
I0506 00:50:40.288873  5276 net.cpp:434] conv3_prescale <- conv3
I0506 00:50:40.288882  5276 net.cpp:395] conv3_prescale -> conv3 (in-place)
I0506 00:50:40.288995  5276 net.cpp:150] Setting up conv3_prescale
I0506 00:50:40.289005  5276 net.cpp:157] Top shape: 512 250 6 6 (4608000)
I0506 00:50:40.289008  5276 net.cpp:165] Memory required for data: 2009427968
I0506 00:50:40.289012  5276 layer_factory.hpp:77] Creating layer conv3_sTanH
I0506 00:50:40.289021  5276 net.cpp:100] Creating Layer conv3_sTanH
I0506 00:50:40.289023  5276 net.cpp:434] conv3_sTanH <- conv3
I0506 00:50:40.289034  5276 net.cpp:395] conv3_sTanH -> conv3 (in-place)
I0506 00:50:40.293298  5276 net.cpp:150] Setting up conv3_sTanH
I0506 00:50:40.293315  5276 net.cpp:157] Top shape: 512 250 6 6 (4608000)
I0506 00:50:40.293319  5276 net.cpp:165] Memory required for data: 2027859968
I0506 00:50:40.293323  5276 layer_factory.hpp:77] Creating layer conv3_postscale
I0506 00:50:40.293334  5276 net.cpp:100] Creating Layer conv3_postscale
I0506 00:50:40.293337  5276 net.cpp:434] conv3_postscale <- conv3
I0506 00:50:40.293344  5276 net.cpp:395] conv3_postscale -> conv3 (in-place)
I0506 00:50:40.293455  5276 net.cpp:150] Setting up conv3_postscale
I0506 00:50:40.293463  5276 net.cpp:157] Top shape: 512 250 6 6 (4608000)
I0506 00:50:40.293468  5276 net.cpp:165] Memory required for data: 2046291968
I0506 00:50:40.293473  5276 layer_factory.hpp:77] Creating layer pool3
I0506 00:50:40.293484  5276 net.cpp:100] Creating Layer pool3
I0506 00:50:40.293489  5276 net.cpp:434] pool3 <- conv3
I0506 00:50:40.293494  5276 net.cpp:408] pool3 -> pool3
I0506 00:50:40.293540  5276 net.cpp:150] Setting up pool3
I0506 00:50:40.293548  5276 net.cpp:157] Top shape: 512 250 3 3 (1152000)
I0506 00:50:40.293552  5276 net.cpp:165] Memory required for data: 2050899968
I0506 00:50:40.293555  5276 layer_factory.hpp:77] Creating layer fc4_300
I0506 00:50:40.293561  5276 net.cpp:100] Creating Layer fc4_300
I0506 00:50:40.293565  5276 net.cpp:434] fc4_300 <- pool3
I0506 00:50:40.293570  5276 net.cpp:408] fc4_300 -> fc4_300
I0506 00:50:40.338986  5276 net.cpp:150] Setting up fc4_300
I0506 00:50:40.339015  5276 net.cpp:157] Top shape: 512 300 (153600)
I0506 00:50:40.339017  5276 net.cpp:165] Memory required for data: 2051514368
I0506 00:50:40.339028  5276 layer_factory.hpp:77] Creating layer fc4_prescale
I0506 00:50:40.339042  5276 net.cpp:100] Creating Layer fc4_prescale
I0506 00:50:40.339048  5276 net.cpp:434] fc4_prescale <- fc4_300
I0506 00:50:40.339054  5276 net.cpp:395] fc4_prescale -> fc4_300 (in-place)
I0506 00:50:40.339159  5276 net.cpp:150] Setting up fc4_prescale
I0506 00:50:40.339169  5276 net.cpp:157] Top shape: 512 300 (153600)
I0506 00:50:40.339171  5276 net.cpp:165] Memory required for data: 2052128768
I0506 00:50:40.339200  5276 layer_factory.hpp:77] Creating layer fc4_sTanH
I0506 00:50:40.339208  5276 net.cpp:100] Creating Layer fc4_sTanH
I0506 00:50:40.339221  5276 net.cpp:434] fc4_sTanH <- fc4_300
I0506 00:50:40.339226  5276 net.cpp:395] fc4_sTanH -> fc4_300 (in-place)
I0506 00:50:40.339462  5276 net.cpp:150] Setting up fc4_sTanH
I0506 00:50:40.339480  5276 net.cpp:157] Top shape: 512 300 (153600)
I0506 00:50:40.339483  5276 net.cpp:165] Memory required for data: 2052743168
I0506 00:50:40.339489  5276 layer_factory.hpp:77] Creating layer fc4_postscale
I0506 00:50:40.339498  5276 net.cpp:100] Creating Layer fc4_postscale
I0506 00:50:40.339503  5276 net.cpp:434] fc4_postscale <- fc4_300
I0506 00:50:40.339507  5276 net.cpp:395] fc4_postscale -> fc4_300 (in-place)
I0506 00:50:40.339612  5276 net.cpp:150] Setting up fc4_postscale
I0506 00:50:40.339620  5276 net.cpp:157] Top shape: 512 300 (153600)
I0506 00:50:40.339625  5276 net.cpp:165] Memory required for data: 2053357568
I0506 00:50:40.339630  5276 layer_factory.hpp:77] Creating layer fc5_116
I0506 00:50:40.339637  5276 net.cpp:100] Creating Layer fc5_116
I0506 00:50:40.339643  5276 net.cpp:434] fc5_116 <- fc4_300
I0506 00:50:40.339648  5276 net.cpp:408] fc5_116 -> fc5_classes
I0506 00:50:40.340013  5276 net.cpp:150] Setting up fc5_116
I0506 00:50:40.340023  5276 net.cpp:157] Top shape: 512 116 (59392)
I0506 00:50:40.340025  5276 net.cpp:165] Memory required for data: 2053595136
I0506 00:50:40.340036  5276 layer_factory.hpp:77] Creating layer fc5_classes_fc5_116_0_split
I0506 00:50:40.340046  5276 net.cpp:100] Creating Layer fc5_classes_fc5_116_0_split
I0506 00:50:40.340051  5276 net.cpp:434] fc5_classes_fc5_116_0_split <- fc5_classes
I0506 00:50:40.340056  5276 net.cpp:408] fc5_classes_fc5_116_0_split -> fc5_classes_fc5_116_0_split_0
I0506 00:50:40.340064  5276 net.cpp:408] fc5_classes_fc5_116_0_split -> fc5_classes_fc5_116_0_split_1
I0506 00:50:40.340070  5276 net.cpp:408] fc5_classes_fc5_116_0_split -> fc5_classes_fc5_116_0_split_2
I0506 00:50:40.340129  5276 net.cpp:150] Setting up fc5_classes_fc5_116_0_split
I0506 00:50:40.340140  5276 net.cpp:157] Top shape: 512 116 (59392)
I0506 00:50:40.340144  5276 net.cpp:157] Top shape: 512 116 (59392)
I0506 00:50:40.340147  5276 net.cpp:157] Top shape: 512 116 (59392)
I0506 00:50:40.340149  5276 net.cpp:165] Memory required for data: 2054307840
I0506 00:50:40.340152  5276 layer_factory.hpp:77] Creating layer softmax
I0506 00:50:40.340160  5276 net.cpp:100] Creating Layer softmax
I0506 00:50:40.340165  5276 net.cpp:434] softmax <- fc5_classes_fc5_116_0_split_0
I0506 00:50:40.340170  5276 net.cpp:408] softmax -> softmax
I0506 00:50:40.340445  5276 net.cpp:150] Setting up softmax
I0506 00:50:40.340458  5276 net.cpp:157] Top shape: 512 116 (59392)
I0506 00:50:40.340462  5276 net.cpp:165] Memory required for data: 2054545408
I0506 00:50:40.340466  5276 layer_factory.hpp:77] Creating layer loss
I0506 00:50:40.340473  5276 net.cpp:100] Creating Layer loss
I0506 00:50:40.340478  5276 net.cpp:434] loss <- softmax
I0506 00:50:40.340482  5276 net.cpp:434] loss <- label_data_1_split_0
I0506 00:50:40.340490  5276 net.cpp:408] loss -> loss
I0506 00:50:40.340522  5276 net.cpp:150] Setting up loss
I0506 00:50:40.340529  5276 net.cpp:157] Top shape: (1)
I0506 00:50:40.340531  5276 net.cpp:160]     with loss weight 1
I0506 00:50:40.340543  5276 net.cpp:165] Memory required for data: 2054545412
I0506 00:50:40.340546  5276 layer_factory.hpp:77] Creating layer accuracy_1
I0506 00:50:40.340554  5276 net.cpp:100] Creating Layer accuracy_1
I0506 00:50:40.340562  5276 net.cpp:434] accuracy_1 <- fc5_classes_fc5_116_0_split_1
I0506 00:50:40.340570  5276 net.cpp:434] accuracy_1 <- label_data_1_split_1
I0506 00:50:40.340580  5276 net.cpp:408] accuracy_1 -> accuracy_1
I0506 00:50:40.340595  5276 net.cpp:150] Setting up accuracy_1
I0506 00:50:40.340600  5276 net.cpp:157] Top shape: (1)
I0506 00:50:40.340603  5276 net.cpp:165] Memory required for data: 2054545416
I0506 00:50:40.340606  5276 layer_factory.hpp:77] Creating layer accuracy_5
I0506 00:50:40.340626  5276 net.cpp:100] Creating Layer accuracy_5
I0506 00:50:40.340631  5276 net.cpp:434] accuracy_5 <- fc5_classes_fc5_116_0_split_2
I0506 00:50:40.340636  5276 net.cpp:434] accuracy_5 <- label_data_1_split_2
I0506 00:50:40.340641  5276 net.cpp:408] accuracy_5 -> accuracy_5
I0506 00:50:40.340648  5276 net.cpp:150] Setting up accuracy_5
I0506 00:50:40.340652  5276 net.cpp:157] Top shape: (1)
I0506 00:50:40.340656  5276 net.cpp:165] Memory required for data: 2054545420
I0506 00:50:40.340658  5276 net.cpp:228] accuracy_5 does not need backward computation.
I0506 00:50:40.340662  5276 net.cpp:228] accuracy_1 does not need backward computation.
I0506 00:50:40.340665  5276 net.cpp:226] loss needs backward computation.
I0506 00:50:40.340669  5276 net.cpp:226] softmax needs backward computation.
I0506 00:50:40.340672  5276 net.cpp:226] fc5_classes_fc5_116_0_split needs backward computation.
I0506 00:50:40.340675  5276 net.cpp:226] fc5_116 needs backward computation.
I0506 00:50:40.340678  5276 net.cpp:226] fc4_postscale needs backward computation.
I0506 00:50:40.340682  5276 net.cpp:226] fc4_sTanH needs backward computation.
I0506 00:50:40.340684  5276 net.cpp:226] fc4_prescale needs backward computation.
I0506 00:50:40.340687  5276 net.cpp:226] fc4_300 needs backward computation.
I0506 00:50:40.340689  5276 net.cpp:226] pool3 needs backward computation.
I0506 00:50:40.340701  5276 net.cpp:226] conv3_postscale needs backward computation.
I0506 00:50:40.340704  5276 net.cpp:226] conv3_sTanH needs backward computation.
I0506 00:50:40.340708  5276 net.cpp:226] conv3_prescale needs backward computation.
I0506 00:50:40.340710  5276 net.cpp:226] conv3 needs backward computation.
I0506 00:50:40.340713  5276 net.cpp:226] pool2 needs backward computation.
I0506 00:50:40.340716  5276 net.cpp:226] conv2_postscale needs backward computation.
I0506 00:50:40.340719  5276 net.cpp:226] conv2_sTanH needs backward computation.
I0506 00:50:40.340723  5276 net.cpp:226] conv2_prescale needs backward computation.
I0506 00:50:40.340725  5276 net.cpp:226] conv2 needs backward computation.
I0506 00:50:40.340730  5276 net.cpp:226] pool1 needs backward computation.
I0506 00:50:40.340735  5276 net.cpp:226] conv1_postscale needs backward computation.
I0506 00:50:40.340739  5276 net.cpp:226] conv1_sTanH needs backward computation.
I0506 00:50:40.340740  5276 net.cpp:226] conv1_prescale needs backward computation.
I0506 00:50:40.340744  5276 net.cpp:226] conv1 needs backward computation.
I0506 00:50:40.340754  5276 net.cpp:228] label_data_1_split does not need backward computation.
I0506 00:50:40.340759  5276 net.cpp:228] data does not need backward computation.
I0506 00:50:40.340761  5276 net.cpp:270] This network produces output accuracy_1
I0506 00:50:40.340764  5276 net.cpp:270] This network produces output accuracy_5
I0506 00:50:40.340767  5276 net.cpp:270] This network produces output loss
I0506 00:50:40.340787  5276 net.cpp:283] Network initialization done.
I0506 00:50:40.340879  5276 solver.cpp:72] Solver scaffolding done.
I0506 00:50:40.341822  5276 caffe.cpp:251] Starting Optimization
I0506 00:50:40.341831  5276 solver.cpp:291] Solving 
I0506 00:50:40.341833  5276 solver.cpp:292] Learning Rate Policy: step
I0506 00:50:40.349565  5276 solver.cpp:349] Iteration 0, Testing net (#0)
I0506 00:50:40.366127  5276 net.cpp:693] Ignoring source layer silence
I0506 00:50:42.699558  5276 solver.cpp:416]     Test net output #0: accuracy_1 = 0.0099954
I0506 00:50:42.699585  5276 solver.cpp:416]     Test net output #1: accuracy_5 = 0.0464154
I0506 00:50:42.699596  5276 solver.cpp:416]     Test net output #2: loss = 4.84152 (* 1 = 4.84152 loss)
I0506 00:50:42.829612  5276 solver.cpp:240] Iteration 0, loss = 4.93036
I0506 00:50:42.829650  5276 solver.cpp:256]     Train net output #0: loss = 4.93036 (* 1 = 4.93036 loss)
I0506 00:50:42.829663  5276 sgd_solver.cpp:106] Iteration 0, lr = 1e-06
I0506 00:50:43.013664  5276 solver.cpp:240] Iteration 1, loss = 4.9073
I0506 00:50:43.013700  5276 solver.cpp:256]     Train net output #0: loss = 4.9073 (* 1 = 4.9073 loss)
I0506 00:50:43.013732  5276 sgd_solver.cpp:106] Iteration 1, lr = 1e-06
I0506 00:50:43.203327  5276 solver.cpp:240] Iteration 2, loss = 4.91873
I0506 00:50:43.203368  5276 solver.cpp:256]     Train net output #0: loss = 4.91873 (* 1 = 4.91873 loss)
I0506 00:50:43.203377  5276 sgd_solver.cpp:106] Iteration 2, lr = 1e-06
I0506 00:50:43.389930  5276 solver.cpp:240] Iteration 3, loss = 4.90609
I0506 00:50:43.389964  5276 solver.cpp:256]     Train net output #0: loss = 4.90609 (* 1 = 4.90609 loss)
I0506 00:50:43.389973  5276 sgd_solver.cpp:106] Iteration 3, lr = 1e-06
I0506 00:50:43.578501  5276 solver.cpp:240] Iteration 4, loss = 4.90537
I0506 00:50:43.578538  5276 solver.cpp:256]     Train net output #0: loss = 4.90537 (* 1 = 4.90537 loss)
I0506 00:50:43.578547  5276 sgd_solver.cpp:106] Iteration 4, lr = 1e-06
I0506 00:50:43.767916  5276 solver.cpp:240] Iteration 5, loss = 4.92105
I0506 00:50:43.767951  5276 solver.cpp:256]     Train net output #0: loss = 4.92105 (* 1 = 4.92105 loss)
I0506 00:50:43.767959  5276 sgd_solver.cpp:106] Iteration 5, lr = 1e-06
I0506 00:50:43.955544  5276 solver.cpp:240] Iteration 6, loss = 4.89824
I0506 00:50:43.955582  5276 solver.cpp:256]     Train net output #0: loss = 4.89824 (* 1 = 4.89824 loss)
I0506 00:50:43.955590  5276 sgd_solver.cpp:106] Iteration 6, lr = 1e-06
I0506 00:50:44.146083  5276 solver.cpp:240] Iteration 7, loss = 4.90725
I0506 00:50:44.146122  5276 solver.cpp:256]     Train net output #0: loss = 4.90725 (* 1 = 4.90725 loss)
I0506 00:50:44.146129  5276 sgd_solver.cpp:106] Iteration 7, lr = 1e-06
I0506 00:50:44.333122  5276 solver.cpp:240] Iteration 8, loss = 4.91855
I0506 00:50:44.333170  5276 solver.cpp:256]     Train net output #0: loss = 4.91855 (* 1 = 4.91855 loss)
I0506 00:50:44.333183  5276 sgd_solver.cpp:106] Iteration 8, lr = 1e-06
I0506 00:50:44.520443  5276 solver.cpp:240] Iteration 9, loss = 4.89779
I0506 00:50:44.520481  5276 solver.cpp:256]     Train net output #0: loss = 4.89779 (* 1 = 4.89779 loss)
I0506 00:50:44.520489  5276 sgd_solver.cpp:106] Iteration 9, lr = 1e-06
I0506 00:50:44.711431  5276 solver.cpp:240] Iteration 10, loss = 4.91026
I0506 00:50:44.711467  5276 solver.cpp:256]     Train net output #0: loss = 4.91026 (* 1 = 4.91026 loss)
I0506 00:50:44.711475  5276 sgd_solver.cpp:106] Iteration 10, lr = 1e-06
I0506 00:50:44.899363  5276 solver.cpp:240] Iteration 11, loss = 4.86899
I0506 00:50:44.899399  5276 solver.cpp:256]     Train net output #0: loss = 4.86899 (* 1 = 4.86899 loss)
I0506 00:50:44.899406  5276 sgd_solver.cpp:106] Iteration 11, lr = 1e-06
I0506 00:50:45.088861  5276 solver.cpp:240] Iteration 12, loss = 4.89932
I0506 00:50:45.088897  5276 solver.cpp:256]     Train net output #0: loss = 4.89932 (* 1 = 4.89932 loss)
I0506 00:50:45.088906  5276 sgd_solver.cpp:106] Iteration 12, lr = 1e-06
I0506 00:50:45.276039  5276 solver.cpp:240] Iteration 13, loss = 4.90831
I0506 00:50:45.276080  5276 solver.cpp:256]     Train net output #0: loss = 4.90831 (* 1 = 4.90831 loss)
I0506 00:50:45.276093  5276 sgd_solver.cpp:106] Iteration 13, lr = 1e-06
I0506 00:50:45.464818  5276 solver.cpp:240] Iteration 14, loss = 4.8769
I0506 00:50:45.464854  5276 solver.cpp:256]     Train net output #0: loss = 4.8769 (* 1 = 4.8769 loss)
I0506 00:50:45.464861  5276 sgd_solver.cpp:106] Iteration 14, lr = 1e-06
I0506 00:50:45.656793  5276 solver.cpp:240] Iteration 15, loss = 4.90384
I0506 00:50:45.656828  5276 solver.cpp:256]     Train net output #0: loss = 4.90384 (* 1 = 4.90384 loss)
I0506 00:50:45.656836  5276 sgd_solver.cpp:106] Iteration 15, lr = 1e-06
I0506 00:50:45.845320  5276 solver.cpp:240] Iteration 16, loss = 4.87202
I0506 00:50:45.845356  5276 solver.cpp:256]     Train net output #0: loss = 4.87202 (* 1 = 4.87202 loss)
I0506 00:50:45.845365  5276 sgd_solver.cpp:106] Iteration 16, lr = 1e-06
I0506 00:50:46.035428  5276 solver.cpp:240] Iteration 17, loss = 4.87034
I0506 00:50:46.035467  5276 solver.cpp:256]     Train net output #0: loss = 4.87034 (* 1 = 4.87034 loss)
I0506 00:50:46.035475  5276 sgd_solver.cpp:106] Iteration 17, lr = 1e-06
I0506 00:50:46.222724  5276 solver.cpp:240] Iteration 18, loss = 4.8691
I0506 00:50:46.222760  5276 solver.cpp:256]     Train net output #0: loss = 4.8691 (* 1 = 4.8691 loss)
I0506 00:50:46.222769  5276 sgd_solver.cpp:106] Iteration 18, lr = 1e-06
I0506 00:50:46.412577  5276 solver.cpp:240] Iteration 19, loss = 4.86706
I0506 00:50:46.412616  5276 solver.cpp:256]     Train net output #0: loss = 4.86706 (* 1 = 4.86706 loss)
I0506 00:50:46.412624  5276 sgd_solver.cpp:106] Iteration 19, lr = 1e-06
I0506 00:50:46.599608  5276 solver.cpp:240] Iteration 20, loss = 4.86782
I0506 00:50:46.599645  5276 solver.cpp:256]     Train net output #0: loss = 4.86782 (* 1 = 4.86782 loss)
I0506 00:50:46.599653  5276 sgd_solver.cpp:106] Iteration 20, lr = 1e-06
I0506 00:50:46.787948  5276 solver.cpp:240] Iteration 21, loss = 4.87949
I0506 00:50:46.787984  5276 solver.cpp:256]     Train net output #0: loss = 4.87949 (* 1 = 4.87949 loss)
I0506 00:50:46.787992  5276 sgd_solver.cpp:106] Iteration 21, lr = 1e-06
I0506 00:50:46.978010  5276 solver.cpp:240] Iteration 22, loss = 4.83187
I0506 00:50:46.978049  5276 solver.cpp:256]     Train net output #0: loss = 4.83187 (* 1 = 4.83187 loss)
I0506 00:50:46.978056  5276 sgd_solver.cpp:106] Iteration 22, lr = 1e-06
I0506 00:50:47.165853  5276 solver.cpp:240] Iteration 23, loss = 4.84965
I0506 00:50:47.165889  5276 solver.cpp:256]     Train net output #0: loss = 4.84965 (* 1 = 4.84965 loss)
I0506 00:50:47.165896  5276 sgd_solver.cpp:106] Iteration 23, lr = 1e-06
I0506 00:50:47.356081  5276 solver.cpp:240] Iteration 24, loss = 4.85301
I0506 00:50:47.356117  5276 solver.cpp:256]     Train net output #0: loss = 4.85301 (* 1 = 4.85301 loss)
I0506 00:50:47.356125  5276 sgd_solver.cpp:106] Iteration 24, lr = 1e-06
I0506 00:50:47.543454  5276 solver.cpp:240] Iteration 25, loss = 4.84064
I0506 00:50:47.543489  5276 solver.cpp:256]     Train net output #0: loss = 4.84064 (* 1 = 4.84064 loss)
I0506 00:50:47.543498  5276 sgd_solver.cpp:106] Iteration 25, lr = 1e-06
I0506 00:50:47.732239  5276 solver.cpp:240] Iteration 26, loss = 4.82933
I0506 00:50:47.732273  5276 solver.cpp:256]     Train net output #0: loss = 4.82933 (* 1 = 4.82933 loss)
I0506 00:50:47.732280  5276 sgd_solver.cpp:106] Iteration 26, lr = 1e-06
I0506 00:50:47.921619  5276 solver.cpp:240] Iteration 27, loss = 4.83971
I0506 00:50:47.921656  5276 solver.cpp:256]     Train net output #0: loss = 4.83971 (* 1 = 4.83971 loss)
I0506 00:50:47.921663  5276 sgd_solver.cpp:106] Iteration 27, lr = 1e-06
I0506 00:50:48.108768  5276 solver.cpp:240] Iteration 28, loss = 4.84426
I0506 00:50:48.108803  5276 solver.cpp:256]     Train net output #0: loss = 4.84426 (* 1 = 4.84426 loss)
I0506 00:50:48.108811  5276 sgd_solver.cpp:106] Iteration 28, lr = 1e-06
I0506 00:50:48.300114  5276 solver.cpp:240] Iteration 29, loss = 4.8386
I0506 00:50:48.300149  5276 solver.cpp:256]     Train net output #0: loss = 4.8386 (* 1 = 4.8386 loss)
I0506 00:50:48.300156  5276 sgd_solver.cpp:106] Iteration 29, lr = 1e-06
I0506 00:50:48.488890  5276 solver.cpp:240] Iteration 30, loss = 4.86499
I0506 00:50:48.488927  5276 solver.cpp:256]     Train net output #0: loss = 4.86499 (* 1 = 4.86499 loss)
I0506 00:50:48.488934  5276 sgd_solver.cpp:106] Iteration 30, lr = 1e-06
I0506 00:50:48.677173  5276 solver.cpp:240] Iteration 31, loss = 4.82972
I0506 00:50:48.677208  5276 solver.cpp:256]     Train net output #0: loss = 4.82972 (* 1 = 4.82972 loss)
I0506 00:50:48.677217  5276 sgd_solver.cpp:106] Iteration 31, lr = 1e-06
I0506 00:50:48.866340  5276 solver.cpp:240] Iteration 32, loss = 4.83661
I0506 00:50:48.866376  5276 solver.cpp:256]     Train net output #0: loss = 4.83661 (* 1 = 4.83661 loss)
I0506 00:50:48.866384  5276 sgd_solver.cpp:106] Iteration 32, lr = 1e-06
I0506 00:50:49.053699  5276 solver.cpp:240] Iteration 33, loss = 4.80044
I0506 00:50:49.053732  5276 solver.cpp:256]     Train net output #0: loss = 4.80044 (* 1 = 4.80044 loss)
I0506 00:50:49.053740  5276 sgd_solver.cpp:106] Iteration 33, lr = 1e-06
I0506 00:50:49.243605  5276 solver.cpp:240] Iteration 34, loss = 4.80811
I0506 00:50:49.243674  5276 solver.cpp:256]     Train net output #0: loss = 4.80811 (* 1 = 4.80811 loss)
I0506 00:50:49.243681  5276 sgd_solver.cpp:106] Iteration 34, lr = 1e-06
I0506 00:50:49.430840  5276 solver.cpp:240] Iteration 35, loss = 4.82747
I0506 00:50:49.430877  5276 solver.cpp:256]     Train net output #0: loss = 4.82747 (* 1 = 4.82747 loss)
I0506 00:50:49.430884  5276 sgd_solver.cpp:106] Iteration 35, lr = 1e-06
I0506 00:50:49.621282  5276 solver.cpp:240] Iteration 36, loss = 4.8133
I0506 00:50:49.621337  5276 solver.cpp:256]     Train net output #0: loss = 4.8133 (* 1 = 4.8133 loss)
I0506 00:50:49.621352  5276 sgd_solver.cpp:106] Iteration 36, lr = 1e-06
I0506 00:50:49.809540  5276 solver.cpp:240] Iteration 37, loss = 4.81475
I0506 00:50:49.809581  5276 solver.cpp:256]     Train net output #0: loss = 4.81475 (* 1 = 4.81475 loss)
I0506 00:50:49.809589  5276 sgd_solver.cpp:106] Iteration 37, lr = 1e-06
I0506 00:50:49.998428  5276 solver.cpp:240] Iteration 38, loss = 4.78741
I0506 00:50:49.998473  5276 solver.cpp:256]     Train net output #0: loss = 4.78741 (* 1 = 4.78741 loss)
I0506 00:50:49.998482  5276 sgd_solver.cpp:106] Iteration 38, lr = 1e-06
I0506 00:50:50.188874  5276 solver.cpp:240] Iteration 39, loss = 4.79332
I0506 00:50:50.188920  5276 solver.cpp:256]     Train net output #0: loss = 4.79332 (* 1 = 4.79332 loss)
I0506 00:50:50.188927  5276 sgd_solver.cpp:106] Iteration 39, lr = 1e-06
I0506 00:50:50.376700  5276 solver.cpp:240] Iteration 40, loss = 4.81754
I0506 00:50:50.376736  5276 solver.cpp:256]     Train net output #0: loss = 4.81754 (* 1 = 4.81754 loss)
I0506 00:50:50.376744  5276 sgd_solver.cpp:106] Iteration 40, lr = 1e-06
I0506 00:50:50.566370  5276 solver.cpp:240] Iteration 41, loss = 4.80394
I0506 00:50:50.566406  5276 solver.cpp:256]     Train net output #0: loss = 4.80394 (* 1 = 4.80394 loss)
I0506 00:50:50.566414  5276 sgd_solver.cpp:106] Iteration 41, lr = 1e-06
I0506 00:50:50.753389  5276 solver.cpp:240] Iteration 42, loss = 4.77443
I0506 00:50:50.753427  5276 solver.cpp:256]     Train net output #0: loss = 4.77443 (* 1 = 4.77443 loss)
I0506 00:50:50.753434  5276 sgd_solver.cpp:106] Iteration 42, lr = 1e-06
I0506 00:50:50.942395  5276 solver.cpp:240] Iteration 43, loss = 4.78118
I0506 00:50:50.942432  5276 solver.cpp:256]     Train net output #0: loss = 4.78118 (* 1 = 4.78118 loss)
I0506 00:50:50.942440  5276 sgd_solver.cpp:106] Iteration 43, lr = 1e-06
I0506 00:50:51.131378  5276 solver.cpp:240] Iteration 44, loss = 4.79115
I0506 00:50:51.131420  5276 solver.cpp:256]     Train net output #0: loss = 4.79115 (* 1 = 4.79115 loss)
I0506 00:50:51.131428  5276 sgd_solver.cpp:106] Iteration 44, lr = 1e-06
I0506 00:50:51.320030  5276 solver.cpp:240] Iteration 45, loss = 4.76553
I0506 00:50:51.320065  5276 solver.cpp:256]     Train net output #0: loss = 4.76553 (* 1 = 4.76553 loss)
I0506 00:50:51.320072  5276 sgd_solver.cpp:106] Iteration 45, lr = 1e-06
I0506 00:50:51.511359  5276 solver.cpp:240] Iteration 46, loss = 4.75394
I0506 00:50:51.511395  5276 solver.cpp:256]     Train net output #0: loss = 4.75394 (* 1 = 4.75394 loss)
I0506 00:50:51.511404  5276 sgd_solver.cpp:106] Iteration 46, lr = 1e-06
I0506 00:50:51.699865  5276 solver.cpp:240] Iteration 47, loss = 4.76688
I0506 00:50:51.699920  5276 solver.cpp:256]     Train net output #0: loss = 4.76688 (* 1 = 4.76688 loss)
I0506 00:50:51.699929  5276 sgd_solver.cpp:106] Iteration 47, lr = 1e-06
I0506 00:50:51.889719  5276 solver.cpp:240] Iteration 48, loss = 4.75452
I0506 00:50:51.889756  5276 solver.cpp:256]     Train net output #0: loss = 4.75452 (* 1 = 4.75452 loss)
I0506 00:50:51.889765  5276 sgd_solver.cpp:106] Iteration 48, lr = 1e-06
I0506 00:50:52.077087  5276 solver.cpp:240] Iteration 49, loss = 4.77049
I0506 00:50:52.077126  5276 solver.cpp:256]     Train net output #0: loss = 4.77049 (* 1 = 4.77049 loss)
I0506 00:50:52.077133  5276 sgd_solver.cpp:106] Iteration 49, lr = 1e-06
