I0411 00:56:47.366658 16553 caffe.cpp:217] Using GPUs 1
I0411 00:56:47.669276 16553 caffe.cpp:222] GPU 1: GeForce GTX 1070
I0411 00:56:48.527215 16553 solver.cpp:60] Initializing solver from parameters: 
train_net: "./Prototxt/experiment_8/rtsd-r1/CoNorm/trial_1/train.prototxt"
test_net: "./Prototxt/experiment_8/rtsd-r1/CoNorm/trial_1/test.prototxt"
test_iter: 8
test_interval: 25
base_lr: 0.001
display: 1
max_iter: 2500
lr_policy: "step"
gamma: 0.5
momentum: 0.9
weight_decay: 0.0005
stepsize: 500
snapshot: 250
snapshot_prefix: "./snapshots/experiment_8/rtsd-r1/CoNorm/trial_1/snap"
solver_mode: GPU
device_id: 1
train_state {
  level: 0
  stage: ""
}
iter_size: 1
type: "Adam"
I0411 00:56:48.527367 16553 solver.cpp:93] Creating training net from train_net file: ./Prototxt/experiment_8/rtsd-r1/CoNorm/trial_1/train.prototxt
I0411 00:56:48.527717 16553 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_1
I0411 00:56:48.527730 16553 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_5
I0411 00:56:48.527922 16553 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: false
    crop_size: 48
    mean_value: 132
    mean_value: 132
    mean_value: 131
  }
  data_param {
    source: "../local_data/lmdb/rtsd-r1/CoNorm/train/lmdb"
    batch_size: 1024
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 7
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_prescale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv1_sTanH"
  type: "TanH"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_postscale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 150
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_prescale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv2_sTanH"
  type: "TanH"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_postscale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 250
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_prescale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv3_sTanH"
  type: "TanH"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_postscale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc4_300"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc4_300"
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fc4_prescale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "fc4_sTanH"
  type: "TanH"
  bottom: "fc4_300"
  top: "fc4_300"
}
layer {
  name: "fc4_postscale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "fc5_67"
  type: "InnerProduct"
  bottom: "fc4_300"
  top: "fc5_classes"
  inner_product_param {
    num_output: 67
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc5_classes"
  bottom: "label"
  top: "loss"
}
I0411 00:56:48.528044 16553 layer_factory.hpp:77] Creating layer data
I0411 00:56:48.529332 16553 net.cpp:100] Creating Layer data
I0411 00:56:48.529361 16553 net.cpp:408] data -> data
I0411 00:56:48.529386 16553 net.cpp:408] data -> label
I0411 00:56:48.532997 16658 db_lmdb.cpp:35] Opened lmdb ../local_data/lmdb/rtsd-r1/CoNorm/train/lmdb
I0411 00:56:48.553200 16553 data_layer.cpp:41] output data size: 1024,3,48,48
I0411 00:56:48.614699 16553 net.cpp:150] Setting up data
I0411 00:56:48.614735 16553 net.cpp:157] Top shape: 1024 3 48 48 (7077888)
I0411 00:56:48.614742 16553 net.cpp:157] Top shape: 1024 (1024)
I0411 00:56:48.614747 16553 net.cpp:165] Memory required for data: 28315648
I0411 00:56:48.614758 16553 layer_factory.hpp:77] Creating layer conv1
I0411 00:56:48.614784 16553 net.cpp:100] Creating Layer conv1
I0411 00:56:48.614792 16553 net.cpp:434] conv1 <- data
I0411 00:56:48.614810 16553 net.cpp:408] conv1 -> conv1
I0411 00:56:48.957546 16553 net.cpp:150] Setting up conv1
I0411 00:56:48.957577 16553 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:56:48.957582 16553 net.cpp:165] Memory required for data: 750850048
I0411 00:56:48.957607 16553 layer_factory.hpp:77] Creating layer conv1_prescale
I0411 00:56:48.957623 16553 net.cpp:100] Creating Layer conv1_prescale
I0411 00:56:48.957630 16553 net.cpp:434] conv1_prescale <- conv1
I0411 00:56:48.957638 16553 net.cpp:395] conv1_prescale -> conv1 (in-place)
I0411 00:56:48.957769 16553 net.cpp:150] Setting up conv1_prescale
I0411 00:56:48.957780 16553 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:56:48.957783 16553 net.cpp:165] Memory required for data: 1473384448
I0411 00:56:48.957792 16553 layer_factory.hpp:77] Creating layer conv1_sTanH
I0411 00:56:48.957801 16553 net.cpp:100] Creating Layer conv1_sTanH
I0411 00:56:48.957805 16553 net.cpp:434] conv1_sTanH <- conv1
I0411 00:56:48.957810 16553 net.cpp:395] conv1_sTanH -> conv1 (in-place)
I0411 00:56:48.958053 16553 net.cpp:150] Setting up conv1_sTanH
I0411 00:56:48.958067 16553 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:56:48.958072 16553 net.cpp:165] Memory required for data: 2195918848
I0411 00:56:48.958077 16553 layer_factory.hpp:77] Creating layer conv1_postscale
I0411 00:56:48.958087 16553 net.cpp:100] Creating Layer conv1_postscale
I0411 00:56:48.958092 16553 net.cpp:434] conv1_postscale <- conv1
I0411 00:56:48.958099 16553 net.cpp:395] conv1_postscale -> conv1 (in-place)
I0411 00:56:48.958214 16553 net.cpp:150] Setting up conv1_postscale
I0411 00:56:48.958225 16553 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:56:48.958227 16553 net.cpp:165] Memory required for data: 2918453248
I0411 00:56:48.958235 16553 layer_factory.hpp:77] Creating layer pool1
I0411 00:56:48.958243 16553 net.cpp:100] Creating Layer pool1
I0411 00:56:48.958250 16553 net.cpp:434] pool1 <- conv1
I0411 00:56:48.958254 16553 net.cpp:408] pool1 -> pool1
I0411 00:56:48.958309 16553 net.cpp:150] Setting up pool1
I0411 00:56:48.958318 16553 net.cpp:157] Top shape: 1024 100 21 21 (45158400)
I0411 00:56:48.958323 16553 net.cpp:165] Memory required for data: 3099086848
I0411 00:56:48.958346 16553 layer_factory.hpp:77] Creating layer conv2
I0411 00:56:48.958359 16553 net.cpp:100] Creating Layer conv2
I0411 00:56:48.958364 16553 net.cpp:434] conv2 <- pool1
I0411 00:56:48.958370 16553 net.cpp:408] conv2 -> conv2
I0411 00:56:48.970638 16553 net.cpp:150] Setting up conv2
I0411 00:56:48.970660 16553 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:56:48.970666 16553 net.cpp:165] Memory required for data: 3298152448
I0411 00:56:48.970679 16553 layer_factory.hpp:77] Creating layer conv2_prescale
I0411 00:56:48.970691 16553 net.cpp:100] Creating Layer conv2_prescale
I0411 00:56:48.970697 16553 net.cpp:434] conv2_prescale <- conv2
I0411 00:56:48.970705 16553 net.cpp:395] conv2_prescale -> conv2 (in-place)
I0411 00:56:48.970839 16553 net.cpp:150] Setting up conv2_prescale
I0411 00:56:48.970849 16553 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:56:48.970852 16553 net.cpp:165] Memory required for data: 3497218048
I0411 00:56:48.970859 16553 layer_factory.hpp:77] Creating layer conv2_sTanH
I0411 00:56:48.970868 16553 net.cpp:100] Creating Layer conv2_sTanH
I0411 00:56:48.970873 16553 net.cpp:434] conv2_sTanH <- conv2
I0411 00:56:48.970880 16553 net.cpp:395] conv2_sTanH -> conv2 (in-place)
I0411 00:56:48.972611 16553 net.cpp:150] Setting up conv2_sTanH
I0411 00:56:48.972631 16553 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:56:48.972636 16553 net.cpp:165] Memory required for data: 3696283648
I0411 00:56:48.972641 16553 layer_factory.hpp:77] Creating layer conv2_postscale
I0411 00:56:48.972653 16553 net.cpp:100] Creating Layer conv2_postscale
I0411 00:56:48.972659 16553 net.cpp:434] conv2_postscale <- conv2
I0411 00:56:48.972666 16553 net.cpp:395] conv2_postscale -> conv2 (in-place)
I0411 00:56:48.972784 16553 net.cpp:150] Setting up conv2_postscale
I0411 00:56:48.972793 16553 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:56:48.972796 16553 net.cpp:165] Memory required for data: 3895349248
I0411 00:56:48.972803 16553 layer_factory.hpp:77] Creating layer pool2
I0411 00:56:48.972811 16553 net.cpp:100] Creating Layer pool2
I0411 00:56:48.972817 16553 net.cpp:434] pool2 <- conv2
I0411 00:56:48.972823 16553 net.cpp:408] pool2 -> pool2
I0411 00:56:48.972869 16553 net.cpp:150] Setting up pool2
I0411 00:56:48.972877 16553 net.cpp:157] Top shape: 1024 150 9 9 (12441600)
I0411 00:56:48.972882 16553 net.cpp:165] Memory required for data: 3945115648
I0411 00:56:48.972885 16553 layer_factory.hpp:77] Creating layer conv3
I0411 00:56:48.972894 16553 net.cpp:100] Creating Layer conv3
I0411 00:56:48.972900 16553 net.cpp:434] conv3 <- pool2
I0411 00:56:48.972906 16553 net.cpp:408] conv3 -> conv3
I0411 00:56:48.980469 16553 net.cpp:150] Setting up conv3
I0411 00:56:48.980491 16553 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:56:48.980502 16553 net.cpp:165] Memory required for data: 3981979648
I0411 00:56:48.980516 16553 layer_factory.hpp:77] Creating layer conv3_prescale
I0411 00:56:48.980526 16553 net.cpp:100] Creating Layer conv3_prescale
I0411 00:56:48.980536 16553 net.cpp:434] conv3_prescale <- conv3
I0411 00:56:48.980543 16553 net.cpp:395] conv3_prescale -> conv3 (in-place)
I0411 00:56:48.980659 16553 net.cpp:150] Setting up conv3_prescale
I0411 00:56:48.980669 16553 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:56:48.980674 16553 net.cpp:165] Memory required for data: 4018843648
I0411 00:56:48.980679 16553 layer_factory.hpp:77] Creating layer conv3_sTanH
I0411 00:56:48.980686 16553 net.cpp:100] Creating Layer conv3_sTanH
I0411 00:56:48.980690 16553 net.cpp:434] conv3_sTanH <- conv3
I0411 00:56:48.980695 16553 net.cpp:395] conv3_sTanH -> conv3 (in-place)
I0411 00:56:48.982332 16553 net.cpp:150] Setting up conv3_sTanH
I0411 00:56:48.982355 16553 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:56:48.982360 16553 net.cpp:165] Memory required for data: 4055707648
I0411 00:56:48.982365 16553 layer_factory.hpp:77] Creating layer conv3_postscale
I0411 00:56:48.982375 16553 net.cpp:100] Creating Layer conv3_postscale
I0411 00:56:48.982398 16553 net.cpp:434] conv3_postscale <- conv3
I0411 00:56:48.982406 16553 net.cpp:395] conv3_postscale -> conv3 (in-place)
I0411 00:56:48.982540 16553 net.cpp:150] Setting up conv3_postscale
I0411 00:56:48.982552 16553 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:56:48.982558 16553 net.cpp:165] Memory required for data: 4092571648
I0411 00:56:48.982564 16553 layer_factory.hpp:77] Creating layer pool3
I0411 00:56:48.982574 16553 net.cpp:100] Creating Layer pool3
I0411 00:56:48.982580 16553 net.cpp:434] pool3 <- conv3
I0411 00:56:48.982586 16553 net.cpp:408] pool3 -> pool3
I0411 00:56:48.982635 16553 net.cpp:150] Setting up pool3
I0411 00:56:48.982643 16553 net.cpp:157] Top shape: 1024 250 3 3 (2304000)
I0411 00:56:48.982647 16553 net.cpp:165] Memory required for data: 4101787648
I0411 00:56:48.982651 16553 layer_factory.hpp:77] Creating layer fc4_300
I0411 00:56:48.982662 16553 net.cpp:100] Creating Layer fc4_300
I0411 00:56:48.982667 16553 net.cpp:434] fc4_300 <- pool3
I0411 00:56:48.982674 16553 net.cpp:408] fc4_300 -> fc4_300
I0411 00:56:48.990761 16553 net.cpp:150] Setting up fc4_300
I0411 00:56:48.990784 16553 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:56:48.990789 16553 net.cpp:165] Memory required for data: 4103016448
I0411 00:56:48.990799 16553 layer_factory.hpp:77] Creating layer fc4_prescale
I0411 00:56:48.990809 16553 net.cpp:100] Creating Layer fc4_prescale
I0411 00:56:48.990814 16553 net.cpp:434] fc4_prescale <- fc4_300
I0411 00:56:48.990821 16553 net.cpp:395] fc4_prescale -> fc4_300 (in-place)
I0411 00:56:48.990936 16553 net.cpp:150] Setting up fc4_prescale
I0411 00:56:48.990947 16553 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:56:48.990953 16553 net.cpp:165] Memory required for data: 4104245248
I0411 00:56:48.990959 16553 layer_factory.hpp:77] Creating layer fc4_sTanH
I0411 00:56:48.990967 16553 net.cpp:100] Creating Layer fc4_sTanH
I0411 00:56:48.990973 16553 net.cpp:434] fc4_sTanH <- fc4_300
I0411 00:56:48.990978 16553 net.cpp:395] fc4_sTanH -> fc4_300 (in-place)
I0411 00:56:48.991214 16553 net.cpp:150] Setting up fc4_sTanH
I0411 00:56:48.991230 16553 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:56:48.991235 16553 net.cpp:165] Memory required for data: 4105474048
I0411 00:56:48.991240 16553 layer_factory.hpp:77] Creating layer fc4_postscale
I0411 00:56:48.991250 16553 net.cpp:100] Creating Layer fc4_postscale
I0411 00:56:48.991255 16553 net.cpp:434] fc4_postscale <- fc4_300
I0411 00:56:48.991262 16553 net.cpp:395] fc4_postscale -> fc4_300 (in-place)
I0411 00:56:48.991379 16553 net.cpp:150] Setting up fc4_postscale
I0411 00:56:48.991389 16553 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:56:48.991394 16553 net.cpp:165] Memory required for data: 4106702848
I0411 00:56:48.991400 16553 layer_factory.hpp:77] Creating layer fc5_67
I0411 00:56:48.991408 16553 net.cpp:100] Creating Layer fc5_67
I0411 00:56:48.991415 16553 net.cpp:434] fc5_67 <- fc4_300
I0411 00:56:48.991420 16553 net.cpp:408] fc5_67 -> fc5_classes
I0411 00:56:48.993310 16553 net.cpp:150] Setting up fc5_67
I0411 00:56:48.993330 16553 net.cpp:157] Top shape: 1024 67 (68608)
I0411 00:56:48.993335 16553 net.cpp:165] Memory required for data: 4106977280
I0411 00:56:48.993348 16553 layer_factory.hpp:77] Creating layer loss
I0411 00:56:48.993360 16553 net.cpp:100] Creating Layer loss
I0411 00:56:48.993363 16553 net.cpp:434] loss <- fc5_classes
I0411 00:56:48.993369 16553 net.cpp:434] loss <- label
I0411 00:56:48.993376 16553 net.cpp:408] loss -> loss
I0411 00:56:48.993391 16553 layer_factory.hpp:77] Creating layer loss
I0411 00:56:48.993810 16553 net.cpp:150] Setting up loss
I0411 00:56:48.993824 16553 net.cpp:157] Top shape: (1)
I0411 00:56:48.993829 16553 net.cpp:160]     with loss weight 1
I0411 00:56:48.993857 16553 net.cpp:165] Memory required for data: 4106977284
I0411 00:56:48.993863 16553 net.cpp:226] loss needs backward computation.
I0411 00:56:48.993872 16553 net.cpp:226] fc5_67 needs backward computation.
I0411 00:56:48.993876 16553 net.cpp:226] fc4_postscale needs backward computation.
I0411 00:56:48.993896 16553 net.cpp:226] fc4_sTanH needs backward computation.
I0411 00:56:48.993901 16553 net.cpp:226] fc4_prescale needs backward computation.
I0411 00:56:48.993904 16553 net.cpp:226] fc4_300 needs backward computation.
I0411 00:56:48.993909 16553 net.cpp:226] pool3 needs backward computation.
I0411 00:56:48.993913 16553 net.cpp:226] conv3_postscale needs backward computation.
I0411 00:56:48.993917 16553 net.cpp:226] conv3_sTanH needs backward computation.
I0411 00:56:48.993921 16553 net.cpp:226] conv3_prescale needs backward computation.
I0411 00:56:48.993926 16553 net.cpp:226] conv3 needs backward computation.
I0411 00:56:48.993929 16553 net.cpp:226] pool2 needs backward computation.
I0411 00:56:48.993933 16553 net.cpp:226] conv2_postscale needs backward computation.
I0411 00:56:48.993937 16553 net.cpp:226] conv2_sTanH needs backward computation.
I0411 00:56:48.993942 16553 net.cpp:226] conv2_prescale needs backward computation.
I0411 00:56:48.993945 16553 net.cpp:226] conv2 needs backward computation.
I0411 00:56:48.993949 16553 net.cpp:226] pool1 needs backward computation.
I0411 00:56:48.993953 16553 net.cpp:226] conv1_postscale needs backward computation.
I0411 00:56:48.993957 16553 net.cpp:226] conv1_sTanH needs backward computation.
I0411 00:56:48.993962 16553 net.cpp:226] conv1_prescale needs backward computation.
I0411 00:56:48.993964 16553 net.cpp:226] conv1 needs backward computation.
I0411 00:56:48.993970 16553 net.cpp:228] data does not need backward computation.
I0411 00:56:48.993974 16553 net.cpp:270] This network produces output loss
I0411 00:56:48.993993 16553 net.cpp:283] Network initialization done.
I0411 00:56:48.994313 16553 solver.cpp:193] Creating test net (#0) specified by test_net file: ./Prototxt/experiment_8/rtsd-r1/CoNorm/trial_1/test.prototxt
I0411 00:56:48.994536 16553 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 48
    mean_value: 133
    mean_value: 133
    mean_value: 132
  }
  data_param {
    source: "../local_data/lmdb/rtsd-r1/CoNorm/test/lmdb"
    batch_size: 1024
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 7
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_prescale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv1_sTanH"
  type: "TanH"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_postscale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 150
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_prescale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv2_sTanH"
  type: "TanH"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_postscale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 250
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_prescale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv3_sTanH"
  type: "TanH"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_postscale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc4_300"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc4_300"
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fc4_prescale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "fc4_sTanH"
  type: "TanH"
  bottom: "fc4_300"
  top: "fc4_300"
}
layer {
  name: "fc4_postscale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "fc5_67"
  type: "InnerProduct"
  bottom: "fc4_300"
  top: "fc5_classes"
  inner_product_param {
    num_output: 67
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc5_classes"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy_1"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_1"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 1
  }
}
layer {
  name: "accuracy_5"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0411 00:56:48.994673 16553 layer_factory.hpp:77] Creating layer data
I0411 00:56:48.995519 16553 net.cpp:100] Creating Layer data
I0411 00:56:48.995550 16553 net.cpp:408] data -> data
I0411 00:56:48.995564 16553 net.cpp:408] data -> label
I0411 00:56:48.997941 16700 db_lmdb.cpp:35] Opened lmdb ../local_data/lmdb/rtsd-r1/CoNorm/test/lmdb
I0411 00:56:48.998180 16553 data_layer.cpp:41] output data size: 1024,3,48,48
I0411 00:56:49.045528 16553 net.cpp:150] Setting up data
I0411 00:56:49.045558 16553 net.cpp:157] Top shape: 1024 3 48 48 (7077888)
I0411 00:56:49.045565 16553 net.cpp:157] Top shape: 1024 (1024)
I0411 00:56:49.045569 16553 net.cpp:165] Memory required for data: 28315648
I0411 00:56:49.045577 16553 layer_factory.hpp:77] Creating layer label_data_1_split
I0411 00:56:49.045593 16553 net.cpp:100] Creating Layer label_data_1_split
I0411 00:56:49.045599 16553 net.cpp:434] label_data_1_split <- label
I0411 00:56:49.045608 16553 net.cpp:408] label_data_1_split -> label_data_1_split_0
I0411 00:56:49.045626 16553 net.cpp:408] label_data_1_split -> label_data_1_split_1
I0411 00:56:49.045636 16553 net.cpp:408] label_data_1_split -> label_data_1_split_2
I0411 00:56:49.045742 16553 net.cpp:150] Setting up label_data_1_split
I0411 00:56:49.045753 16553 net.cpp:157] Top shape: 1024 (1024)
I0411 00:56:49.045759 16553 net.cpp:157] Top shape: 1024 (1024)
I0411 00:56:49.045764 16553 net.cpp:157] Top shape: 1024 (1024)
I0411 00:56:49.045768 16553 net.cpp:165] Memory required for data: 28327936
I0411 00:56:49.045773 16553 layer_factory.hpp:77] Creating layer conv1
I0411 00:56:49.045789 16553 net.cpp:100] Creating Layer conv1
I0411 00:56:49.045794 16553 net.cpp:434] conv1 <- data
I0411 00:56:49.045801 16553 net.cpp:408] conv1 -> conv1
I0411 00:56:49.057896 16553 net.cpp:150] Setting up conv1
I0411 00:56:49.057927 16553 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:56:49.057932 16553 net.cpp:165] Memory required for data: 750862336
I0411 00:56:49.057970 16553 layer_factory.hpp:77] Creating layer conv1_prescale
I0411 00:56:49.057993 16553 net.cpp:100] Creating Layer conv1_prescale
I0411 00:56:49.058001 16553 net.cpp:434] conv1_prescale <- conv1
I0411 00:56:49.058007 16553 net.cpp:395] conv1_prescale -> conv1 (in-place)
I0411 00:56:49.060286 16553 net.cpp:150] Setting up conv1_prescale
I0411 00:56:49.060305 16553 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:56:49.060309 16553 net.cpp:165] Memory required for data: 1473396736
I0411 00:56:49.060333 16553 layer_factory.hpp:77] Creating layer conv1_sTanH
I0411 00:56:49.060344 16553 net.cpp:100] Creating Layer conv1_sTanH
I0411 00:56:49.060353 16553 net.cpp:434] conv1_sTanH <- conv1
I0411 00:56:49.060359 16553 net.cpp:395] conv1_sTanH -> conv1 (in-place)
I0411 00:56:49.060616 16553 net.cpp:150] Setting up conv1_sTanH
I0411 00:56:49.060629 16553 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:56:49.060633 16553 net.cpp:165] Memory required for data: 2195931136
I0411 00:56:49.060637 16553 layer_factory.hpp:77] Creating layer conv1_postscale
I0411 00:56:49.060645 16553 net.cpp:100] Creating Layer conv1_postscale
I0411 00:56:49.060649 16553 net.cpp:434] conv1_postscale <- conv1
I0411 00:56:49.060657 16553 net.cpp:395] conv1_postscale -> conv1 (in-place)
I0411 00:56:49.060799 16553 net.cpp:150] Setting up conv1_postscale
I0411 00:56:49.060809 16553 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:56:49.060813 16553 net.cpp:165] Memory required for data: 2918465536
I0411 00:56:49.060819 16553 layer_factory.hpp:77] Creating layer pool1
I0411 00:56:49.060830 16553 net.cpp:100] Creating Layer pool1
I0411 00:56:49.060837 16553 net.cpp:434] pool1 <- conv1
I0411 00:56:49.060844 16553 net.cpp:408] pool1 -> pool1
I0411 00:56:49.060907 16553 net.cpp:150] Setting up pool1
I0411 00:56:49.060919 16553 net.cpp:157] Top shape: 1024 100 21 21 (45158400)
I0411 00:56:49.060925 16553 net.cpp:165] Memory required for data: 3099099136
I0411 00:56:49.060928 16553 layer_factory.hpp:77] Creating layer conv2
I0411 00:56:49.060941 16553 net.cpp:100] Creating Layer conv2
I0411 00:56:49.060947 16553 net.cpp:434] conv2 <- pool1
I0411 00:56:49.060953 16553 net.cpp:408] conv2 -> conv2
I0411 00:56:49.066764 16553 net.cpp:150] Setting up conv2
I0411 00:56:49.066784 16553 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:56:49.066790 16553 net.cpp:165] Memory required for data: 3298164736
I0411 00:56:49.066803 16553 layer_factory.hpp:77] Creating layer conv2_prescale
I0411 00:56:49.066818 16553 net.cpp:100] Creating Layer conv2_prescale
I0411 00:56:49.066825 16553 net.cpp:434] conv2_prescale <- conv2
I0411 00:56:49.066844 16553 net.cpp:395] conv2_prescale -> conv2 (in-place)
I0411 00:56:49.066985 16553 net.cpp:150] Setting up conv2_prescale
I0411 00:56:49.066997 16553 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:56:49.067001 16553 net.cpp:165] Memory required for data: 3497230336
I0411 00:56:49.067008 16553 layer_factory.hpp:77] Creating layer conv2_sTanH
I0411 00:56:49.067015 16553 net.cpp:100] Creating Layer conv2_sTanH
I0411 00:56:49.067021 16553 net.cpp:434] conv2_sTanH <- conv2
I0411 00:56:49.067028 16553 net.cpp:395] conv2_sTanH -> conv2 (in-place)
I0411 00:56:49.068084 16553 net.cpp:150] Setting up conv2_sTanH
I0411 00:56:49.068104 16553 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:56:49.068110 16553 net.cpp:165] Memory required for data: 3696295936
I0411 00:56:49.068115 16553 layer_factory.hpp:77] Creating layer conv2_postscale
I0411 00:56:49.068125 16553 net.cpp:100] Creating Layer conv2_postscale
I0411 00:56:49.068131 16553 net.cpp:434] conv2_postscale <- conv2
I0411 00:56:49.068142 16553 net.cpp:395] conv2_postscale -> conv2 (in-place)
I0411 00:56:49.068270 16553 net.cpp:150] Setting up conv2_postscale
I0411 00:56:49.068282 16553 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:56:49.068287 16553 net.cpp:165] Memory required for data: 3895361536
I0411 00:56:49.068295 16553 layer_factory.hpp:77] Creating layer pool2
I0411 00:56:49.068323 16553 net.cpp:100] Creating Layer pool2
I0411 00:56:49.068330 16553 net.cpp:434] pool2 <- conv2
I0411 00:56:49.068336 16553 net.cpp:408] pool2 -> pool2
I0411 00:56:49.068397 16553 net.cpp:150] Setting up pool2
I0411 00:56:49.068406 16553 net.cpp:157] Top shape: 1024 150 9 9 (12441600)
I0411 00:56:49.068411 16553 net.cpp:165] Memory required for data: 3945127936
I0411 00:56:49.068415 16553 layer_factory.hpp:77] Creating layer conv3
I0411 00:56:49.068426 16553 net.cpp:100] Creating Layer conv3
I0411 00:56:49.068433 16553 net.cpp:434] conv3 <- pool2
I0411 00:56:49.068440 16553 net.cpp:408] conv3 -> conv3
I0411 00:56:49.076171 16553 net.cpp:150] Setting up conv3
I0411 00:56:49.076195 16553 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:56:49.076201 16553 net.cpp:165] Memory required for data: 3981991936
I0411 00:56:49.076216 16553 layer_factory.hpp:77] Creating layer conv3_prescale
I0411 00:56:49.076227 16553 net.cpp:100] Creating Layer conv3_prescale
I0411 00:56:49.076233 16553 net.cpp:434] conv3_prescale <- conv3
I0411 00:56:49.076242 16553 net.cpp:395] conv3_prescale -> conv3 (in-place)
I0411 00:56:49.076362 16553 net.cpp:150] Setting up conv3_prescale
I0411 00:56:49.076373 16553 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:56:49.076377 16553 net.cpp:165] Memory required for data: 4018855936
I0411 00:56:49.076383 16553 layer_factory.hpp:77] Creating layer conv3_sTanH
I0411 00:56:49.076393 16553 net.cpp:100] Creating Layer conv3_sTanH
I0411 00:56:49.076400 16553 net.cpp:434] conv3_sTanH <- conv3
I0411 00:56:49.076406 16553 net.cpp:395] conv3_sTanH -> conv3 (in-place)
I0411 00:56:49.086688 16553 net.cpp:150] Setting up conv3_sTanH
I0411 00:56:49.086710 16553 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:56:49.086717 16553 net.cpp:165] Memory required for data: 4055719936
I0411 00:56:49.086722 16553 layer_factory.hpp:77] Creating layer conv3_postscale
I0411 00:56:49.086732 16553 net.cpp:100] Creating Layer conv3_postscale
I0411 00:56:49.086738 16553 net.cpp:434] conv3_postscale <- conv3
I0411 00:56:49.086748 16553 net.cpp:395] conv3_postscale -> conv3 (in-place)
I0411 00:56:49.086880 16553 net.cpp:150] Setting up conv3_postscale
I0411 00:56:49.086891 16553 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:56:49.086895 16553 net.cpp:165] Memory required for data: 4092583936
I0411 00:56:49.086902 16553 layer_factory.hpp:77] Creating layer pool3
I0411 00:56:49.086916 16553 net.cpp:100] Creating Layer pool3
I0411 00:56:49.086922 16553 net.cpp:434] pool3 <- conv3
I0411 00:56:49.086928 16553 net.cpp:408] pool3 -> pool3
I0411 00:56:49.086984 16553 net.cpp:150] Setting up pool3
I0411 00:56:49.086994 16553 net.cpp:157] Top shape: 1024 250 3 3 (2304000)
I0411 00:56:49.086998 16553 net.cpp:165] Memory required for data: 4101799936
I0411 00:56:49.087002 16553 layer_factory.hpp:77] Creating layer fc4_300
I0411 00:56:49.087010 16553 net.cpp:100] Creating Layer fc4_300
I0411 00:56:49.087016 16553 net.cpp:434] fc4_300 <- pool3
I0411 00:56:49.087024 16553 net.cpp:408] fc4_300 -> fc4_300
I0411 00:56:49.095422 16553 net.cpp:150] Setting up fc4_300
I0411 00:56:49.095448 16553 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:56:49.095453 16553 net.cpp:165] Memory required for data: 4103028736
I0411 00:56:49.095461 16553 layer_factory.hpp:77] Creating layer fc4_prescale
I0411 00:56:49.095474 16553 net.cpp:100] Creating Layer fc4_prescale
I0411 00:56:49.095479 16553 net.cpp:434] fc4_prescale <- fc4_300
I0411 00:56:49.095487 16553 net.cpp:395] fc4_prescale -> fc4_300 (in-place)
I0411 00:56:49.095605 16553 net.cpp:150] Setting up fc4_prescale
I0411 00:56:49.095615 16553 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:56:49.095619 16553 net.cpp:165] Memory required for data: 4104257536
I0411 00:56:49.095625 16553 layer_factory.hpp:77] Creating layer fc4_sTanH
I0411 00:56:49.095638 16553 net.cpp:100] Creating Layer fc4_sTanH
I0411 00:56:49.095643 16553 net.cpp:434] fc4_sTanH <- fc4_300
I0411 00:56:49.095650 16553 net.cpp:395] fc4_sTanH -> fc4_300 (in-place)
I0411 00:56:49.095903 16553 net.cpp:150] Setting up fc4_sTanH
I0411 00:56:49.095935 16553 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:56:49.095940 16553 net.cpp:165] Memory required for data: 4105486336
I0411 00:56:49.095945 16553 layer_factory.hpp:77] Creating layer fc4_postscale
I0411 00:56:49.095955 16553 net.cpp:100] Creating Layer fc4_postscale
I0411 00:56:49.095962 16553 net.cpp:434] fc4_postscale <- fc4_300
I0411 00:56:49.095971 16553 net.cpp:395] fc4_postscale -> fc4_300 (in-place)
I0411 00:56:49.096103 16553 net.cpp:150] Setting up fc4_postscale
I0411 00:56:49.096120 16553 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:56:49.096123 16553 net.cpp:165] Memory required for data: 4106715136
I0411 00:56:49.096129 16553 layer_factory.hpp:77] Creating layer fc5_67
I0411 00:56:49.096143 16553 net.cpp:100] Creating Layer fc5_67
I0411 00:56:49.096148 16553 net.cpp:434] fc5_67 <- fc4_300
I0411 00:56:49.096154 16553 net.cpp:408] fc5_67 -> fc5_classes
I0411 00:56:49.096463 16553 net.cpp:150] Setting up fc5_67
I0411 00:56:49.096473 16553 net.cpp:157] Top shape: 1024 67 (68608)
I0411 00:56:49.096477 16553 net.cpp:165] Memory required for data: 4106989568
I0411 00:56:49.096489 16553 layer_factory.hpp:77] Creating layer fc5_classes_fc5_67_0_split
I0411 00:56:49.096498 16553 net.cpp:100] Creating Layer fc5_classes_fc5_67_0_split
I0411 00:56:49.096501 16553 net.cpp:434] fc5_classes_fc5_67_0_split <- fc5_classes
I0411 00:56:49.096509 16553 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_0
I0411 00:56:49.096520 16553 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_1
I0411 00:56:49.096527 16553 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_2
I0411 00:56:49.096596 16553 net.cpp:150] Setting up fc5_classes_fc5_67_0_split
I0411 00:56:49.096603 16553 net.cpp:157] Top shape: 1024 67 (68608)
I0411 00:56:49.096608 16553 net.cpp:157] Top shape: 1024 67 (68608)
I0411 00:56:49.096612 16553 net.cpp:157] Top shape: 1024 67 (68608)
I0411 00:56:49.096616 16553 net.cpp:165] Memory required for data: 4107812864
I0411 00:56:49.096619 16553 layer_factory.hpp:77] Creating layer loss
I0411 00:56:49.096627 16553 net.cpp:100] Creating Layer loss
I0411 00:56:49.096632 16553 net.cpp:434] loss <- fc5_classes_fc5_67_0_split_0
I0411 00:56:49.096638 16553 net.cpp:434] loss <- label_data_1_split_0
I0411 00:56:49.096645 16553 net.cpp:408] loss -> loss
I0411 00:56:49.096659 16553 layer_factory.hpp:77] Creating layer loss
I0411 00:56:49.097075 16553 net.cpp:150] Setting up loss
I0411 00:56:49.097090 16553 net.cpp:157] Top shape: (1)
I0411 00:56:49.097095 16553 net.cpp:160]     with loss weight 1
I0411 00:56:49.097107 16553 net.cpp:165] Memory required for data: 4107812868
I0411 00:56:49.097112 16553 layer_factory.hpp:77] Creating layer accuracy_1
I0411 00:56:49.097123 16553 net.cpp:100] Creating Layer accuracy_1
I0411 00:56:49.097129 16553 net.cpp:434] accuracy_1 <- fc5_classes_fc5_67_0_split_1
I0411 00:56:49.097136 16553 net.cpp:434] accuracy_1 <- label_data_1_split_1
I0411 00:56:49.097142 16553 net.cpp:408] accuracy_1 -> accuracy_1
I0411 00:56:49.097154 16553 net.cpp:150] Setting up accuracy_1
I0411 00:56:49.097162 16553 net.cpp:157] Top shape: (1)
I0411 00:56:49.097164 16553 net.cpp:165] Memory required for data: 4107812872
I0411 00:56:49.097168 16553 layer_factory.hpp:77] Creating layer accuracy_5
I0411 00:56:49.097177 16553 net.cpp:100] Creating Layer accuracy_5
I0411 00:56:49.097180 16553 net.cpp:434] accuracy_5 <- fc5_classes_fc5_67_0_split_2
I0411 00:56:49.097185 16553 net.cpp:434] accuracy_5 <- label_data_1_split_2
I0411 00:56:49.097192 16553 net.cpp:408] accuracy_5 -> accuracy_5
I0411 00:56:49.097200 16553 net.cpp:150] Setting up accuracy_5
I0411 00:56:49.097208 16553 net.cpp:157] Top shape: (1)
I0411 00:56:49.097213 16553 net.cpp:165] Memory required for data: 4107812876
I0411 00:56:49.097226 16553 net.cpp:228] accuracy_5 does not need backward computation.
I0411 00:56:49.097231 16553 net.cpp:228] accuracy_1 does not need backward computation.
I0411 00:56:49.097235 16553 net.cpp:226] loss needs backward computation.
I0411 00:56:49.097260 16553 net.cpp:226] fc5_classes_fc5_67_0_split needs backward computation.
I0411 00:56:49.097265 16553 net.cpp:226] fc5_67 needs backward computation.
I0411 00:56:49.097270 16553 net.cpp:226] fc4_postscale needs backward computation.
I0411 00:56:49.097275 16553 net.cpp:226] fc4_sTanH needs backward computation.
I0411 00:56:49.097277 16553 net.cpp:226] fc4_prescale needs backward computation.
I0411 00:56:49.097281 16553 net.cpp:226] fc4_300 needs backward computation.
I0411 00:56:49.097285 16553 net.cpp:226] pool3 needs backward computation.
I0411 00:56:49.097288 16553 net.cpp:226] conv3_postscale needs backward computation.
I0411 00:56:49.097292 16553 net.cpp:226] conv3_sTanH needs backward computation.
I0411 00:56:49.097296 16553 net.cpp:226] conv3_prescale needs backward computation.
I0411 00:56:49.097299 16553 net.cpp:226] conv3 needs backward computation.
I0411 00:56:49.097303 16553 net.cpp:226] pool2 needs backward computation.
I0411 00:56:49.097307 16553 net.cpp:226] conv2_postscale needs backward computation.
I0411 00:56:49.097311 16553 net.cpp:226] conv2_sTanH needs backward computation.
I0411 00:56:49.097316 16553 net.cpp:226] conv2_prescale needs backward computation.
I0411 00:56:49.097318 16553 net.cpp:226] conv2 needs backward computation.
I0411 00:56:49.097324 16553 net.cpp:226] pool1 needs backward computation.
I0411 00:56:49.097328 16553 net.cpp:226] conv1_postscale needs backward computation.
I0411 00:56:49.097332 16553 net.cpp:226] conv1_sTanH needs backward computation.
I0411 00:56:49.097335 16553 net.cpp:226] conv1_prescale needs backward computation.
I0411 00:56:49.097339 16553 net.cpp:226] conv1 needs backward computation.
I0411 00:56:49.097343 16553 net.cpp:228] label_data_1_split does not need backward computation.
I0411 00:56:49.097348 16553 net.cpp:228] data does not need backward computation.
I0411 00:56:49.097352 16553 net.cpp:270] This network produces output accuracy_1
I0411 00:56:49.097357 16553 net.cpp:270] This network produces output accuracy_5
I0411 00:56:49.097360 16553 net.cpp:270] This network produces output loss
I0411 00:56:49.097400 16553 net.cpp:283] Network initialization done.
I0411 00:56:49.097487 16553 solver.cpp:72] Solver scaffolding done.
I0411 00:56:49.098609 16553 caffe.cpp:251] Starting Optimization
I0411 00:56:49.098619 16553 solver.cpp:291] Solving 
I0411 00:56:49.098623 16553 solver.cpp:292] Learning Rate Policy: step
I0411 00:56:49.103124 16553 solver.cpp:349] Iteration 0, Testing net (#0)
I0411 00:56:49.105150 16553 blocking_queue.cpp:50] Data layer prefetch queue empty
I0411 00:56:50.212738 16553 solver.cpp:416]     Test net output #0: accuracy_1 = 0.00549316
I0411 00:56:50.212769 16553 solver.cpp:416]     Test net output #1: accuracy_5 = 0.0625
I0411 00:56:50.212779 16553 solver.cpp:416]     Test net output #2: loss = 4.84607 (* 1 = 4.84607 loss)
I0411 00:56:50.375643 16553 solver.cpp:240] Iteration 0, loss = 4.85129
I0411 00:56:50.375677 16553 solver.cpp:256]     Train net output #0: loss = 4.85129 (* 1 = 4.85129 loss)
I0411 00:56:50.375697 16553 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0411 00:56:50.744047 16553 solver.cpp:240] Iteration 1, loss = 4.53758
I0411 00:56:50.744081 16553 solver.cpp:256]     Train net output #0: loss = 4.53758 (* 1 = 4.53758 loss)
I0411 00:56:50.744089 16553 sgd_solver.cpp:106] Iteration 1, lr = 0.001
I0411 00:56:51.119909 16553 solver.cpp:240] Iteration 2, loss = 4.36791
I0411 00:56:51.119942 16553 solver.cpp:256]     Train net output #0: loss = 4.36791 (* 1 = 4.36791 loss)
I0411 00:56:51.119951 16553 sgd_solver.cpp:106] Iteration 2, lr = 0.001
I0411 00:56:51.498284 16553 solver.cpp:240] Iteration 3, loss = 5.1216
I0411 00:56:51.498319 16553 solver.cpp:256]     Train net output #0: loss = 5.1216 (* 1 = 5.1216 loss)
I0411 00:56:51.498327 16553 sgd_solver.cpp:106] Iteration 3, lr = 0.001
I0411 00:56:51.876214 16553 solver.cpp:240] Iteration 4, loss = 5.41434
I0411 00:56:51.876252 16553 solver.cpp:256]     Train net output #0: loss = 5.41434 (* 1 = 5.41434 loss)
I0411 00:56:51.876261 16553 sgd_solver.cpp:106] Iteration 4, lr = 0.001
I0411 00:56:52.248395 16553 solver.cpp:240] Iteration 5, loss = 5.35222
I0411 00:56:52.248430 16553 solver.cpp:256]     Train net output #0: loss = 5.35222 (* 1 = 5.35222 loss)
I0411 00:56:52.248438 16553 sgd_solver.cpp:106] Iteration 5, lr = 0.001
I0411 00:56:52.624770 16553 solver.cpp:240] Iteration 6, loss = 4.81377
I0411 00:56:52.624804 16553 solver.cpp:256]     Train net output #0: loss = 4.81377 (* 1 = 4.81377 loss)
I0411 00:56:52.624812 16553 sgd_solver.cpp:106] Iteration 6, lr = 0.001
I0411 00:56:53.006067 16553 solver.cpp:240] Iteration 7, loss = 4.90174
I0411 00:56:53.006099 16553 solver.cpp:256]     Train net output #0: loss = 4.90174 (* 1 = 4.90174 loss)
I0411 00:56:53.006108 16553 sgd_solver.cpp:106] Iteration 7, lr = 0.001
I0411 00:56:53.383707 16553 solver.cpp:240] Iteration 8, loss = 4.84668
I0411 00:56:53.383750 16553 solver.cpp:256]     Train net output #0: loss = 4.84668 (* 1 = 4.84668 loss)
I0411 00:56:53.383759 16553 sgd_solver.cpp:106] Iteration 8, lr = 0.001
I0411 00:56:53.757571 16553 solver.cpp:240] Iteration 9, loss = 5.0915
I0411 00:56:53.757606 16553 solver.cpp:256]     Train net output #0: loss = 5.0915 (* 1 = 5.0915 loss)
I0411 00:56:53.757614 16553 sgd_solver.cpp:106] Iteration 9, lr = 0.001
I0411 00:56:54.133114 16553 solver.cpp:240] Iteration 10, loss = 4.75927
I0411 00:56:54.133158 16553 solver.cpp:256]     Train net output #0: loss = 4.75927 (* 1 = 4.75927 loss)
I0411 00:56:54.133168 16553 sgd_solver.cpp:106] Iteration 10, lr = 0.001
I0411 00:56:54.505750 16553 solver.cpp:240] Iteration 11, loss = 4.57621
I0411 00:56:54.505785 16553 solver.cpp:256]     Train net output #0: loss = 4.57621 (* 1 = 4.57621 loss)
I0411 00:56:54.505794 16553 sgd_solver.cpp:106] Iteration 11, lr = 0.001
I0411 00:56:54.881224 16553 solver.cpp:240] Iteration 12, loss = 4.52922
I0411 00:56:54.881264 16553 solver.cpp:256]     Train net output #0: loss = 4.52922 (* 1 = 4.52922 loss)
I0411 00:56:54.881276 16553 sgd_solver.cpp:106] Iteration 12, lr = 0.001
I0411 00:56:55.256391 16553 solver.cpp:240] Iteration 13, loss = 4.39223
I0411 00:56:55.256423 16553 solver.cpp:256]     Train net output #0: loss = 4.39223 (* 1 = 4.39223 loss)
I0411 00:56:55.256431 16553 sgd_solver.cpp:106] Iteration 13, lr = 0.001
I0411 00:56:55.632405 16553 solver.cpp:240] Iteration 14, loss = 4.48096
I0411 00:56:55.632437 16553 solver.cpp:256]     Train net output #0: loss = 4.48096 (* 1 = 4.48096 loss)
I0411 00:56:55.632447 16553 sgd_solver.cpp:106] Iteration 14, lr = 0.001
I0411 00:56:56.004267 16553 solver.cpp:240] Iteration 15, loss = 4.81251
I0411 00:56:56.004302 16553 solver.cpp:256]     Train net output #0: loss = 4.81251 (* 1 = 4.81251 loss)
I0411 00:56:56.004310 16553 sgd_solver.cpp:106] Iteration 15, lr = 0.001
I0411 00:56:56.381201 16553 solver.cpp:240] Iteration 16, loss = 5.22046
I0411 00:56:56.381235 16553 solver.cpp:256]     Train net output #0: loss = 5.22046 (* 1 = 5.22046 loss)
I0411 00:56:56.381243 16553 sgd_solver.cpp:106] Iteration 16, lr = 0.001
I0411 00:56:56.757699 16553 solver.cpp:240] Iteration 17, loss = 5.43888
I0411 00:56:56.757732 16553 solver.cpp:256]     Train net output #0: loss = 5.43888 (* 1 = 5.43888 loss)
I0411 00:56:56.757741 16553 sgd_solver.cpp:106] Iteration 17, lr = 0.001
I0411 00:56:57.131122 16553 solver.cpp:240] Iteration 18, loss = 5.40244
I0411 00:56:57.131156 16553 solver.cpp:256]     Train net output #0: loss = 5.40244 (* 1 = 5.40244 loss)
I0411 00:56:57.131165 16553 sgd_solver.cpp:106] Iteration 18, lr = 0.001
I0411 00:56:57.503799 16553 solver.cpp:240] Iteration 19, loss = 5.41924
I0411 00:56:57.503834 16553 solver.cpp:256]     Train net output #0: loss = 5.41924 (* 1 = 5.41924 loss)
I0411 00:56:57.503842 16553 sgd_solver.cpp:106] Iteration 19, lr = 0.001
I0411 00:56:57.883113 16553 solver.cpp:240] Iteration 20, loss = 5.56798
I0411 00:56:57.883149 16553 solver.cpp:256]     Train net output #0: loss = 5.56798 (* 1 = 5.56798 loss)
I0411 00:56:57.883158 16553 sgd_solver.cpp:106] Iteration 20, lr = 0.001
I0411 00:56:58.259141 16553 solver.cpp:240] Iteration 21, loss = 5.06037
I0411 00:56:58.259198 16553 solver.cpp:256]     Train net output #0: loss = 5.06037 (* 1 = 5.06037 loss)
I0411 00:56:58.259208 16553 sgd_solver.cpp:106] Iteration 21, lr = 0.001
I0411 00:56:58.631389 16553 solver.cpp:240] Iteration 22, loss = 4.69944
I0411 00:56:58.631422 16553 solver.cpp:256]     Train net output #0: loss = 4.69944 (* 1 = 4.69944 loss)
I0411 00:56:58.631431 16553 sgd_solver.cpp:106] Iteration 22, lr = 0.001
I0411 00:56:59.005901 16553 solver.cpp:240] Iteration 23, loss = 4.97687
I0411 00:56:59.005934 16553 solver.cpp:256]     Train net output #0: loss = 4.97687 (* 1 = 4.97687 loss)
I0411 00:56:59.005942 16553 sgd_solver.cpp:106] Iteration 23, lr = 0.001
I0411 00:56:59.383620 16553 solver.cpp:240] Iteration 24, loss = 5.27292
I0411 00:56:59.383653 16553 solver.cpp:256]     Train net output #0: loss = 5.27292 (* 1 = 5.27292 loss)
I0411 00:56:59.383661 16553 sgd_solver.cpp:106] Iteration 24, lr = 0.001
I0411 00:56:59.383991 16553 solver.cpp:349] Iteration 25, Testing net (#0)
I0411 00:57:00.682245 16553 solver.cpp:416]     Test net output #0: accuracy_1 = 0.0145264
I0411 00:57:00.682273 16553 solver.cpp:416]     Test net output #1: accuracy_5 = 0.0675049
I0411 00:57:00.682282 16553 solver.cpp:416]     Test net output #2: loss = 5.02607 (* 1 = 5.02607 loss)
I0411 00:57:00.812057 16553 solver.cpp:240] Iteration 25, loss = 5.30761
I0411 00:57:00.812090 16553 solver.cpp:256]     Train net output #0: loss = 5.30761 (* 1 = 5.30761 loss)
I0411 00:57:00.812099 16553 sgd_solver.cpp:106] Iteration 25, lr = 0.001
I0411 00:57:01.191660 16553 solver.cpp:240] Iteration 26, loss = 5.12552
I0411 00:57:01.191694 16553 solver.cpp:256]     Train net output #0: loss = 5.12552 (* 1 = 5.12552 loss)
I0411 00:57:01.191701 16553 sgd_solver.cpp:106] Iteration 26, lr = 0.001
I0411 00:57:01.558985 16553 solver.cpp:240] Iteration 27, loss = 4.95343
I0411 00:57:01.559020 16553 solver.cpp:256]     Train net output #0: loss = 4.95343 (* 1 = 4.95343 loss)
I0411 00:57:01.559027 16553 sgd_solver.cpp:106] Iteration 27, lr = 0.001
I0411 00:57:01.936241 16553 solver.cpp:240] Iteration 28, loss = 4.73733
I0411 00:57:01.936275 16553 solver.cpp:256]     Train net output #0: loss = 4.73733 (* 1 = 4.73733 loss)
I0411 00:57:01.936283 16553 sgd_solver.cpp:106] Iteration 28, lr = 0.001
I0411 00:57:02.310304 16553 solver.cpp:240] Iteration 29, loss = 4.6724
I0411 00:57:02.310338 16553 solver.cpp:256]     Train net output #0: loss = 4.6724 (* 1 = 4.6724 loss)
I0411 00:57:02.310346 16553 sgd_solver.cpp:106] Iteration 29, lr = 0.001
I0411 00:57:02.682286 16553 solver.cpp:240] Iteration 30, loss = 4.61732
I0411 00:57:02.682322 16553 solver.cpp:256]     Train net output #0: loss = 4.61732 (* 1 = 4.61732 loss)
I0411 00:57:02.682329 16553 sgd_solver.cpp:106] Iteration 30, lr = 0.001
I0411 00:57:03.061281 16553 solver.cpp:240] Iteration 31, loss = 4.78148
I0411 00:57:03.061314 16553 solver.cpp:256]     Train net output #0: loss = 4.78148 (* 1 = 4.78148 loss)
I0411 00:57:03.061322 16553 sgd_solver.cpp:106] Iteration 31, lr = 0.001
I0411 00:57:03.439858 16553 solver.cpp:240] Iteration 32, loss = 4.80098
I0411 00:57:03.439904 16553 solver.cpp:256]     Train net output #0: loss = 4.80098 (* 1 = 4.80098 loss)
I0411 00:57:03.439913 16553 sgd_solver.cpp:106] Iteration 32, lr = 0.001
I0411 00:57:03.813745 16553 solver.cpp:240] Iteration 33, loss = 4.8043
I0411 00:57:03.813781 16553 solver.cpp:256]     Train net output #0: loss = 4.8043 (* 1 = 4.8043 loss)
I0411 00:57:03.813789 16553 sgd_solver.cpp:106] Iteration 33, lr = 0.001
I0411 00:57:04.183615 16553 solver.cpp:240] Iteration 34, loss = 5.23925
I0411 00:57:04.183648 16553 solver.cpp:256]     Train net output #0: loss = 5.23925 (* 1 = 5.23925 loss)
I0411 00:57:04.183656 16553 sgd_solver.cpp:106] Iteration 34, lr = 0.001
I0411 00:57:04.565280 16553 solver.cpp:240] Iteration 35, loss = 4.98218
I0411 00:57:04.565313 16553 solver.cpp:256]     Train net output #0: loss = 4.98218 (* 1 = 4.98218 loss)
I0411 00:57:04.565321 16553 sgd_solver.cpp:106] Iteration 35, lr = 0.001
I0411 00:57:04.942101 16553 solver.cpp:240] Iteration 36, loss = 5.00175
I0411 00:57:04.942170 16553 solver.cpp:256]     Train net output #0: loss = 5.00175 (* 1 = 5.00175 loss)
I0411 00:57:04.942179 16553 sgd_solver.cpp:106] Iteration 36, lr = 0.001
I0411 00:57:05.313100 16553 solver.cpp:240] Iteration 37, loss = 5.29351
I0411 00:57:05.313136 16553 solver.cpp:256]     Train net output #0: loss = 5.29351 (* 1 = 5.29351 loss)
I0411 00:57:05.313144 16553 sgd_solver.cpp:106] Iteration 37, lr = 0.001
I0411 00:57:05.687696 16553 solver.cpp:240] Iteration 38, loss = 5.18983
I0411 00:57:05.687729 16553 solver.cpp:256]     Train net output #0: loss = 5.18983 (* 1 = 5.18983 loss)
I0411 00:57:05.687737 16553 sgd_solver.cpp:106] Iteration 38, lr = 0.001
I0411 00:57:06.066264 16553 solver.cpp:240] Iteration 39, loss = 5.62633
I0411 00:57:06.066296 16553 solver.cpp:256]     Train net output #0: loss = 5.62633 (* 1 = 5.62633 loss)
I0411 00:57:06.066304 16553 sgd_solver.cpp:106] Iteration 39, lr = 0.001
I0411 00:57:06.442489 16553 solver.cpp:240] Iteration 40, loss = 5.85062
I0411 00:57:06.442523 16553 solver.cpp:256]     Train net output #0: loss = 5.85062 (* 1 = 5.85062 loss)
I0411 00:57:06.442531 16553 sgd_solver.cpp:106] Iteration 40, lr = 0.001
I0411 00:57:06.816138 16553 solver.cpp:240] Iteration 41, loss = 5.30356
I0411 00:57:06.816174 16553 solver.cpp:256]     Train net output #0: loss = 5.30356 (* 1 = 5.30356 loss)
I0411 00:57:06.816181 16553 sgd_solver.cpp:106] Iteration 41, lr = 0.001
I0411 00:57:07.190552 16553 solver.cpp:240] Iteration 42, loss = 5.38589
I0411 00:57:07.190595 16553 solver.cpp:256]     Train net output #0: loss = 5.38589 (* 1 = 5.38589 loss)
I0411 00:57:07.190603 16553 sgd_solver.cpp:106] Iteration 42, lr = 0.001
I0411 00:57:07.563591 16553 solver.cpp:240] Iteration 43, loss = 5.49637
I0411 00:57:07.563622 16553 solver.cpp:256]     Train net output #0: loss = 5.49637 (* 1 = 5.49637 loss)
I0411 00:57:07.563632 16553 sgd_solver.cpp:106] Iteration 43, lr = 0.001
I0411 00:57:07.938655 16553 solver.cpp:240] Iteration 44, loss = 5.57521
I0411 00:57:07.938688 16553 solver.cpp:256]     Train net output #0: loss = 5.57521 (* 1 = 5.57521 loss)
I0411 00:57:07.938696 16553 sgd_solver.cpp:106] Iteration 44, lr = 0.001
I0411 00:57:08.314519 16553 solver.cpp:240] Iteration 45, loss = 5.60667
I0411 00:57:08.314551 16553 solver.cpp:256]     Train net output #0: loss = 5.60667 (* 1 = 5.60667 loss)
I0411 00:57:08.314560 16553 sgd_solver.cpp:106] Iteration 45, lr = 0.001
I0411 00:57:08.688810 16553 solver.cpp:240] Iteration 46, loss = 5.02395
I0411 00:57:08.688843 16553 solver.cpp:256]     Train net output #0: loss = 5.02395 (* 1 = 5.02395 loss)
I0411 00:57:08.688851 16553 sgd_solver.cpp:106] Iteration 46, lr = 0.001
I0411 00:57:09.060972 16553 solver.cpp:240] Iteration 47, loss = 4.88368
I0411 00:57:09.061007 16553 solver.cpp:256]     Train net output #0: loss = 4.88368 (* 1 = 4.88368 loss)
I0411 00:57:09.061015 16553 sgd_solver.cpp:106] Iteration 47, lr = 0.001
I0411 00:57:09.440676 16553 solver.cpp:240] Iteration 48, loss = 4.89178
I0411 00:57:09.440709 16553 solver.cpp:256]     Train net output #0: loss = 4.89178 (* 1 = 4.89178 loss)
I0411 00:57:09.440717 16553 sgd_solver.cpp:106] Iteration 48, lr = 0.001
I0411 00:57:09.818071 16553 solver.cpp:240] Iteration 49, loss = 4.83226
I0411 00:57:09.818106 16553 solver.cpp:256]     Train net output #0: loss = 4.83226 (* 1 = 4.83226 loss)
I0411 00:57:09.818114 16553 sgd_solver.cpp:106] Iteration 49, lr = 0.001
I0411 00:57:09.818428 16553 solver.cpp:349] Iteration 50, Testing net (#0)
I0411 00:57:11.114226 16553 solver.cpp:416]     Test net output #0: accuracy_1 = 0.0106201
I0411 00:57:11.114253 16553 solver.cpp:416]     Test net output #1: accuracy_5 = 0.184448
I0411 00:57:11.114264 16553 solver.cpp:416]     Test net output #2: loss = 5.73943 (* 1 = 5.73943 loss)
I0411 00:57:11.242971 16553 solver.cpp:240] Iteration 50, loss = 5.01552
I0411 00:57:11.243010 16553 solver.cpp:256]     Train net output #0: loss = 5.01552 (* 1 = 5.01552 loss)
I0411 00:57:11.243018 16553 sgd_solver.cpp:106] Iteration 50, lr = 0.001
I0411 00:57:11.618880 16553 solver.cpp:240] Iteration 51, loss = 5.29589
I0411 00:57:11.618916 16553 solver.cpp:256]     Train net output #0: loss = 5.29589 (* 1 = 5.29589 loss)
I0411 00:57:11.618923 16553 sgd_solver.cpp:106] Iteration 51, lr = 0.001
I0411 00:57:11.994405 16553 solver.cpp:240] Iteration 52, loss = 5.46377
I0411 00:57:11.994437 16553 solver.cpp:256]     Train net output #0: loss = 5.46377 (* 1 = 5.46377 loss)
I0411 00:57:11.994446 16553 sgd_solver.cpp:106] Iteration 52, lr = 0.001
I0411 00:57:12.367277 16553 solver.cpp:240] Iteration 53, loss = 5.85652
I0411 00:57:12.367311 16553 solver.cpp:256]     Train net output #0: loss = 5.85652 (* 1 = 5.85652 loss)
I0411 00:57:12.367321 16553 sgd_solver.cpp:106] Iteration 53, lr = 0.001
I0411 00:57:12.743041 16553 solver.cpp:240] Iteration 54, loss = 6.60507
I0411 00:57:12.743083 16553 solver.cpp:256]     Train net output #0: loss = 6.60507 (* 1 = 6.60507 loss)
I0411 00:57:12.743091 16553 sgd_solver.cpp:106] Iteration 54, lr = 0.001
I0411 00:57:13.113397 16553 solver.cpp:240] Iteration 55, loss = 7.47923
I0411 00:57:13.113430 16553 solver.cpp:256]     Train net output #0: loss = 7.47923 (* 1 = 7.47923 loss)
I0411 00:57:13.113438 16553 sgd_solver.cpp:106] Iteration 55, lr = 0.001
I0411 00:57:13.491755 16553 solver.cpp:240] Iteration 56, loss = 8.43365
I0411 00:57:13.491791 16553 solver.cpp:256]     Train net output #0: loss = 8.43365 (* 1 = 8.43365 loss)
I0411 00:57:13.491801 16553 sgd_solver.cpp:106] Iteration 56, lr = 0.001
I0411 00:57:13.867959 16553 solver.cpp:240] Iteration 57, loss = 9.643
I0411 00:57:13.867993 16553 solver.cpp:256]     Train net output #0: loss = 9.643 (* 1 = 9.643 loss)
I0411 00:57:13.868008 16553 sgd_solver.cpp:106] Iteration 57, lr = 0.001
I0411 00:57:14.242323 16553 solver.cpp:240] Iteration 58, loss = 10.4015
I0411 00:57:14.242359 16553 solver.cpp:256]     Train net output #0: loss = 10.4015 (* 1 = 10.4015 loss)
I0411 00:57:14.242367 16553 sgd_solver.cpp:106] Iteration 58, lr = 0.001
I0411 00:57:14.618069 16553 solver.cpp:240] Iteration 59, loss = 11.6161
I0411 00:57:14.618116 16553 solver.cpp:256]     Train net output #0: loss = 11.6161 (* 1 = 11.6161 loss)
I0411 00:57:14.618125 16553 sgd_solver.cpp:106] Iteration 59, lr = 0.001
I0411 00:57:14.997648 16553 solver.cpp:240] Iteration 60, loss = 11.6565
I0411 00:57:14.997684 16553 solver.cpp:256]     Train net output #0: loss = 11.6565 (* 1 = 11.6565 loss)
I0411 00:57:14.997691 16553 sgd_solver.cpp:106] Iteration 60, lr = 0.001
I0411 00:57:15.373620 16553 solver.cpp:240] Iteration 61, loss = 11.7987
I0411 00:57:15.373656 16553 solver.cpp:256]     Train net output #0: loss = 11.7987 (* 1 = 11.7987 loss)
I0411 00:57:15.373664 16553 sgd_solver.cpp:106] Iteration 61, lr = 0.001
I0411 00:57:15.747087 16553 solver.cpp:240] Iteration 62, loss = 11.0576
I0411 00:57:15.747123 16553 solver.cpp:256]     Train net output #0: loss = 11.0576 (* 1 = 11.0576 loss)
I0411 00:57:15.747131 16553 sgd_solver.cpp:106] Iteration 62, lr = 0.001
I0411 00:57:16.122869 16553 solver.cpp:240] Iteration 63, loss = 9.96891
I0411 00:57:16.122902 16553 solver.cpp:256]     Train net output #0: loss = 9.96891 (* 1 = 9.96891 loss)
I0411 00:57:16.122910 16553 sgd_solver.cpp:106] Iteration 63, lr = 0.001
I0411 00:57:16.500375 16553 solver.cpp:240] Iteration 64, loss = 7.32144
I0411 00:57:16.500407 16553 solver.cpp:256]     Train net output #0: loss = 7.32144 (* 1 = 7.32144 loss)
I0411 00:57:16.500416 16553 sgd_solver.cpp:106] Iteration 64, lr = 0.001
I0411 00:57:16.876204 16553 solver.cpp:240] Iteration 65, loss = 6.07007
I0411 00:57:16.876238 16553 solver.cpp:256]     Train net output #0: loss = 6.07007 (* 1 = 6.07007 loss)
I0411 00:57:16.876246 16553 sgd_solver.cpp:106] Iteration 65, lr = 0.001
I0411 00:57:17.251721 16553 solver.cpp:240] Iteration 66, loss = 7.74011
I0411 00:57:17.251755 16553 solver.cpp:256]     Train net output #0: loss = 7.74011 (* 1 = 7.74011 loss)
I0411 00:57:17.251765 16553 sgd_solver.cpp:106] Iteration 66, lr = 0.001
I0411 00:57:17.625907 16553 solver.cpp:240] Iteration 67, loss = 8.62666
I0411 00:57:17.626090 16553 solver.cpp:256]     Train net output #0: loss = 8.62666 (* 1 = 8.62666 loss)
I0411 00:57:17.626101 16553 sgd_solver.cpp:106] Iteration 67, lr = 0.001
I0411 00:57:18.005472 16553 solver.cpp:240] Iteration 68, loss = 9.41071
I0411 00:57:18.005508 16553 solver.cpp:256]     Train net output #0: loss = 9.41071 (* 1 = 9.41071 loss)
I0411 00:57:18.005517 16553 sgd_solver.cpp:106] Iteration 68, lr = 0.001
I0411 00:57:18.381973 16553 solver.cpp:240] Iteration 69, loss = 10.1915
I0411 00:57:18.382007 16553 solver.cpp:256]     Train net output #0: loss = 10.1915 (* 1 = 10.1915 loss)
I0411 00:57:18.382016 16553 sgd_solver.cpp:106] Iteration 69, lr = 0.001
I0411 00:57:18.757690 16553 solver.cpp:240] Iteration 70, loss = 10.5425
I0411 00:57:18.757725 16553 solver.cpp:256]     Train net output #0: loss = 10.5425 (* 1 = 10.5425 loss)
I0411 00:57:18.757733 16553 sgd_solver.cpp:106] Iteration 70, lr = 0.001
I0411 00:57:19.135713 16553 solver.cpp:240] Iteration 71, loss = 9.36701
I0411 00:57:19.135754 16553 solver.cpp:256]     Train net output #0: loss = 9.36701 (* 1 = 9.36701 loss)
I0411 00:57:19.135762 16553 sgd_solver.cpp:106] Iteration 71, lr = 0.001
