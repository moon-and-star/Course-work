I0411 00:38:10.107625  9160 caffe.cpp:217] Using GPUs 1
I0411 00:38:10.460961  9160 caffe.cpp:222] GPU 1: GeForce GTX 1070
I0411 00:38:11.241245  9160 solver.cpp:60] Initializing solver from parameters: 
train_net: "./Prototxt/experiment_8/rtsd-r1/CoNorm/trial_1/train.prototxt"
test_net: "./Prototxt/experiment_8/rtsd-r1/CoNorm/trial_1/test.prototxt"
test_iter: 8
test_interval: 25
base_lr: 1e-05
display: 1
max_iter: 2500
lr_policy: "step"
gamma: 0.5
momentum: 0.9
weight_decay: 0.0005
stepsize: 500
snapshot: 250
snapshot_prefix: "./snapshots/experiment_8/rtsd-r1/CoNorm/trial_1/snap"
solver_mode: GPU
device_id: 1
train_state {
  level: 0
  stage: ""
}
iter_size: 1
type: "Adam"
I0411 00:38:11.241402  9160 solver.cpp:93] Creating training net from train_net file: ./Prototxt/experiment_8/rtsd-r1/CoNorm/trial_1/train.prototxt
I0411 00:38:11.241751  9160 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_1
I0411 00:38:11.241765  9160 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_5
I0411 00:38:11.241937  9160 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: false
    crop_size: 48
    mean_value: 132
    mean_value: 132
    mean_value: 131
  }
  data_param {
    source: "../local_data/lmdb/rtsd-r1/CoNorm/train/lmdb"
    batch_size: 1024
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 7
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_prescale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
  }
}
layer {
  name: "conv1_sTanH"
  type: "TanH"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_postscale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 150
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_prescale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
  }
}
layer {
  name: "conv2_sTanH"
  type: "TanH"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_postscale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 250
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_prescale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
  }
}
layer {
  name: "conv3_sTanH"
  type: "TanH"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_postscale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc4_300"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc4_300"
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fc4_prescale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "fc4_sTanH"
  type: "TanH"
  bottom: "fc4_300"
  top: "fc4_300"
}
layer {
  name: "fc4_postscale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "fc5_67"
  type: "InnerProduct"
  bottom: "fc4_300"
  top: "fc5_classes"
  inner_product_param {
    num_output: 67
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc5_classes"
  bottom: "label"
  top: "loss"
}
I0411 00:38:11.242065  9160 layer_factory.hpp:77] Creating layer data
I0411 00:38:11.243363  9160 net.cpp:100] Creating Layer data
I0411 00:38:11.243381  9160 net.cpp:408] data -> data
I0411 00:38:11.243408  9160 net.cpp:408] data -> label
I0411 00:38:11.246991  9265 db_lmdb.cpp:35] Opened lmdb ../local_data/lmdb/rtsd-r1/CoNorm/train/lmdb
I0411 00:38:11.267009  9160 data_layer.cpp:41] output data size: 1024,3,48,48
I0411 00:38:11.326414  9160 net.cpp:150] Setting up data
I0411 00:38:11.326458  9160 net.cpp:157] Top shape: 1024 3 48 48 (7077888)
I0411 00:38:11.326465  9160 net.cpp:157] Top shape: 1024 (1024)
I0411 00:38:11.326469  9160 net.cpp:165] Memory required for data: 28315648
I0411 00:38:11.326483  9160 layer_factory.hpp:77] Creating layer conv1
I0411 00:38:11.326508  9160 net.cpp:100] Creating Layer conv1
I0411 00:38:11.326515  9160 net.cpp:434] conv1 <- data
I0411 00:38:11.326530  9160 net.cpp:408] conv1 -> conv1
I0411 00:38:11.668395  9160 net.cpp:150] Setting up conv1
I0411 00:38:11.668427  9160 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:38:11.668432  9160 net.cpp:165] Memory required for data: 750850048
I0411 00:38:11.668457  9160 layer_factory.hpp:77] Creating layer conv1_prescale
I0411 00:38:11.668475  9160 net.cpp:100] Creating Layer conv1_prescale
I0411 00:38:11.668483  9160 net.cpp:434] conv1_prescale <- conv1
I0411 00:38:11.668490  9160 net.cpp:395] conv1_prescale -> conv1 (in-place)
I0411 00:38:11.668620  9160 net.cpp:150] Setting up conv1_prescale
I0411 00:38:11.668632  9160 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:38:11.668637  9160 net.cpp:165] Memory required for data: 1473384448
I0411 00:38:11.668644  9160 layer_factory.hpp:77] Creating layer conv1_sTanH
I0411 00:38:11.668653  9160 net.cpp:100] Creating Layer conv1_sTanH
I0411 00:38:11.668658  9160 net.cpp:434] conv1_sTanH <- conv1
I0411 00:38:11.668664  9160 net.cpp:395] conv1_sTanH -> conv1 (in-place)
I0411 00:38:11.668895  9160 net.cpp:150] Setting up conv1_sTanH
I0411 00:38:11.668908  9160 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:38:11.668913  9160 net.cpp:165] Memory required for data: 2195918848
I0411 00:38:11.668918  9160 layer_factory.hpp:77] Creating layer conv1_postscale
I0411 00:38:11.668927  9160 net.cpp:100] Creating Layer conv1_postscale
I0411 00:38:11.668933  9160 net.cpp:434] conv1_postscale <- conv1
I0411 00:38:11.668939  9160 net.cpp:395] conv1_postscale -> conv1 (in-place)
I0411 00:38:11.669054  9160 net.cpp:150] Setting up conv1_postscale
I0411 00:38:11.669062  9160 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:38:11.669066  9160 net.cpp:165] Memory required for data: 2918453248
I0411 00:38:11.669072  9160 layer_factory.hpp:77] Creating layer pool1
I0411 00:38:11.669081  9160 net.cpp:100] Creating Layer pool1
I0411 00:38:11.669086  9160 net.cpp:434] pool1 <- conv1
I0411 00:38:11.669092  9160 net.cpp:408] pool1 -> pool1
I0411 00:38:11.669147  9160 net.cpp:150] Setting up pool1
I0411 00:38:11.669155  9160 net.cpp:157] Top shape: 1024 100 21 21 (45158400)
I0411 00:38:11.669160  9160 net.cpp:165] Memory required for data: 3099086848
I0411 00:38:11.669164  9160 layer_factory.hpp:77] Creating layer conv2
I0411 00:38:11.669196  9160 net.cpp:100] Creating Layer conv2
I0411 00:38:11.669203  9160 net.cpp:434] conv2 <- pool1
I0411 00:38:11.669209  9160 net.cpp:408] conv2 -> conv2
I0411 00:38:11.675487  9160 net.cpp:150] Setting up conv2
I0411 00:38:11.675509  9160 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:38:11.675515  9160 net.cpp:165] Memory required for data: 3298152448
I0411 00:38:11.675528  9160 layer_factory.hpp:77] Creating layer conv2_prescale
I0411 00:38:11.675539  9160 net.cpp:100] Creating Layer conv2_prescale
I0411 00:38:11.675544  9160 net.cpp:434] conv2_prescale <- conv2
I0411 00:38:11.675554  9160 net.cpp:395] conv2_prescale -> conv2 (in-place)
I0411 00:38:11.675689  9160 net.cpp:150] Setting up conv2_prescale
I0411 00:38:11.675700  9160 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:38:11.675705  9160 net.cpp:165] Memory required for data: 3497218048
I0411 00:38:11.675711  9160 layer_factory.hpp:77] Creating layer conv2_sTanH
I0411 00:38:11.675721  9160 net.cpp:100] Creating Layer conv2_sTanH
I0411 00:38:11.675726  9160 net.cpp:434] conv2_sTanH <- conv2
I0411 00:38:11.675732  9160 net.cpp:395] conv2_sTanH -> conv2 (in-place)
I0411 00:38:11.678247  9160 net.cpp:150] Setting up conv2_sTanH
I0411 00:38:11.678267  9160 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:38:11.678272  9160 net.cpp:165] Memory required for data: 3696283648
I0411 00:38:11.678277  9160 layer_factory.hpp:77] Creating layer conv2_postscale
I0411 00:38:11.678287  9160 net.cpp:100] Creating Layer conv2_postscale
I0411 00:38:11.678292  9160 net.cpp:434] conv2_postscale <- conv2
I0411 00:38:11.678299  9160 net.cpp:395] conv2_postscale -> conv2 (in-place)
I0411 00:38:11.678419  9160 net.cpp:150] Setting up conv2_postscale
I0411 00:38:11.678429  9160 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:38:11.678432  9160 net.cpp:165] Memory required for data: 3895349248
I0411 00:38:11.678439  9160 layer_factory.hpp:77] Creating layer pool2
I0411 00:38:11.678450  9160 net.cpp:100] Creating Layer pool2
I0411 00:38:11.678457  9160 net.cpp:434] pool2 <- conv2
I0411 00:38:11.678462  9160 net.cpp:408] pool2 -> pool2
I0411 00:38:11.678511  9160 net.cpp:150] Setting up pool2
I0411 00:38:11.678519  9160 net.cpp:157] Top shape: 1024 150 9 9 (12441600)
I0411 00:38:11.678524  9160 net.cpp:165] Memory required for data: 3945115648
I0411 00:38:11.678527  9160 layer_factory.hpp:77] Creating layer conv3
I0411 00:38:11.678539  9160 net.cpp:100] Creating Layer conv3
I0411 00:38:11.678544  9160 net.cpp:434] conv3 <- pool2
I0411 00:38:11.678550  9160 net.cpp:408] conv3 -> conv3
I0411 00:38:11.684795  9160 net.cpp:150] Setting up conv3
I0411 00:38:11.684815  9160 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:38:11.684819  9160 net.cpp:165] Memory required for data: 3981979648
I0411 00:38:11.684833  9160 layer_factory.hpp:77] Creating layer conv3_prescale
I0411 00:38:11.684842  9160 net.cpp:100] Creating Layer conv3_prescale
I0411 00:38:11.684847  9160 net.cpp:434] conv3_prescale <- conv3
I0411 00:38:11.684856  9160 net.cpp:395] conv3_prescale -> conv3 (in-place)
I0411 00:38:11.684953  9160 net.cpp:150] Setting up conv3_prescale
I0411 00:38:11.684962  9160 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:38:11.684965  9160 net.cpp:165] Memory required for data: 4018843648
I0411 00:38:11.684969  9160 layer_factory.hpp:77] Creating layer conv3_sTanH
I0411 00:38:11.684976  9160 net.cpp:100] Creating Layer conv3_sTanH
I0411 00:38:11.684980  9160 net.cpp:434] conv3_sTanH <- conv3
I0411 00:38:11.684984  9160 net.cpp:395] conv3_sTanH -> conv3 (in-place)
I0411 00:38:11.689944  9160 net.cpp:150] Setting up conv3_sTanH
I0411 00:38:11.689980  9160 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:38:11.689986  9160 net.cpp:165] Memory required for data: 4055707648
I0411 00:38:11.689995  9160 layer_factory.hpp:77] Creating layer conv3_postscale
I0411 00:38:11.690008  9160 net.cpp:100] Creating Layer conv3_postscale
I0411 00:38:11.690016  9160 net.cpp:434] conv3_postscale <- conv3
I0411 00:38:11.690032  9160 net.cpp:395] conv3_postscale -> conv3 (in-place)
I0411 00:38:11.690281  9160 net.cpp:150] Setting up conv3_postscale
I0411 00:38:11.690300  9160 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:38:11.690309  9160 net.cpp:165] Memory required for data: 4092571648
I0411 00:38:11.690320  9160 layer_factory.hpp:77] Creating layer pool3
I0411 00:38:11.690333  9160 net.cpp:100] Creating Layer pool3
I0411 00:38:11.690341  9160 net.cpp:434] pool3 <- conv3
I0411 00:38:11.690351  9160 net.cpp:408] pool3 -> pool3
I0411 00:38:11.690433  9160 net.cpp:150] Setting up pool3
I0411 00:38:11.690446  9160 net.cpp:157] Top shape: 1024 250 3 3 (2304000)
I0411 00:38:11.690454  9160 net.cpp:165] Memory required for data: 4101787648
I0411 00:38:11.690460  9160 layer_factory.hpp:77] Creating layer fc4_300
I0411 00:38:11.690482  9160 net.cpp:100] Creating Layer fc4_300
I0411 00:38:11.690491  9160 net.cpp:434] fc4_300 <- pool3
I0411 00:38:11.690501  9160 net.cpp:408] fc4_300 -> fc4_300
I0411 00:38:11.696877  9160 net.cpp:150] Setting up fc4_300
I0411 00:38:11.696894  9160 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:38:11.696899  9160 net.cpp:165] Memory required for data: 4103016448
I0411 00:38:11.696907  9160 layer_factory.hpp:77] Creating layer fc4_prescale
I0411 00:38:11.696918  9160 net.cpp:100] Creating Layer fc4_prescale
I0411 00:38:11.696923  9160 net.cpp:434] fc4_prescale <- fc4_300
I0411 00:38:11.696929  9160 net.cpp:395] fc4_prescale -> fc4_300 (in-place)
I0411 00:38:11.697016  9160 net.cpp:150] Setting up fc4_prescale
I0411 00:38:11.697024  9160 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:38:11.697027  9160 net.cpp:165] Memory required for data: 4104245248
I0411 00:38:11.697032  9160 layer_factory.hpp:77] Creating layer fc4_sTanH
I0411 00:38:11.697038  9160 net.cpp:100] Creating Layer fc4_sTanH
I0411 00:38:11.697043  9160 net.cpp:434] fc4_sTanH <- fc4_300
I0411 00:38:11.697048  9160 net.cpp:395] fc4_sTanH -> fc4_300 (in-place)
I0411 00:38:11.697232  9160 net.cpp:150] Setting up fc4_sTanH
I0411 00:38:11.697244  9160 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:38:11.697247  9160 net.cpp:165] Memory required for data: 4105474048
I0411 00:38:11.697250  9160 layer_factory.hpp:77] Creating layer fc4_postscale
I0411 00:38:11.697258  9160 net.cpp:100] Creating Layer fc4_postscale
I0411 00:38:11.697263  9160 net.cpp:434] fc4_postscale <- fc4_300
I0411 00:38:11.697268  9160 net.cpp:395] fc4_postscale -> fc4_300 (in-place)
I0411 00:38:11.697366  9160 net.cpp:150] Setting up fc4_postscale
I0411 00:38:11.697374  9160 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:38:11.697377  9160 net.cpp:165] Memory required for data: 4106702848
I0411 00:38:11.697382  9160 layer_factory.hpp:77] Creating layer fc5_67
I0411 00:38:11.697389  9160 net.cpp:100] Creating Layer fc5_67
I0411 00:38:11.697393  9160 net.cpp:434] fc5_67 <- fc4_300
I0411 00:38:11.697398  9160 net.cpp:408] fc5_67 -> fc5_classes
I0411 00:38:11.706267  9160 net.cpp:150] Setting up fc5_67
I0411 00:38:11.706285  9160 net.cpp:157] Top shape: 1024 67 (68608)
I0411 00:38:11.706288  9160 net.cpp:165] Memory required for data: 4106977280
I0411 00:38:11.706300  9160 layer_factory.hpp:77] Creating layer loss
I0411 00:38:11.706307  9160 net.cpp:100] Creating Layer loss
I0411 00:38:11.706311  9160 net.cpp:434] loss <- fc5_classes
I0411 00:38:11.706315  9160 net.cpp:434] loss <- label
I0411 00:38:11.706322  9160 net.cpp:408] loss -> loss
I0411 00:38:11.706336  9160 layer_factory.hpp:77] Creating layer loss
I0411 00:38:11.706678  9160 net.cpp:150] Setting up loss
I0411 00:38:11.706691  9160 net.cpp:157] Top shape: (1)
I0411 00:38:11.706693  9160 net.cpp:160]     with loss weight 1
I0411 00:38:11.706714  9160 net.cpp:165] Memory required for data: 4106977284
I0411 00:38:11.706718  9160 net.cpp:226] loss needs backward computation.
I0411 00:38:11.706724  9160 net.cpp:226] fc5_67 needs backward computation.
I0411 00:38:11.706728  9160 net.cpp:226] fc4_postscale needs backward computation.
I0411 00:38:11.706732  9160 net.cpp:226] fc4_sTanH needs backward computation.
I0411 00:38:11.706748  9160 net.cpp:226] fc4_prescale needs backward computation.
I0411 00:38:11.706751  9160 net.cpp:226] fc4_300 needs backward computation.
I0411 00:38:11.706754  9160 net.cpp:226] pool3 needs backward computation.
I0411 00:38:11.706758  9160 net.cpp:226] conv3_postscale needs backward computation.
I0411 00:38:11.706760  9160 net.cpp:226] conv3_sTanH needs backward computation.
I0411 00:38:11.706763  9160 net.cpp:226] conv3_prescale needs backward computation.
I0411 00:38:11.706765  9160 net.cpp:226] conv3 needs backward computation.
I0411 00:38:11.706768  9160 net.cpp:226] pool2 needs backward computation.
I0411 00:38:11.706771  9160 net.cpp:226] conv2_postscale needs backward computation.
I0411 00:38:11.706774  9160 net.cpp:226] conv2_sTanH needs backward computation.
I0411 00:38:11.706778  9160 net.cpp:226] conv2_prescale needs backward computation.
I0411 00:38:11.706780  9160 net.cpp:226] conv2 needs backward computation.
I0411 00:38:11.706784  9160 net.cpp:226] pool1 needs backward computation.
I0411 00:38:11.706786  9160 net.cpp:226] conv1_postscale needs backward computation.
I0411 00:38:11.706789  9160 net.cpp:226] conv1_sTanH needs backward computation.
I0411 00:38:11.706791  9160 net.cpp:226] conv1_prescale needs backward computation.
I0411 00:38:11.706794  9160 net.cpp:226] conv1 needs backward computation.
I0411 00:38:11.706797  9160 net.cpp:228] data does not need backward computation.
I0411 00:38:11.706801  9160 net.cpp:270] This network produces output loss
I0411 00:38:11.706817  9160 net.cpp:283] Network initialization done.
I0411 00:38:11.707080  9160 solver.cpp:193] Creating test net (#0) specified by test_net file: ./Prototxt/experiment_8/rtsd-r1/CoNorm/trial_1/test.prototxt
I0411 00:38:11.707257  9160 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 48
    mean_value: 133
    mean_value: 133
    mean_value: 132
  }
  data_param {
    source: "../local_data/lmdb/rtsd-r1/CoNorm/test/lmdb"
    batch_size: 1024
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 7
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_prescale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
  }
}
layer {
  name: "conv1_sTanH"
  type: "TanH"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_postscale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 150
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_prescale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
  }
}
layer {
  name: "conv2_sTanH"
  type: "TanH"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_postscale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 250
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_prescale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
  }
}
layer {
  name: "conv3_sTanH"
  type: "TanH"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_postscale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc4_300"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc4_300"
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fc4_prescale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "fc4_sTanH"
  type: "TanH"
  bottom: "fc4_300"
  top: "fc4_300"
}
layer {
  name: "fc4_postscale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "fc5_67"
  type: "InnerProduct"
  bottom: "fc4_300"
  top: "fc5_classes"
  inner_product_param {
    num_output: 67
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc5_classes"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy_1"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_1"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 1
  }
}
layer {
  name: "accuracy_5"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0411 00:38:11.707376  9160 layer_factory.hpp:77] Creating layer data
I0411 00:38:11.708061  9160 net.cpp:100] Creating Layer data
I0411 00:38:11.708076  9160 net.cpp:408] data -> data
I0411 00:38:11.708086  9160 net.cpp:408] data -> label
I0411 00:38:11.713230  9311 db_lmdb.cpp:35] Opened lmdb ../local_data/lmdb/rtsd-r1/CoNorm/test/lmdb
I0411 00:38:11.713438  9160 data_layer.cpp:41] output data size: 1024,3,48,48
I0411 00:38:11.767969  9160 net.cpp:150] Setting up data
I0411 00:38:11.767997  9160 net.cpp:157] Top shape: 1024 3 48 48 (7077888)
I0411 00:38:11.768002  9160 net.cpp:157] Top shape: 1024 (1024)
I0411 00:38:11.768005  9160 net.cpp:165] Memory required for data: 28315648
I0411 00:38:11.768010  9160 layer_factory.hpp:77] Creating layer label_data_1_split
I0411 00:38:11.768025  9160 net.cpp:100] Creating Layer label_data_1_split
I0411 00:38:11.768029  9160 net.cpp:434] label_data_1_split <- label
I0411 00:38:11.768038  9160 net.cpp:408] label_data_1_split -> label_data_1_split_0
I0411 00:38:11.768049  9160 net.cpp:408] label_data_1_split -> label_data_1_split_1
I0411 00:38:11.768059  9160 net.cpp:408] label_data_1_split -> label_data_1_split_2
I0411 00:38:11.768213  9160 net.cpp:150] Setting up label_data_1_split
I0411 00:38:11.768221  9160 net.cpp:157] Top shape: 1024 (1024)
I0411 00:38:11.768225  9160 net.cpp:157] Top shape: 1024 (1024)
I0411 00:38:11.768229  9160 net.cpp:157] Top shape: 1024 (1024)
I0411 00:38:11.768231  9160 net.cpp:165] Memory required for data: 28327936
I0411 00:38:11.768234  9160 layer_factory.hpp:77] Creating layer conv1
I0411 00:38:11.768249  9160 net.cpp:100] Creating Layer conv1
I0411 00:38:11.768254  9160 net.cpp:434] conv1 <- data
I0411 00:38:11.768260  9160 net.cpp:408] conv1 -> conv1
I0411 00:38:11.771579  9160 net.cpp:150] Setting up conv1
I0411 00:38:11.771598  9160 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:38:11.771605  9160 net.cpp:165] Memory required for data: 750862336
I0411 00:38:11.771616  9160 layer_factory.hpp:77] Creating layer conv1_prescale
I0411 00:38:11.771627  9160 net.cpp:100] Creating Layer conv1_prescale
I0411 00:38:11.771632  9160 net.cpp:434] conv1_prescale <- conv1
I0411 00:38:11.771657  9160 net.cpp:395] conv1_prescale -> conv1 (in-place)
I0411 00:38:11.771775  9160 net.cpp:150] Setting up conv1_prescale
I0411 00:38:11.771785  9160 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:38:11.771787  9160 net.cpp:165] Memory required for data: 1473396736
I0411 00:38:11.771795  9160 layer_factory.hpp:77] Creating layer conv1_sTanH
I0411 00:38:11.771805  9160 net.cpp:100] Creating Layer conv1_sTanH
I0411 00:38:11.771809  9160 net.cpp:434] conv1_sTanH <- conv1
I0411 00:38:11.771813  9160 net.cpp:395] conv1_sTanH -> conv1 (in-place)
I0411 00:38:11.772027  9160 net.cpp:150] Setting up conv1_sTanH
I0411 00:38:11.772042  9160 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:38:11.772047  9160 net.cpp:165] Memory required for data: 2195931136
I0411 00:38:11.772050  9160 layer_factory.hpp:77] Creating layer conv1_postscale
I0411 00:38:11.772058  9160 net.cpp:100] Creating Layer conv1_postscale
I0411 00:38:11.772060  9160 net.cpp:434] conv1_postscale <- conv1
I0411 00:38:11.772066  9160 net.cpp:395] conv1_postscale -> conv1 (in-place)
I0411 00:38:11.772176  9160 net.cpp:150] Setting up conv1_postscale
I0411 00:38:11.772184  9160 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:38:11.772187  9160 net.cpp:165] Memory required for data: 2918465536
I0411 00:38:11.772192  9160 layer_factory.hpp:77] Creating layer pool1
I0411 00:38:11.772198  9160 net.cpp:100] Creating Layer pool1
I0411 00:38:11.772202  9160 net.cpp:434] pool1 <- conv1
I0411 00:38:11.772207  9160 net.cpp:408] pool1 -> pool1
I0411 00:38:11.772251  9160 net.cpp:150] Setting up pool1
I0411 00:38:11.772258  9160 net.cpp:157] Top shape: 1024 100 21 21 (45158400)
I0411 00:38:11.772261  9160 net.cpp:165] Memory required for data: 3099099136
I0411 00:38:11.772264  9160 layer_factory.hpp:77] Creating layer conv2
I0411 00:38:11.772274  9160 net.cpp:100] Creating Layer conv2
I0411 00:38:11.772279  9160 net.cpp:434] conv2 <- pool1
I0411 00:38:11.772285  9160 net.cpp:408] conv2 -> conv2
I0411 00:38:11.777401  9160 net.cpp:150] Setting up conv2
I0411 00:38:11.777425  9160 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:38:11.777428  9160 net.cpp:165] Memory required for data: 3298164736
I0411 00:38:11.777438  9160 layer_factory.hpp:77] Creating layer conv2_prescale
I0411 00:38:11.777451  9160 net.cpp:100] Creating Layer conv2_prescale
I0411 00:38:11.777457  9160 net.cpp:434] conv2_prescale <- conv2
I0411 00:38:11.777463  9160 net.cpp:395] conv2_prescale -> conv2 (in-place)
I0411 00:38:11.777577  9160 net.cpp:150] Setting up conv2_prescale
I0411 00:38:11.777586  9160 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:38:11.777590  9160 net.cpp:165] Memory required for data: 3497230336
I0411 00:38:11.777595  9160 layer_factory.hpp:77] Creating layer conv2_sTanH
I0411 00:38:11.777601  9160 net.cpp:100] Creating Layer conv2_sTanH
I0411 00:38:11.777603  9160 net.cpp:434] conv2_sTanH <- conv2
I0411 00:38:11.777608  9160 net.cpp:395] conv2_sTanH -> conv2 (in-place)
I0411 00:38:11.778535  9160 net.cpp:150] Setting up conv2_sTanH
I0411 00:38:11.778550  9160 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:38:11.778554  9160 net.cpp:165] Memory required for data: 3696295936
I0411 00:38:11.778558  9160 layer_factory.hpp:77] Creating layer conv2_postscale
I0411 00:38:11.778566  9160 net.cpp:100] Creating Layer conv2_postscale
I0411 00:38:11.778571  9160 net.cpp:434] conv2_postscale <- conv2
I0411 00:38:11.778578  9160 net.cpp:395] conv2_postscale -> conv2 (in-place)
I0411 00:38:11.778682  9160 net.cpp:150] Setting up conv2_postscale
I0411 00:38:11.778690  9160 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:38:11.778693  9160 net.cpp:165] Memory required for data: 3895361536
I0411 00:38:11.778698  9160 layer_factory.hpp:77] Creating layer pool2
I0411 00:38:11.778707  9160 net.cpp:100] Creating Layer pool2
I0411 00:38:11.778710  9160 net.cpp:434] pool2 <- conv2
I0411 00:38:11.778715  9160 net.cpp:408] pool2 -> pool2
I0411 00:38:11.778775  9160 net.cpp:150] Setting up pool2
I0411 00:38:11.778797  9160 net.cpp:157] Top shape: 1024 150 9 9 (12441600)
I0411 00:38:11.778801  9160 net.cpp:165] Memory required for data: 3945127936
I0411 00:38:11.778805  9160 layer_factory.hpp:77] Creating layer conv3
I0411 00:38:11.778815  9160 net.cpp:100] Creating Layer conv3
I0411 00:38:11.778820  9160 net.cpp:434] conv3 <- pool2
I0411 00:38:11.778826  9160 net.cpp:408] conv3 -> conv3
I0411 00:38:11.784762  9160 net.cpp:150] Setting up conv3
I0411 00:38:11.784780  9160 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:38:11.784783  9160 net.cpp:165] Memory required for data: 3981991936
I0411 00:38:11.784793  9160 layer_factory.hpp:77] Creating layer conv3_prescale
I0411 00:38:11.784804  9160 net.cpp:100] Creating Layer conv3_prescale
I0411 00:38:11.784809  9160 net.cpp:434] conv3_prescale <- conv3
I0411 00:38:11.784816  9160 net.cpp:395] conv3_prescale -> conv3 (in-place)
I0411 00:38:11.784919  9160 net.cpp:150] Setting up conv3_prescale
I0411 00:38:11.784927  9160 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:38:11.784930  9160 net.cpp:165] Memory required for data: 4018855936
I0411 00:38:11.784934  9160 layer_factory.hpp:77] Creating layer conv3_sTanH
I0411 00:38:11.784940  9160 net.cpp:100] Creating Layer conv3_sTanH
I0411 00:38:11.784943  9160 net.cpp:434] conv3_sTanH <- conv3
I0411 00:38:11.784948  9160 net.cpp:395] conv3_sTanH -> conv3 (in-place)
I0411 00:38:11.785723  9160 net.cpp:150] Setting up conv3_sTanH
I0411 00:38:11.785739  9160 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:38:11.785742  9160 net.cpp:165] Memory required for data: 4055719936
I0411 00:38:11.785747  9160 layer_factory.hpp:77] Creating layer conv3_postscale
I0411 00:38:11.785756  9160 net.cpp:100] Creating Layer conv3_postscale
I0411 00:38:11.785763  9160 net.cpp:434] conv3_postscale <- conv3
I0411 00:38:11.785768  9160 net.cpp:395] conv3_postscale -> conv3 (in-place)
I0411 00:38:11.785871  9160 net.cpp:150] Setting up conv3_postscale
I0411 00:38:11.785879  9160 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:38:11.785882  9160 net.cpp:165] Memory required for data: 4092583936
I0411 00:38:11.785887  9160 layer_factory.hpp:77] Creating layer pool3
I0411 00:38:11.785897  9160 net.cpp:100] Creating Layer pool3
I0411 00:38:11.785902  9160 net.cpp:434] pool3 <- conv3
I0411 00:38:11.785907  9160 net.cpp:408] pool3 -> pool3
I0411 00:38:11.785949  9160 net.cpp:150] Setting up pool3
I0411 00:38:11.785956  9160 net.cpp:157] Top shape: 1024 250 3 3 (2304000)
I0411 00:38:11.785959  9160 net.cpp:165] Memory required for data: 4101799936
I0411 00:38:11.785964  9160 layer_factory.hpp:77] Creating layer fc4_300
I0411 00:38:11.785969  9160 net.cpp:100] Creating Layer fc4_300
I0411 00:38:11.785974  9160 net.cpp:434] fc4_300 <- pool3
I0411 00:38:11.785980  9160 net.cpp:408] fc4_300 -> fc4_300
I0411 00:38:11.791347  9160 net.cpp:150] Setting up fc4_300
I0411 00:38:11.791363  9160 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:38:11.791366  9160 net.cpp:165] Memory required for data: 4103028736
I0411 00:38:11.791373  9160 layer_factory.hpp:77] Creating layer fc4_prescale
I0411 00:38:11.791380  9160 net.cpp:100] Creating Layer fc4_prescale
I0411 00:38:11.791385  9160 net.cpp:434] fc4_prescale <- fc4_300
I0411 00:38:11.791393  9160 net.cpp:395] fc4_prescale -> fc4_300 (in-place)
I0411 00:38:11.791504  9160 net.cpp:150] Setting up fc4_prescale
I0411 00:38:11.791513  9160 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:38:11.791517  9160 net.cpp:165] Memory required for data: 4104257536
I0411 00:38:11.791520  9160 layer_factory.hpp:77] Creating layer fc4_sTanH
I0411 00:38:11.791527  9160 net.cpp:100] Creating Layer fc4_sTanH
I0411 00:38:11.791529  9160 net.cpp:434] fc4_sTanH <- fc4_300
I0411 00:38:11.791534  9160 net.cpp:395] fc4_sTanH -> fc4_300 (in-place)
I0411 00:38:11.791730  9160 net.cpp:150] Setting up fc4_sTanH
I0411 00:38:11.791741  9160 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:38:11.791744  9160 net.cpp:165] Memory required for data: 4105486336
I0411 00:38:11.791748  9160 layer_factory.hpp:77] Creating layer fc4_postscale
I0411 00:38:11.791769  9160 net.cpp:100] Creating Layer fc4_postscale
I0411 00:38:11.791774  9160 net.cpp:434] fc4_postscale <- fc4_300
I0411 00:38:11.791782  9160 net.cpp:395] fc4_postscale -> fc4_300 (in-place)
I0411 00:38:11.791899  9160 net.cpp:150] Setting up fc4_postscale
I0411 00:38:11.791909  9160 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:38:11.791913  9160 net.cpp:165] Memory required for data: 4106715136
I0411 00:38:11.791918  9160 layer_factory.hpp:77] Creating layer fc5_67
I0411 00:38:11.791923  9160 net.cpp:100] Creating Layer fc5_67
I0411 00:38:11.791929  9160 net.cpp:434] fc5_67 <- fc4_300
I0411 00:38:11.791935  9160 net.cpp:408] fc5_67 -> fc5_classes
I0411 00:38:11.792188  9160 net.cpp:150] Setting up fc5_67
I0411 00:38:11.792197  9160 net.cpp:157] Top shape: 1024 67 (68608)
I0411 00:38:11.792201  9160 net.cpp:165] Memory required for data: 4106989568
I0411 00:38:11.792215  9160 layer_factory.hpp:77] Creating layer fc5_classes_fc5_67_0_split
I0411 00:38:11.792224  9160 net.cpp:100] Creating Layer fc5_classes_fc5_67_0_split
I0411 00:38:11.792229  9160 net.cpp:434] fc5_classes_fc5_67_0_split <- fc5_classes
I0411 00:38:11.792235  9160 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_0
I0411 00:38:11.792243  9160 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_1
I0411 00:38:11.792248  9160 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_2
I0411 00:38:11.792299  9160 net.cpp:150] Setting up fc5_classes_fc5_67_0_split
I0411 00:38:11.792305  9160 net.cpp:157] Top shape: 1024 67 (68608)
I0411 00:38:11.792309  9160 net.cpp:157] Top shape: 1024 67 (68608)
I0411 00:38:11.792312  9160 net.cpp:157] Top shape: 1024 67 (68608)
I0411 00:38:11.792315  9160 net.cpp:165] Memory required for data: 4107812864
I0411 00:38:11.792317  9160 layer_factory.hpp:77] Creating layer loss
I0411 00:38:11.792325  9160 net.cpp:100] Creating Layer loss
I0411 00:38:11.792328  9160 net.cpp:434] loss <- fc5_classes_fc5_67_0_split_0
I0411 00:38:11.792332  9160 net.cpp:434] loss <- label_data_1_split_0
I0411 00:38:11.792337  9160 net.cpp:408] loss -> loss
I0411 00:38:11.792351  9160 layer_factory.hpp:77] Creating layer loss
I0411 00:38:11.792686  9160 net.cpp:150] Setting up loss
I0411 00:38:11.792701  9160 net.cpp:157] Top shape: (1)
I0411 00:38:11.792706  9160 net.cpp:160]     with loss weight 1
I0411 00:38:11.792717  9160 net.cpp:165] Memory required for data: 4107812868
I0411 00:38:11.792721  9160 layer_factory.hpp:77] Creating layer accuracy_1
I0411 00:38:11.792729  9160 net.cpp:100] Creating Layer accuracy_1
I0411 00:38:11.792735  9160 net.cpp:434] accuracy_1 <- fc5_classes_fc5_67_0_split_1
I0411 00:38:11.792739  9160 net.cpp:434] accuracy_1 <- label_data_1_split_1
I0411 00:38:11.792747  9160 net.cpp:408] accuracy_1 -> accuracy_1
I0411 00:38:11.792757  9160 net.cpp:150] Setting up accuracy_1
I0411 00:38:11.792763  9160 net.cpp:157] Top shape: (1)
I0411 00:38:11.792765  9160 net.cpp:165] Memory required for data: 4107812872
I0411 00:38:11.792768  9160 layer_factory.hpp:77] Creating layer accuracy_5
I0411 00:38:11.792773  9160 net.cpp:100] Creating Layer accuracy_5
I0411 00:38:11.792776  9160 net.cpp:434] accuracy_5 <- fc5_classes_fc5_67_0_split_2
I0411 00:38:11.792780  9160 net.cpp:434] accuracy_5 <- label_data_1_split_2
I0411 00:38:11.792786  9160 net.cpp:408] accuracy_5 -> accuracy_5
I0411 00:38:11.792804  9160 net.cpp:150] Setting up accuracy_5
I0411 00:38:11.792810  9160 net.cpp:157] Top shape: (1)
I0411 00:38:11.792812  9160 net.cpp:165] Memory required for data: 4107812876
I0411 00:38:11.792815  9160 net.cpp:228] accuracy_5 does not need backward computation.
I0411 00:38:11.792819  9160 net.cpp:228] accuracy_1 does not need backward computation.
I0411 00:38:11.792822  9160 net.cpp:226] loss needs backward computation.
I0411 00:38:11.792826  9160 net.cpp:226] fc5_classes_fc5_67_0_split needs backward computation.
I0411 00:38:11.792829  9160 net.cpp:226] fc5_67 needs backward computation.
I0411 00:38:11.792832  9160 net.cpp:226] fc4_postscale needs backward computation.
I0411 00:38:11.792847  9160 net.cpp:226] fc4_sTanH needs backward computation.
I0411 00:38:11.792851  9160 net.cpp:226] fc4_prescale needs backward computation.
I0411 00:38:11.792855  9160 net.cpp:226] fc4_300 needs backward computation.
I0411 00:38:11.792862  9160 net.cpp:226] pool3 needs backward computation.
I0411 00:38:11.792875  9160 net.cpp:226] conv3_postscale needs backward computation.
I0411 00:38:11.792877  9160 net.cpp:226] conv3_sTanH needs backward computation.
I0411 00:38:11.792881  9160 net.cpp:226] conv3_prescale needs backward computation.
I0411 00:38:11.792882  9160 net.cpp:226] conv3 needs backward computation.
I0411 00:38:11.792886  9160 net.cpp:226] pool2 needs backward computation.
I0411 00:38:11.792888  9160 net.cpp:226] conv2_postscale needs backward computation.
I0411 00:38:11.792891  9160 net.cpp:226] conv2_sTanH needs backward computation.
I0411 00:38:11.792893  9160 net.cpp:226] conv2_prescale needs backward computation.
I0411 00:38:11.792896  9160 net.cpp:226] conv2 needs backward computation.
I0411 00:38:11.792899  9160 net.cpp:226] pool1 needs backward computation.
I0411 00:38:11.792903  9160 net.cpp:226] conv1_postscale needs backward computation.
I0411 00:38:11.792906  9160 net.cpp:226] conv1_sTanH needs backward computation.
I0411 00:38:11.792908  9160 net.cpp:226] conv1_prescale needs backward computation.
I0411 00:38:11.792912  9160 net.cpp:226] conv1 needs backward computation.
I0411 00:38:11.792915  9160 net.cpp:228] label_data_1_split does not need backward computation.
I0411 00:38:11.792919  9160 net.cpp:228] data does not need backward computation.
I0411 00:38:11.792922  9160 net.cpp:270] This network produces output accuracy_1
I0411 00:38:11.792925  9160 net.cpp:270] This network produces output accuracy_5
I0411 00:38:11.792928  9160 net.cpp:270] This network produces output loss
I0411 00:38:11.792949  9160 net.cpp:283] Network initialization done.
I0411 00:38:11.793030  9160 solver.cpp:72] Solver scaffolding done.
I0411 00:38:11.793931  9160 caffe.cpp:251] Starting Optimization
I0411 00:38:11.793939  9160 solver.cpp:291] Solving 
I0411 00:38:11.793946  9160 solver.cpp:292] Learning Rate Policy: step
I0411 00:38:11.796214  9160 solver.cpp:349] Iteration 0, Testing net (#0)
I0411 00:38:11.797592  9160 blocking_queue.cpp:50] Data layer prefetch queue empty
I0411 00:38:12.897162  9160 solver.cpp:416]     Test net output #0: accuracy_1 = 0.0141602
I0411 00:38:12.897192  9160 solver.cpp:416]     Test net output #1: accuracy_5 = 0.0792236
I0411 00:38:12.897200  9160 solver.cpp:416]     Test net output #2: loss = 4.62747 (* 1 = 4.62747 loss)
I0411 00:38:13.061264  9160 solver.cpp:240] Iteration 0, loss = 4.71459
I0411 00:38:13.061314  9160 solver.cpp:256]     Train net output #0: loss = 4.71459 (* 1 = 4.71459 loss)
I0411 00:38:13.061334  9160 sgd_solver.cpp:106] Iteration 0, lr = 1e-05
I0411 00:38:13.428357  9160 solver.cpp:240] Iteration 1, loss = 4.60269
I0411 00:38:13.428400  9160 solver.cpp:256]     Train net output #0: loss = 4.60269 (* 1 = 4.60269 loss)
I0411 00:38:13.428408  9160 sgd_solver.cpp:106] Iteration 1, lr = 1e-05
I0411 00:38:13.796886  9160 solver.cpp:240] Iteration 2, loss = 4.61779
I0411 00:38:13.796918  9160 solver.cpp:256]     Train net output #0: loss = 4.61779 (* 1 = 4.61779 loss)
I0411 00:38:13.796926  9160 sgd_solver.cpp:106] Iteration 2, lr = 1e-05
I0411 00:38:14.163655  9160 solver.cpp:240] Iteration 3, loss = 4.59911
I0411 00:38:14.163687  9160 solver.cpp:256]     Train net output #0: loss = 4.59911 (* 1 = 4.59911 loss)
I0411 00:38:14.163697  9160 sgd_solver.cpp:106] Iteration 3, lr = 1e-05
I0411 00:38:14.529651  9160 solver.cpp:240] Iteration 4, loss = 4.45684
I0411 00:38:14.529683  9160 solver.cpp:256]     Train net output #0: loss = 4.45684 (* 1 = 4.45684 loss)
I0411 00:38:14.529691  9160 sgd_solver.cpp:106] Iteration 4, lr = 1e-05
I0411 00:38:14.895604  9160 solver.cpp:240] Iteration 5, loss = 4.48899
I0411 00:38:14.895637  9160 solver.cpp:256]     Train net output #0: loss = 4.48899 (* 1 = 4.48899 loss)
I0411 00:38:14.895645  9160 sgd_solver.cpp:106] Iteration 5, lr = 1e-05
I0411 00:38:15.261950  9160 solver.cpp:240] Iteration 6, loss = 4.45783
I0411 00:38:15.261981  9160 solver.cpp:256]     Train net output #0: loss = 4.45783 (* 1 = 4.45783 loss)
I0411 00:38:15.261991  9160 sgd_solver.cpp:106] Iteration 6, lr = 1e-05
I0411 00:38:15.623837  9160 solver.cpp:240] Iteration 7, loss = 4.41217
I0411 00:38:15.623869  9160 solver.cpp:256]     Train net output #0: loss = 4.41217 (* 1 = 4.41217 loss)
I0411 00:38:15.623895  9160 sgd_solver.cpp:106] Iteration 7, lr = 1e-05
I0411 00:38:15.990066  9160 solver.cpp:240] Iteration 8, loss = 4.37313
I0411 00:38:15.990109  9160 solver.cpp:256]     Train net output #0: loss = 4.37313 (* 1 = 4.37313 loss)
I0411 00:38:15.990116  9160 sgd_solver.cpp:106] Iteration 8, lr = 1e-05
I0411 00:38:16.356124  9160 solver.cpp:240] Iteration 9, loss = 4.35133
I0411 00:38:16.356158  9160 solver.cpp:256]     Train net output #0: loss = 4.35133 (* 1 = 4.35133 loss)
I0411 00:38:16.356165  9160 sgd_solver.cpp:106] Iteration 9, lr = 1e-05
I0411 00:38:16.720433  9160 solver.cpp:240] Iteration 10, loss = 4.33615
I0411 00:38:16.720465  9160 solver.cpp:256]     Train net output #0: loss = 4.33615 (* 1 = 4.33615 loss)
I0411 00:38:16.720479  9160 sgd_solver.cpp:106] Iteration 10, lr = 1e-05
I0411 00:38:17.086482  9160 solver.cpp:240] Iteration 11, loss = 4.30335
I0411 00:38:17.086527  9160 solver.cpp:256]     Train net output #0: loss = 4.30335 (* 1 = 4.30335 loss)
I0411 00:38:17.086536  9160 sgd_solver.cpp:106] Iteration 11, lr = 1e-05
I0411 00:38:17.451699  9160 solver.cpp:240] Iteration 12, loss = 4.25088
I0411 00:38:17.451731  9160 solver.cpp:256]     Train net output #0: loss = 4.25088 (* 1 = 4.25088 loss)
I0411 00:38:17.451740  9160 sgd_solver.cpp:106] Iteration 12, lr = 1e-05
I0411 00:38:17.816721  9160 solver.cpp:240] Iteration 13, loss = 4.21411
I0411 00:38:17.816752  9160 solver.cpp:256]     Train net output #0: loss = 4.21411 (* 1 = 4.21411 loss)
I0411 00:38:17.816761  9160 sgd_solver.cpp:106] Iteration 13, lr = 1e-05
I0411 00:38:18.186156  9160 solver.cpp:240] Iteration 14, loss = 4.20514
I0411 00:38:18.186200  9160 solver.cpp:256]     Train net output #0: loss = 4.20514 (* 1 = 4.20514 loss)
I0411 00:38:18.186208  9160 sgd_solver.cpp:106] Iteration 14, lr = 1e-05
I0411 00:38:18.552734  9160 solver.cpp:240] Iteration 15, loss = 4.13266
I0411 00:38:18.552778  9160 solver.cpp:256]     Train net output #0: loss = 4.13266 (* 1 = 4.13266 loss)
I0411 00:38:18.552786  9160 sgd_solver.cpp:106] Iteration 15, lr = 1e-05
I0411 00:38:18.917774  9160 solver.cpp:240] Iteration 16, loss = 4.12612
I0411 00:38:18.917819  9160 solver.cpp:256]     Train net output #0: loss = 4.12612 (* 1 = 4.12612 loss)
I0411 00:38:18.917826  9160 sgd_solver.cpp:106] Iteration 16, lr = 1e-05
I0411 00:38:19.286370  9160 solver.cpp:240] Iteration 17, loss = 4.1238
I0411 00:38:19.286409  9160 solver.cpp:256]     Train net output #0: loss = 4.1238 (* 1 = 4.1238 loss)
I0411 00:38:19.286419  9160 sgd_solver.cpp:106] Iteration 17, lr = 1e-05
I0411 00:38:19.657973  9160 solver.cpp:240] Iteration 18, loss = 4.125
I0411 00:38:19.658012  9160 solver.cpp:256]     Train net output #0: loss = 4.125 (* 1 = 4.125 loss)
I0411 00:38:19.658025  9160 sgd_solver.cpp:106] Iteration 18, lr = 1e-05
I0411 00:38:20.025146  9160 solver.cpp:240] Iteration 19, loss = 4.0912
I0411 00:38:20.025182  9160 solver.cpp:256]     Train net output #0: loss = 4.0912 (* 1 = 4.0912 loss)
I0411 00:38:20.025193  9160 sgd_solver.cpp:106] Iteration 19, lr = 1e-05
I0411 00:38:20.392437  9160 solver.cpp:240] Iteration 20, loss = 4.10453
I0411 00:38:20.392474  9160 solver.cpp:256]     Train net output #0: loss = 4.10453 (* 1 = 4.10453 loss)
I0411 00:38:20.392487  9160 sgd_solver.cpp:106] Iteration 20, lr = 1e-05
I0411 00:38:20.759038  9160 solver.cpp:240] Iteration 21, loss = 4.01491
I0411 00:38:20.759075  9160 solver.cpp:256]     Train net output #0: loss = 4.01491 (* 1 = 4.01491 loss)
I0411 00:38:20.759088  9160 sgd_solver.cpp:106] Iteration 21, lr = 1e-05
I0411 00:38:21.125977  9160 solver.cpp:240] Iteration 22, loss = 4.0839
I0411 00:38:21.126039  9160 solver.cpp:256]     Train net output #0: loss = 4.0839 (* 1 = 4.0839 loss)
I0411 00:38:21.126051  9160 sgd_solver.cpp:106] Iteration 22, lr = 1e-05
I0411 00:38:21.487793  9160 solver.cpp:240] Iteration 23, loss = 4.00271
I0411 00:38:21.487828  9160 solver.cpp:256]     Train net output #0: loss = 4.00271 (* 1 = 4.00271 loss)
I0411 00:38:21.487840  9160 sgd_solver.cpp:106] Iteration 23, lr = 1e-05
I0411 00:38:21.858239  9160 solver.cpp:240] Iteration 24, loss = 4.0098
I0411 00:38:21.858273  9160 solver.cpp:256]     Train net output #0: loss = 4.0098 (* 1 = 4.0098 loss)
I0411 00:38:21.858284  9160 sgd_solver.cpp:106] Iteration 24, lr = 1e-05
I0411 00:38:21.858611  9160 solver.cpp:349] Iteration 25, Testing net (#0)
I0411 00:38:23.134757  9160 solver.cpp:416]     Test net output #0: accuracy_1 = 0.0924072
I0411 00:38:23.134785  9160 solver.cpp:416]     Test net output #1: accuracy_5 = 0.266724
I0411 00:38:23.134799  9160 solver.cpp:416]     Test net output #2: loss = 4.09303 (* 1 = 4.09303 loss)
I0411 00:38:23.262019  9160 solver.cpp:240] Iteration 25, loss = 4.01107
I0411 00:38:23.262054  9160 solver.cpp:256]     Train net output #0: loss = 4.01107 (* 1 = 4.01107 loss)
I0411 00:38:23.262076  9160 sgd_solver.cpp:106] Iteration 25, lr = 1e-05
I0411 00:38:23.626292  9160 solver.cpp:240] Iteration 26, loss = 3.94494
I0411 00:38:23.626324  9160 solver.cpp:256]     Train net output #0: loss = 3.94494 (* 1 = 3.94494 loss)
I0411 00:38:23.626335  9160 sgd_solver.cpp:106] Iteration 26, lr = 1e-05
I0411 00:38:23.988011  9160 solver.cpp:240] Iteration 27, loss = 3.95834
I0411 00:38:23.988049  9160 solver.cpp:256]     Train net output #0: loss = 3.95834 (* 1 = 3.95834 loss)
I0411 00:38:23.988060  9160 sgd_solver.cpp:106] Iteration 27, lr = 1e-05
I0411 00:38:24.356179  9160 solver.cpp:240] Iteration 28, loss = 3.96773
I0411 00:38:24.356225  9160 solver.cpp:256]     Train net output #0: loss = 3.96773 (* 1 = 3.96773 loss)
I0411 00:38:24.356236  9160 sgd_solver.cpp:106] Iteration 28, lr = 1e-05
I0411 00:38:24.723345  9160 solver.cpp:240] Iteration 29, loss = 3.86075
I0411 00:38:24.723379  9160 solver.cpp:256]     Train net output #0: loss = 3.86075 (* 1 = 3.86075 loss)
I0411 00:38:24.723392  9160 sgd_solver.cpp:106] Iteration 29, lr = 1e-05
I0411 00:38:25.090796  9160 solver.cpp:240] Iteration 30, loss = 3.94205
I0411 00:38:25.090831  9160 solver.cpp:256]     Train net output #0: loss = 3.94205 (* 1 = 3.94205 loss)
I0411 00:38:25.090853  9160 sgd_solver.cpp:106] Iteration 30, lr = 1e-05
I0411 00:38:25.456877  9160 solver.cpp:240] Iteration 31, loss = 3.88291
I0411 00:38:25.456921  9160 solver.cpp:256]     Train net output #0: loss = 3.88291 (* 1 = 3.88291 loss)
I0411 00:38:25.456943  9160 sgd_solver.cpp:106] Iteration 31, lr = 1e-05
I0411 00:38:25.826009  9160 solver.cpp:240] Iteration 32, loss = 3.91228
I0411 00:38:25.826042  9160 solver.cpp:256]     Train net output #0: loss = 3.91228 (* 1 = 3.91228 loss)
I0411 00:38:25.826064  9160 sgd_solver.cpp:106] Iteration 32, lr = 1e-05
I0411 00:38:26.198231  9160 solver.cpp:240] Iteration 33, loss = 3.84927
I0411 00:38:26.198267  9160 solver.cpp:256]     Train net output #0: loss = 3.84927 (* 1 = 3.84927 loss)
I0411 00:38:26.198278  9160 sgd_solver.cpp:106] Iteration 33, lr = 1e-05
I0411 00:38:26.565660  9160 solver.cpp:240] Iteration 34, loss = 3.90165
I0411 00:38:26.565696  9160 solver.cpp:256]     Train net output #0: loss = 3.90165 (* 1 = 3.90165 loss)
I0411 00:38:26.565706  9160 sgd_solver.cpp:106] Iteration 34, lr = 1e-05
I0411 00:38:26.932557  9160 solver.cpp:240] Iteration 35, loss = 3.8551
I0411 00:38:26.932591  9160 solver.cpp:256]     Train net output #0: loss = 3.8551 (* 1 = 3.8551 loss)
I0411 00:38:26.932615  9160 sgd_solver.cpp:106] Iteration 35, lr = 1e-05
I0411 00:38:27.299943  9160 solver.cpp:240] Iteration 36, loss = 3.84911
I0411 00:38:27.299978  9160 solver.cpp:256]     Train net output #0: loss = 3.84911 (* 1 = 3.84911 loss)
I0411 00:38:27.299989  9160 sgd_solver.cpp:106] Iteration 36, lr = 1e-05
I0411 00:38:27.667182  9160 solver.cpp:240] Iteration 37, loss = 3.81597
I0411 00:38:27.667243  9160 solver.cpp:256]     Train net output #0: loss = 3.81597 (* 1 = 3.81597 loss)
I0411 00:38:27.667255  9160 sgd_solver.cpp:106] Iteration 37, lr = 1e-05
I0411 00:38:28.038645  9160 solver.cpp:240] Iteration 38, loss = 3.79761
I0411 00:38:28.038678  9160 solver.cpp:256]     Train net output #0: loss = 3.79761 (* 1 = 3.79761 loss)
I0411 00:38:28.038689  9160 sgd_solver.cpp:106] Iteration 38, lr = 1e-05
I0411 00:38:28.405288  9160 solver.cpp:240] Iteration 39, loss = 3.85582
I0411 00:38:28.405323  9160 solver.cpp:256]     Train net output #0: loss = 3.85582 (* 1 = 3.85582 loss)
I0411 00:38:28.405333  9160 sgd_solver.cpp:106] Iteration 39, lr = 1e-05
I0411 00:38:28.770912  9160 solver.cpp:240] Iteration 40, loss = 3.7391
I0411 00:38:28.770947  9160 solver.cpp:256]     Train net output #0: loss = 3.7391 (* 1 = 3.7391 loss)
I0411 00:38:28.770958  9160 sgd_solver.cpp:106] Iteration 40, lr = 1e-05
I0411 00:38:29.138648  9160 solver.cpp:240] Iteration 41, loss = 3.78771
I0411 00:38:29.138682  9160 solver.cpp:256]     Train net output #0: loss = 3.78771 (* 1 = 3.78771 loss)
I0411 00:38:29.138694  9160 sgd_solver.cpp:106] Iteration 41, lr = 1e-05
I0411 00:38:29.507382  9160 solver.cpp:240] Iteration 42, loss = 3.79584
I0411 00:38:29.507418  9160 solver.cpp:256]     Train net output #0: loss = 3.79584 (* 1 = 3.79584 loss)
I0411 00:38:29.507429  9160 sgd_solver.cpp:106] Iteration 42, lr = 1e-05
I0411 00:38:29.876569  9160 solver.cpp:240] Iteration 43, loss = 3.75521
I0411 00:38:29.876602  9160 solver.cpp:256]     Train net output #0: loss = 3.75521 (* 1 = 3.75521 loss)
I0411 00:38:29.876613  9160 sgd_solver.cpp:106] Iteration 43, lr = 1e-05
I0411 00:38:30.244652  9160 solver.cpp:240] Iteration 44, loss = 3.75478
I0411 00:38:30.244688  9160 solver.cpp:256]     Train net output #0: loss = 3.75478 (* 1 = 3.75478 loss)
I0411 00:38:30.244699  9160 sgd_solver.cpp:106] Iteration 44, lr = 1e-05
I0411 00:38:30.608805  9160 solver.cpp:240] Iteration 45, loss = 3.7484
I0411 00:38:30.608839  9160 solver.cpp:256]     Train net output #0: loss = 3.7484 (* 1 = 3.7484 loss)
I0411 00:38:30.608851  9160 sgd_solver.cpp:106] Iteration 45, lr = 1e-05
I0411 00:38:30.974383  9160 solver.cpp:240] Iteration 46, loss = 3.70934
I0411 00:38:30.974417  9160 solver.cpp:256]     Train net output #0: loss = 3.70934 (* 1 = 3.70934 loss)
I0411 00:38:30.974429  9160 sgd_solver.cpp:106] Iteration 46, lr = 1e-05
I0411 00:38:31.345697  9160 solver.cpp:240] Iteration 47, loss = 3.78757
I0411 00:38:31.345732  9160 solver.cpp:256]     Train net output #0: loss = 3.78757 (* 1 = 3.78757 loss)
I0411 00:38:31.345743  9160 sgd_solver.cpp:106] Iteration 47, lr = 1e-05
I0411 00:38:31.710783  9160 solver.cpp:240] Iteration 48, loss = 3.77307
I0411 00:38:31.710816  9160 solver.cpp:256]     Train net output #0: loss = 3.77307 (* 1 = 3.77307 loss)
I0411 00:38:31.710839  9160 sgd_solver.cpp:106] Iteration 48, lr = 1e-05
I0411 00:38:32.079042  9160 solver.cpp:240] Iteration 49, loss = 3.75724
I0411 00:38:32.079080  9160 solver.cpp:256]     Train net output #0: loss = 3.75724 (* 1 = 3.75724 loss)
I0411 00:38:32.079092  9160 sgd_solver.cpp:106] Iteration 49, lr = 1e-05
I0411 00:38:32.079429  9160 solver.cpp:349] Iteration 50, Testing net (#0)
I0411 00:38:33.360251  9160 solver.cpp:416]     Test net output #0: accuracy_1 = 0.119141
I0411 00:38:33.360281  9160 solver.cpp:416]     Test net output #1: accuracy_5 = 0.330811
I0411 00:38:33.360294  9160 solver.cpp:416]     Test net output #2: loss = 3.91311 (* 1 = 3.91311 loss)
I0411 00:38:33.487052  9160 solver.cpp:240] Iteration 50, loss = 3.68758
I0411 00:38:33.487087  9160 solver.cpp:256]     Train net output #0: loss = 3.68758 (* 1 = 3.68758 loss)
I0411 00:38:33.487097  9160 sgd_solver.cpp:106] Iteration 50, lr = 1e-05
I0411 00:38:33.853655  9160 solver.cpp:240] Iteration 51, loss = 3.72471
I0411 00:38:33.853689  9160 solver.cpp:256]     Train net output #0: loss = 3.72471 (* 1 = 3.72471 loss)
I0411 00:38:33.853711  9160 sgd_solver.cpp:106] Iteration 51, lr = 1e-05
I0411 00:38:34.224066  9160 solver.cpp:240] Iteration 52, loss = 3.74025
I0411 00:38:34.224102  9160 solver.cpp:256]     Train net output #0: loss = 3.74025 (* 1 = 3.74025 loss)
I0411 00:38:34.224112  9160 sgd_solver.cpp:106] Iteration 52, lr = 1e-05
I0411 00:38:34.593017  9160 solver.cpp:240] Iteration 53, loss = 3.70828
I0411 00:38:34.593051  9160 solver.cpp:256]     Train net output #0: loss = 3.70828 (* 1 = 3.70828 loss)
I0411 00:38:34.593062  9160 sgd_solver.cpp:106] Iteration 53, lr = 1e-05
I0411 00:38:34.960973  9160 solver.cpp:240] Iteration 54, loss = 3.65969
I0411 00:38:34.961009  9160 solver.cpp:256]     Train net output #0: loss = 3.65969 (* 1 = 3.65969 loss)
I0411 00:38:34.961020  9160 sgd_solver.cpp:106] Iteration 54, lr = 1e-05
I0411 00:38:35.326414  9160 solver.cpp:240] Iteration 55, loss = 3.7211
I0411 00:38:35.326449  9160 solver.cpp:256]     Train net output #0: loss = 3.7211 (* 1 = 3.7211 loss)
I0411 00:38:35.326462  9160 sgd_solver.cpp:106] Iteration 55, lr = 1e-05
I0411 00:38:35.699697  9160 solver.cpp:240] Iteration 56, loss = 3.67525
I0411 00:38:35.699733  9160 solver.cpp:256]     Train net output #0: loss = 3.67525 (* 1 = 3.67525 loss)
I0411 00:38:35.699743  9160 sgd_solver.cpp:106] Iteration 56, lr = 1e-05
I0411 00:38:36.065994  9160 solver.cpp:240] Iteration 57, loss = 3.71294
I0411 00:38:36.066026  9160 solver.cpp:256]     Train net output #0: loss = 3.71294 (* 1 = 3.71294 loss)
I0411 00:38:36.066048  9160 sgd_solver.cpp:106] Iteration 57, lr = 1e-05
I0411 00:38:36.434798  9160 solver.cpp:240] Iteration 58, loss = 3.67009
I0411 00:38:36.434833  9160 solver.cpp:256]     Train net output #0: loss = 3.67009 (* 1 = 3.67009 loss)
I0411 00:38:36.434844  9160 sgd_solver.cpp:106] Iteration 58, lr = 1e-05
I0411 00:38:36.802775  9160 solver.cpp:240] Iteration 59, loss = 3.65747
I0411 00:38:36.802809  9160 solver.cpp:256]     Train net output #0: loss = 3.65747 (* 1 = 3.65747 loss)
I0411 00:38:36.802821  9160 sgd_solver.cpp:106] Iteration 59, lr = 1e-05
I0411 00:38:37.170130  9160 solver.cpp:240] Iteration 60, loss = 3.63838
I0411 00:38:37.170162  9160 solver.cpp:256]     Train net output #0: loss = 3.63838 (* 1 = 3.63838 loss)
I0411 00:38:37.170172  9160 sgd_solver.cpp:106] Iteration 60, lr = 1e-05
I0411 00:38:37.539414  9160 solver.cpp:240] Iteration 61, loss = 3.68268
I0411 00:38:37.539448  9160 solver.cpp:256]     Train net output #0: loss = 3.68268 (* 1 = 3.68268 loss)
I0411 00:38:37.539459  9160 sgd_solver.cpp:106] Iteration 61, lr = 1e-05
I0411 00:38:37.909667  9160 solver.cpp:240] Iteration 62, loss = 3.631
I0411 00:38:37.909713  9160 solver.cpp:256]     Train net output #0: loss = 3.631 (* 1 = 3.631 loss)
I0411 00:38:37.909735  9160 sgd_solver.cpp:106] Iteration 62, lr = 1e-05
