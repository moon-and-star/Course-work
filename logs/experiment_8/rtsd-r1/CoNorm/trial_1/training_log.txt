I0411 00:00:11.037475 18531 caffe.cpp:217] Using GPUs 1
I0411 00:00:11.356009 18531 caffe.cpp:222] GPU 1: GeForce GTX 1070
I0411 00:00:12.007467 18531 solver.cpp:60] Initializing solver from parameters: 
train_net: "./Prototxt/experiment_8/rtsd-r1/CoNorm/trial_1/train.prototxt"
test_net: "./Prototxt/experiment_8/rtsd-r1/CoNorm/trial_1/test.prototxt"
test_iter: 8
test_interval: 25
base_lr: 0.001
display: 1
max_iter: 2500
lr_policy: "step"
gamma: 0.5
momentum: 0.9
weight_decay: 0.0005
stepsize: 500
snapshot: 250
snapshot_prefix: "./snapshots/experiment_8/rtsd-r1/CoNorm/trial_1/snap"
solver_mode: GPU
device_id: 1
train_state {
  level: 0
  stage: ""
}
iter_size: 1
type: "Adam"
I0411 00:00:12.007613 18531 solver.cpp:93] Creating training net from train_net file: ./Prototxt/experiment_8/rtsd-r1/CoNorm/trial_1/train.prototxt
I0411 00:00:12.007925 18531 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_1
I0411 00:00:12.007937 18531 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_5
I0411 00:00:12.008087 18531 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: false
    crop_size: 48
    mean_value: 132
    mean_value: 132
    mean_value: 131
  }
  data_param {
    source: "../local_data/lmdb/rtsd-r1/CoNorm/train/lmdb"
    batch_size: 1024
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 7
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_prescale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
  }
}
layer {
  name: "conv1_sTanH"
  type: "TanH"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_postscale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 150
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_prescale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
  }
}
layer {
  name: "conv2_sTanH"
  type: "TanH"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_postscale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 250
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_prescale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
  }
}
layer {
  name: "conv3_sTanH"
  type: "TanH"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_postscale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc4_300"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc4_300"
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fc4_prescale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
  }
}
layer {
  name: "fc4_sTanH"
  type: "TanH"
  bottom: "fc4_300"
  top: "fc4_300"
}
layer {
  name: "fc4_postscale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
  }
}
layer {
  name: "fc5_67"
  type: "InnerProduct"
  bottom: "fc4_300"
  top: "fc5_classes"
  inner_product_param {
    num_output: 67
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc5_classes"
  bottom: "label"
  top: "loss"
}
I0411 00:00:12.008190 18531 layer_factory.hpp:77] Creating layer data
I0411 00:00:12.009259 18531 net.cpp:100] Creating Layer data
I0411 00:00:12.009274 18531 net.cpp:408] data -> data
I0411 00:00:12.009299 18531 net.cpp:408] data -> label
I0411 00:00:12.010475 18631 db_lmdb.cpp:35] Opened lmdb ../local_data/lmdb/rtsd-r1/CoNorm/train/lmdb
I0411 00:00:12.027328 18531 data_layer.cpp:41] output data size: 1024,3,48,48
I0411 00:00:12.075177 18531 net.cpp:150] Setting up data
I0411 00:00:12.075206 18531 net.cpp:157] Top shape: 1024 3 48 48 (7077888)
I0411 00:00:12.075212 18531 net.cpp:157] Top shape: 1024 (1024)
I0411 00:00:12.075215 18531 net.cpp:165] Memory required for data: 28315648
I0411 00:00:12.075225 18531 layer_factory.hpp:77] Creating layer conv1
I0411 00:00:12.075258 18531 net.cpp:100] Creating Layer conv1
I0411 00:00:12.075265 18531 net.cpp:434] conv1 <- data
I0411 00:00:12.075278 18531 net.cpp:408] conv1 -> conv1
I0411 00:00:12.356227 18531 net.cpp:150] Setting up conv1
I0411 00:00:12.356269 18531 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:00:12.356274 18531 net.cpp:165] Memory required for data: 750850048
I0411 00:00:12.356297 18531 layer_factory.hpp:77] Creating layer conv1_prescale
I0411 00:00:12.356312 18531 net.cpp:100] Creating Layer conv1_prescale
I0411 00:00:12.356317 18531 net.cpp:434] conv1_prescale <- conv1
I0411 00:00:12.356323 18531 net.cpp:395] conv1_prescale -> conv1 (in-place)
I0411 00:00:12.356431 18531 net.cpp:150] Setting up conv1_prescale
I0411 00:00:12.356441 18531 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:00:12.356443 18531 net.cpp:165] Memory required for data: 1473384448
I0411 00:00:12.356451 18531 layer_factory.hpp:77] Creating layer conv1_sTanH
I0411 00:00:12.356458 18531 net.cpp:100] Creating Layer conv1_sTanH
I0411 00:00:12.356463 18531 net.cpp:434] conv1_sTanH <- conv1
I0411 00:00:12.356468 18531 net.cpp:395] conv1_sTanH -> conv1 (in-place)
I0411 00:00:12.356665 18531 net.cpp:150] Setting up conv1_sTanH
I0411 00:00:12.356678 18531 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:00:12.356683 18531 net.cpp:165] Memory required for data: 2195918848
I0411 00:00:12.356686 18531 layer_factory.hpp:77] Creating layer conv1_postscale
I0411 00:00:12.356694 18531 net.cpp:100] Creating Layer conv1_postscale
I0411 00:00:12.356699 18531 net.cpp:434] conv1_postscale <- conv1
I0411 00:00:12.356704 18531 net.cpp:395] conv1_postscale -> conv1 (in-place)
I0411 00:00:12.356798 18531 net.cpp:150] Setting up conv1_postscale
I0411 00:00:12.356806 18531 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:00:12.356809 18531 net.cpp:165] Memory required for data: 2918453248
I0411 00:00:12.356814 18531 layer_factory.hpp:77] Creating layer pool1
I0411 00:00:12.356822 18531 net.cpp:100] Creating Layer pool1
I0411 00:00:12.356827 18531 net.cpp:434] pool1 <- conv1
I0411 00:00:12.356832 18531 net.cpp:408] pool1 -> pool1
I0411 00:00:12.356878 18531 net.cpp:150] Setting up pool1
I0411 00:00:12.356885 18531 net.cpp:157] Top shape: 1024 100 21 21 (45158400)
I0411 00:00:12.356889 18531 net.cpp:165] Memory required for data: 3099086848
I0411 00:00:12.356892 18531 layer_factory.hpp:77] Creating layer conv2
I0411 00:00:12.356901 18531 net.cpp:100] Creating Layer conv2
I0411 00:00:12.356925 18531 net.cpp:434] conv2 <- pool1
I0411 00:00:12.356930 18531 net.cpp:408] conv2 -> conv2
I0411 00:00:12.362280 18531 net.cpp:150] Setting up conv2
I0411 00:00:12.362298 18531 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:00:12.362303 18531 net.cpp:165] Memory required for data: 3298152448
I0411 00:00:12.362313 18531 layer_factory.hpp:77] Creating layer conv2_prescale
I0411 00:00:12.362321 18531 net.cpp:100] Creating Layer conv2_prescale
I0411 00:00:12.362324 18531 net.cpp:434] conv2_prescale <- conv2
I0411 00:00:12.362330 18531 net.cpp:395] conv2_prescale -> conv2 (in-place)
I0411 00:00:12.362434 18531 net.cpp:150] Setting up conv2_prescale
I0411 00:00:12.362444 18531 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:00:12.362447 18531 net.cpp:165] Memory required for data: 3497218048
I0411 00:00:12.362452 18531 layer_factory.hpp:77] Creating layer conv2_sTanH
I0411 00:00:12.362458 18531 net.cpp:100] Creating Layer conv2_sTanH
I0411 00:00:12.362463 18531 net.cpp:434] conv2_sTanH <- conv2
I0411 00:00:12.362468 18531 net.cpp:395] conv2_sTanH -> conv2 (in-place)
I0411 00:00:12.363229 18531 net.cpp:150] Setting up conv2_sTanH
I0411 00:00:12.363245 18531 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:00:12.363250 18531 net.cpp:165] Memory required for data: 3696283648
I0411 00:00:12.363253 18531 layer_factory.hpp:77] Creating layer conv2_postscale
I0411 00:00:12.363260 18531 net.cpp:100] Creating Layer conv2_postscale
I0411 00:00:12.363265 18531 net.cpp:434] conv2_postscale <- conv2
I0411 00:00:12.363270 18531 net.cpp:395] conv2_postscale -> conv2 (in-place)
I0411 00:00:12.363364 18531 net.cpp:150] Setting up conv2_postscale
I0411 00:00:12.363373 18531 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:00:12.363376 18531 net.cpp:165] Memory required for data: 3895349248
I0411 00:00:12.363381 18531 layer_factory.hpp:77] Creating layer pool2
I0411 00:00:12.363387 18531 net.cpp:100] Creating Layer pool2
I0411 00:00:12.363391 18531 net.cpp:434] pool2 <- conv2
I0411 00:00:12.363395 18531 net.cpp:408] pool2 -> pool2
I0411 00:00:12.363435 18531 net.cpp:150] Setting up pool2
I0411 00:00:12.363442 18531 net.cpp:157] Top shape: 1024 150 9 9 (12441600)
I0411 00:00:12.363445 18531 net.cpp:165] Memory required for data: 3945115648
I0411 00:00:12.363448 18531 layer_factory.hpp:77] Creating layer conv3
I0411 00:00:12.363457 18531 net.cpp:100] Creating Layer conv3
I0411 00:00:12.363463 18531 net.cpp:434] conv3 <- pool2
I0411 00:00:12.363468 18531 net.cpp:408] conv3 -> conv3
I0411 00:00:12.369053 18531 net.cpp:150] Setting up conv3
I0411 00:00:12.369086 18531 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:00:12.369091 18531 net.cpp:165] Memory required for data: 3981979648
I0411 00:00:12.369101 18531 layer_factory.hpp:77] Creating layer conv3_prescale
I0411 00:00:12.369108 18531 net.cpp:100] Creating Layer conv3_prescale
I0411 00:00:12.369112 18531 net.cpp:434] conv3_prescale <- conv3
I0411 00:00:12.369122 18531 net.cpp:395] conv3_prescale -> conv3 (in-place)
I0411 00:00:12.369226 18531 net.cpp:150] Setting up conv3_prescale
I0411 00:00:12.369236 18531 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:00:12.369240 18531 net.cpp:165] Memory required for data: 4018843648
I0411 00:00:12.369244 18531 layer_factory.hpp:77] Creating layer conv3_sTanH
I0411 00:00:12.369251 18531 net.cpp:100] Creating Layer conv3_sTanH
I0411 00:00:12.369254 18531 net.cpp:434] conv3_sTanH <- conv3
I0411 00:00:12.369259 18531 net.cpp:395] conv3_sTanH -> conv3 (in-place)
I0411 00:00:12.370033 18531 net.cpp:150] Setting up conv3_sTanH
I0411 00:00:12.370048 18531 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:00:12.370052 18531 net.cpp:165] Memory required for data: 4055707648
I0411 00:00:12.370056 18531 layer_factory.hpp:77] Creating layer conv3_postscale
I0411 00:00:12.370066 18531 net.cpp:100] Creating Layer conv3_postscale
I0411 00:00:12.370070 18531 net.cpp:434] conv3_postscale <- conv3
I0411 00:00:12.370075 18531 net.cpp:395] conv3_postscale -> conv3 (in-place)
I0411 00:00:12.370192 18531 net.cpp:150] Setting up conv3_postscale
I0411 00:00:12.370201 18531 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:00:12.370204 18531 net.cpp:165] Memory required for data: 4092571648
I0411 00:00:12.370210 18531 layer_factory.hpp:77] Creating layer pool3
I0411 00:00:12.370218 18531 net.cpp:100] Creating Layer pool3
I0411 00:00:12.370223 18531 net.cpp:434] pool3 <- conv3
I0411 00:00:12.370230 18531 net.cpp:408] pool3 -> pool3
I0411 00:00:12.370272 18531 net.cpp:150] Setting up pool3
I0411 00:00:12.370281 18531 net.cpp:157] Top shape: 1024 250 3 3 (2304000)
I0411 00:00:12.370286 18531 net.cpp:165] Memory required for data: 4101787648
I0411 00:00:12.370290 18531 layer_factory.hpp:77] Creating layer fc4_300
I0411 00:00:12.370299 18531 net.cpp:100] Creating Layer fc4_300
I0411 00:00:12.370304 18531 net.cpp:434] fc4_300 <- pool3
I0411 00:00:12.370309 18531 net.cpp:408] fc4_300 -> fc4_300
I0411 00:00:12.375696 18531 net.cpp:150] Setting up fc4_300
I0411 00:00:12.375712 18531 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:00:12.375715 18531 net.cpp:165] Memory required for data: 4103016448
I0411 00:00:12.375722 18531 layer_factory.hpp:77] Creating layer fc4_prescale
I0411 00:00:12.375742 18531 net.cpp:100] Creating Layer fc4_prescale
I0411 00:00:12.375747 18531 net.cpp:434] fc4_prescale <- fc4_300
I0411 00:00:12.375756 18531 net.cpp:395] fc4_prescale -> fc4_300 (in-place)
I0411 00:00:12.375844 18531 net.cpp:150] Setting up fc4_prescale
I0411 00:00:12.375854 18531 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:00:12.375856 18531 net.cpp:165] Memory required for data: 4104245248
I0411 00:00:12.375861 18531 layer_factory.hpp:77] Creating layer fc4_sTanH
I0411 00:00:12.375866 18531 net.cpp:100] Creating Layer fc4_sTanH
I0411 00:00:12.375871 18531 net.cpp:434] fc4_sTanH <- fc4_300
I0411 00:00:12.375875 18531 net.cpp:395] fc4_sTanH -> fc4_300 (in-place)
I0411 00:00:12.376075 18531 net.cpp:150] Setting up fc4_sTanH
I0411 00:00:12.376088 18531 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:00:12.376092 18531 net.cpp:165] Memory required for data: 4105474048
I0411 00:00:12.376096 18531 layer_factory.hpp:77] Creating layer fc4_postscale
I0411 00:00:12.376103 18531 net.cpp:100] Creating Layer fc4_postscale
I0411 00:00:12.376107 18531 net.cpp:434] fc4_postscale <- fc4_300
I0411 00:00:12.376114 18531 net.cpp:395] fc4_postscale -> fc4_300 (in-place)
I0411 00:00:12.376222 18531 net.cpp:150] Setting up fc4_postscale
I0411 00:00:12.376231 18531 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:00:12.376235 18531 net.cpp:165] Memory required for data: 4106702848
I0411 00:00:12.376240 18531 layer_factory.hpp:77] Creating layer fc5_67
I0411 00:00:12.376247 18531 net.cpp:100] Creating Layer fc5_67
I0411 00:00:12.376252 18531 net.cpp:434] fc5_67 <- fc4_300
I0411 00:00:12.376260 18531 net.cpp:408] fc5_67 -> fc5_classes
I0411 00:00:12.377511 18531 net.cpp:150] Setting up fc5_67
I0411 00:00:12.377526 18531 net.cpp:157] Top shape: 1024 67 (68608)
I0411 00:00:12.377532 18531 net.cpp:165] Memory required for data: 4106977280
I0411 00:00:12.377542 18531 layer_factory.hpp:77] Creating layer loss
I0411 00:00:12.377552 18531 net.cpp:100] Creating Layer loss
I0411 00:00:12.377557 18531 net.cpp:434] loss <- fc5_classes
I0411 00:00:12.377562 18531 net.cpp:434] loss <- label
I0411 00:00:12.377568 18531 net.cpp:408] loss -> loss
I0411 00:00:12.377581 18531 layer_factory.hpp:77] Creating layer loss
I0411 00:00:12.377920 18531 net.cpp:150] Setting up loss
I0411 00:00:12.377934 18531 net.cpp:157] Top shape: (1)
I0411 00:00:12.377938 18531 net.cpp:160]     with loss weight 1
I0411 00:00:12.377961 18531 net.cpp:165] Memory required for data: 4106977284
I0411 00:00:12.377965 18531 net.cpp:226] loss needs backward computation.
I0411 00:00:12.377974 18531 net.cpp:226] fc5_67 needs backward computation.
I0411 00:00:12.377977 18531 net.cpp:226] fc4_postscale needs backward computation.
I0411 00:00:12.377981 18531 net.cpp:226] fc4_sTanH needs backward computation.
I0411 00:00:12.377985 18531 net.cpp:226] fc4_prescale needs backward computation.
I0411 00:00:12.378000 18531 net.cpp:226] fc4_300 needs backward computation.
I0411 00:00:12.378005 18531 net.cpp:226] pool3 needs backward computation.
I0411 00:00:12.378007 18531 net.cpp:226] conv3_postscale needs backward computation.
I0411 00:00:12.378010 18531 net.cpp:226] conv3_sTanH needs backward computation.
I0411 00:00:12.378013 18531 net.cpp:226] conv3_prescale needs backward computation.
I0411 00:00:12.378016 18531 net.cpp:226] conv3 needs backward computation.
I0411 00:00:12.378021 18531 net.cpp:226] pool2 needs backward computation.
I0411 00:00:12.378023 18531 net.cpp:226] conv2_postscale needs backward computation.
I0411 00:00:12.378026 18531 net.cpp:226] conv2_sTanH needs backward computation.
I0411 00:00:12.378029 18531 net.cpp:226] conv2_prescale needs backward computation.
I0411 00:00:12.378032 18531 net.cpp:226] conv2 needs backward computation.
I0411 00:00:12.378036 18531 net.cpp:226] pool1 needs backward computation.
I0411 00:00:12.378039 18531 net.cpp:226] conv1_postscale needs backward computation.
I0411 00:00:12.378042 18531 net.cpp:226] conv1_sTanH needs backward computation.
I0411 00:00:12.378046 18531 net.cpp:226] conv1_prescale needs backward computation.
I0411 00:00:12.378048 18531 net.cpp:226] conv1 needs backward computation.
I0411 00:00:12.378052 18531 net.cpp:228] data does not need backward computation.
I0411 00:00:12.378056 18531 net.cpp:270] This network produces output loss
I0411 00:00:12.378074 18531 net.cpp:283] Network initialization done.
I0411 00:00:12.378332 18531 solver.cpp:193] Creating test net (#0) specified by test_net file: ./Prototxt/experiment_8/rtsd-r1/CoNorm/trial_1/test.prototxt
I0411 00:00:12.378509 18531 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 48
    mean_value: 133
    mean_value: 133
    mean_value: 132
  }
  data_param {
    source: "../local_data/lmdb/rtsd-r1/CoNorm/test/lmdb"
    batch_size: 1024
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 7
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_prescale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
  }
}
layer {
  name: "conv1_sTanH"
  type: "TanH"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_postscale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 150
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_prescale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
  }
}
layer {
  name: "conv2_sTanH"
  type: "TanH"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_postscale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 250
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_prescale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
  }
}
layer {
  name: "conv3_sTanH"
  type: "TanH"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_postscale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc4_300"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc4_300"
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fc4_prescale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
  }
}
layer {
  name: "fc4_sTanH"
  type: "TanH"
  bottom: "fc4_300"
  top: "fc4_300"
}
layer {
  name: "fc4_postscale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
  }
}
layer {
  name: "fc5_67"
  type: "InnerProduct"
  bottom: "fc4_300"
  top: "fc5_classes"
  inner_product_param {
    num_output: 67
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc5_classes"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy_1"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_1"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 1
  }
}
layer {
  name: "accuracy_5"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0411 00:00:12.378630 18531 layer_factory.hpp:77] Creating layer data
I0411 00:00:12.379530 18531 net.cpp:100] Creating Layer data
I0411 00:00:12.379544 18531 net.cpp:408] data -> data
I0411 00:00:12.379565 18531 net.cpp:408] data -> label
I0411 00:00:12.380647 18667 db_lmdb.cpp:35] Opened lmdb ../local_data/lmdb/rtsd-r1/CoNorm/test/lmdb
I0411 00:00:12.380789 18531 data_layer.cpp:41] output data size: 1024,3,48,48
I0411 00:00:12.438446 18531 net.cpp:150] Setting up data
I0411 00:00:12.438472 18531 net.cpp:157] Top shape: 1024 3 48 48 (7077888)
I0411 00:00:12.438478 18531 net.cpp:157] Top shape: 1024 (1024)
I0411 00:00:12.438482 18531 net.cpp:165] Memory required for data: 28315648
I0411 00:00:12.438488 18531 layer_factory.hpp:77] Creating layer label_data_1_split
I0411 00:00:12.438503 18531 net.cpp:100] Creating Layer label_data_1_split
I0411 00:00:12.438508 18531 net.cpp:434] label_data_1_split <- label
I0411 00:00:12.438515 18531 net.cpp:408] label_data_1_split -> label_data_1_split_0
I0411 00:00:12.438527 18531 net.cpp:408] label_data_1_split -> label_data_1_split_1
I0411 00:00:12.438535 18531 net.cpp:408] label_data_1_split -> label_data_1_split_2
I0411 00:00:12.438639 18531 net.cpp:150] Setting up label_data_1_split
I0411 00:00:12.438648 18531 net.cpp:157] Top shape: 1024 (1024)
I0411 00:00:12.438652 18531 net.cpp:157] Top shape: 1024 (1024)
I0411 00:00:12.438657 18531 net.cpp:157] Top shape: 1024 (1024)
I0411 00:00:12.438658 18531 net.cpp:165] Memory required for data: 28327936
I0411 00:00:12.438663 18531 layer_factory.hpp:77] Creating layer conv1
I0411 00:00:12.438678 18531 net.cpp:100] Creating Layer conv1
I0411 00:00:12.438683 18531 net.cpp:434] conv1 <- data
I0411 00:00:12.438693 18531 net.cpp:408] conv1 -> conv1
I0411 00:00:12.442201 18531 net.cpp:150] Setting up conv1
I0411 00:00:12.442221 18531 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:00:12.442225 18531 net.cpp:165] Memory required for data: 750862336
I0411 00:00:12.442237 18531 layer_factory.hpp:77] Creating layer conv1_prescale
I0411 00:00:12.442250 18531 net.cpp:100] Creating Layer conv1_prescale
I0411 00:00:12.442255 18531 net.cpp:434] conv1_prescale <- conv1
I0411 00:00:12.442260 18531 net.cpp:395] conv1_prescale -> conv1 (in-place)
I0411 00:00:12.442396 18531 net.cpp:150] Setting up conv1_prescale
I0411 00:00:12.442406 18531 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:00:12.442409 18531 net.cpp:165] Memory required for data: 1473396736
I0411 00:00:12.442417 18531 layer_factory.hpp:77] Creating layer conv1_sTanH
I0411 00:00:12.442428 18531 net.cpp:100] Creating Layer conv1_sTanH
I0411 00:00:12.442433 18531 net.cpp:434] conv1_sTanH <- conv1
I0411 00:00:12.442438 18531 net.cpp:395] conv1_sTanH -> conv1 (in-place)
I0411 00:00:12.444952 18531 net.cpp:150] Setting up conv1_sTanH
I0411 00:00:12.444975 18531 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:00:12.444979 18531 net.cpp:165] Memory required for data: 2195931136
I0411 00:00:12.444983 18531 layer_factory.hpp:77] Creating layer conv1_postscale
I0411 00:00:12.444991 18531 net.cpp:100] Creating Layer conv1_postscale
I0411 00:00:12.444996 18531 net.cpp:434] conv1_postscale <- conv1
I0411 00:00:12.445004 18531 net.cpp:395] conv1_postscale -> conv1 (in-place)
I0411 00:00:12.445132 18531 net.cpp:150] Setting up conv1_postscale
I0411 00:00:12.445143 18531 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0411 00:00:12.445147 18531 net.cpp:165] Memory required for data: 2918465536
I0411 00:00:12.445152 18531 layer_factory.hpp:77] Creating layer pool1
I0411 00:00:12.445160 18531 net.cpp:100] Creating Layer pool1
I0411 00:00:12.445163 18531 net.cpp:434] pool1 <- conv1
I0411 00:00:12.445170 18531 net.cpp:408] pool1 -> pool1
I0411 00:00:12.445217 18531 net.cpp:150] Setting up pool1
I0411 00:00:12.445230 18531 net.cpp:157] Top shape: 1024 100 21 21 (45158400)
I0411 00:00:12.445233 18531 net.cpp:165] Memory required for data: 3099099136
I0411 00:00:12.445236 18531 layer_factory.hpp:77] Creating layer conv2
I0411 00:00:12.445246 18531 net.cpp:100] Creating Layer conv2
I0411 00:00:12.445251 18531 net.cpp:434] conv2 <- pool1
I0411 00:00:12.445257 18531 net.cpp:408] conv2 -> conv2
I0411 00:00:12.451841 18531 net.cpp:150] Setting up conv2
I0411 00:00:12.451861 18531 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:00:12.451865 18531 net.cpp:165] Memory required for data: 3298164736
I0411 00:00:12.451889 18531 layer_factory.hpp:77] Creating layer conv2_prescale
I0411 00:00:12.451903 18531 net.cpp:100] Creating Layer conv2_prescale
I0411 00:00:12.451908 18531 net.cpp:434] conv2_prescale <- conv2
I0411 00:00:12.451915 18531 net.cpp:395] conv2_prescale -> conv2 (in-place)
I0411 00:00:12.452029 18531 net.cpp:150] Setting up conv2_prescale
I0411 00:00:12.452040 18531 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:00:12.452044 18531 net.cpp:165] Memory required for data: 3497230336
I0411 00:00:12.452049 18531 layer_factory.hpp:77] Creating layer conv2_sTanH
I0411 00:00:12.452055 18531 net.cpp:100] Creating Layer conv2_sTanH
I0411 00:00:12.452060 18531 net.cpp:434] conv2_sTanH <- conv2
I0411 00:00:12.452067 18531 net.cpp:395] conv2_sTanH -> conv2 (in-place)
I0411 00:00:12.453699 18531 net.cpp:150] Setting up conv2_sTanH
I0411 00:00:12.453719 18531 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:00:12.453723 18531 net.cpp:165] Memory required for data: 3696295936
I0411 00:00:12.453727 18531 layer_factory.hpp:77] Creating layer conv2_postscale
I0411 00:00:12.453740 18531 net.cpp:100] Creating Layer conv2_postscale
I0411 00:00:12.453745 18531 net.cpp:434] conv2_postscale <- conv2
I0411 00:00:12.453750 18531 net.cpp:395] conv2_postscale -> conv2 (in-place)
I0411 00:00:12.453861 18531 net.cpp:150] Setting up conv2_postscale
I0411 00:00:12.453871 18531 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0411 00:00:12.453874 18531 net.cpp:165] Memory required for data: 3895361536
I0411 00:00:12.453881 18531 layer_factory.hpp:77] Creating layer pool2
I0411 00:00:12.453896 18531 net.cpp:100] Creating Layer pool2
I0411 00:00:12.453902 18531 net.cpp:434] pool2 <- conv2
I0411 00:00:12.453909 18531 net.cpp:408] pool2 -> pool2
I0411 00:00:12.453974 18531 net.cpp:150] Setting up pool2
I0411 00:00:12.453982 18531 net.cpp:157] Top shape: 1024 150 9 9 (12441600)
I0411 00:00:12.454000 18531 net.cpp:165] Memory required for data: 3945127936
I0411 00:00:12.454005 18531 layer_factory.hpp:77] Creating layer conv3
I0411 00:00:12.454017 18531 net.cpp:100] Creating Layer conv3
I0411 00:00:12.454022 18531 net.cpp:434] conv3 <- pool2
I0411 00:00:12.454028 18531 net.cpp:408] conv3 -> conv3
I0411 00:00:12.460141 18531 net.cpp:150] Setting up conv3
I0411 00:00:12.460160 18531 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:00:12.460165 18531 net.cpp:165] Memory required for data: 3981991936
I0411 00:00:12.460176 18531 layer_factory.hpp:77] Creating layer conv3_prescale
I0411 00:00:12.460186 18531 net.cpp:100] Creating Layer conv3_prescale
I0411 00:00:12.460189 18531 net.cpp:434] conv3_prescale <- conv3
I0411 00:00:12.460194 18531 net.cpp:395] conv3_prescale -> conv3 (in-place)
I0411 00:00:12.460299 18531 net.cpp:150] Setting up conv3_prescale
I0411 00:00:12.460309 18531 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:00:12.460312 18531 net.cpp:165] Memory required for data: 4018855936
I0411 00:00:12.460317 18531 layer_factory.hpp:77] Creating layer conv3_sTanH
I0411 00:00:12.460325 18531 net.cpp:100] Creating Layer conv3_sTanH
I0411 00:00:12.460330 18531 net.cpp:434] conv3_sTanH <- conv3
I0411 00:00:12.460335 18531 net.cpp:395] conv3_sTanH -> conv3 (in-place)
I0411 00:00:12.461125 18531 net.cpp:150] Setting up conv3_sTanH
I0411 00:00:12.461143 18531 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:00:12.461146 18531 net.cpp:165] Memory required for data: 4055719936
I0411 00:00:12.461151 18531 layer_factory.hpp:77] Creating layer conv3_postscale
I0411 00:00:12.461163 18531 net.cpp:100] Creating Layer conv3_postscale
I0411 00:00:12.461168 18531 net.cpp:434] conv3_postscale <- conv3
I0411 00:00:12.461175 18531 net.cpp:395] conv3_postscale -> conv3 (in-place)
I0411 00:00:12.461280 18531 net.cpp:150] Setting up conv3_postscale
I0411 00:00:12.461292 18531 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0411 00:00:12.461295 18531 net.cpp:165] Memory required for data: 4092583936
I0411 00:00:12.461300 18531 layer_factory.hpp:77] Creating layer pool3
I0411 00:00:12.461313 18531 net.cpp:100] Creating Layer pool3
I0411 00:00:12.461318 18531 net.cpp:434] pool3 <- conv3
I0411 00:00:12.461324 18531 net.cpp:408] pool3 -> pool3
I0411 00:00:12.461366 18531 net.cpp:150] Setting up pool3
I0411 00:00:12.461375 18531 net.cpp:157] Top shape: 1024 250 3 3 (2304000)
I0411 00:00:12.461380 18531 net.cpp:165] Memory required for data: 4101799936
I0411 00:00:12.461385 18531 layer_factory.hpp:77] Creating layer fc4_300
I0411 00:00:12.461390 18531 net.cpp:100] Creating Layer fc4_300
I0411 00:00:12.461395 18531 net.cpp:434] fc4_300 <- pool3
I0411 00:00:12.461400 18531 net.cpp:408] fc4_300 -> fc4_300
I0411 00:00:12.466820 18531 net.cpp:150] Setting up fc4_300
I0411 00:00:12.466840 18531 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:00:12.466845 18531 net.cpp:165] Memory required for data: 4103028736
I0411 00:00:12.466852 18531 layer_factory.hpp:77] Creating layer fc4_prescale
I0411 00:00:12.466866 18531 net.cpp:100] Creating Layer fc4_prescale
I0411 00:00:12.466871 18531 net.cpp:434] fc4_prescale <- fc4_300
I0411 00:00:12.466876 18531 net.cpp:395] fc4_prescale -> fc4_300 (in-place)
I0411 00:00:12.466972 18531 net.cpp:150] Setting up fc4_prescale
I0411 00:00:12.466981 18531 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:00:12.466984 18531 net.cpp:165] Memory required for data: 4104257536
I0411 00:00:12.466989 18531 layer_factory.hpp:77] Creating layer fc4_sTanH
I0411 00:00:12.466996 18531 net.cpp:100] Creating Layer fc4_sTanH
I0411 00:00:12.467001 18531 net.cpp:434] fc4_sTanH <- fc4_300
I0411 00:00:12.467006 18531 net.cpp:395] fc4_sTanH -> fc4_300 (in-place)
I0411 00:00:12.467208 18531 net.cpp:150] Setting up fc4_sTanH
I0411 00:00:12.467221 18531 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:00:12.467226 18531 net.cpp:165] Memory required for data: 4105486336
I0411 00:00:12.467231 18531 layer_factory.hpp:77] Creating layer fc4_postscale
I0411 00:00:12.467238 18531 net.cpp:100] Creating Layer fc4_postscale
I0411 00:00:12.467257 18531 net.cpp:434] fc4_postscale <- fc4_300
I0411 00:00:12.467263 18531 net.cpp:395] fc4_postscale -> fc4_300 (in-place)
I0411 00:00:12.467371 18531 net.cpp:150] Setting up fc4_postscale
I0411 00:00:12.467381 18531 net.cpp:157] Top shape: 1024 300 (307200)
I0411 00:00:12.467384 18531 net.cpp:165] Memory required for data: 4106715136
I0411 00:00:12.467389 18531 layer_factory.hpp:77] Creating layer fc5_67
I0411 00:00:12.467398 18531 net.cpp:100] Creating Layer fc5_67
I0411 00:00:12.467403 18531 net.cpp:434] fc5_67 <- fc4_300
I0411 00:00:12.467411 18531 net.cpp:408] fc5_67 -> fc5_classes
I0411 00:00:12.467667 18531 net.cpp:150] Setting up fc5_67
I0411 00:00:12.467677 18531 net.cpp:157] Top shape: 1024 67 (68608)
I0411 00:00:12.467679 18531 net.cpp:165] Memory required for data: 4106989568
I0411 00:00:12.467692 18531 layer_factory.hpp:77] Creating layer fc5_classes_fc5_67_0_split
I0411 00:00:12.467701 18531 net.cpp:100] Creating Layer fc5_classes_fc5_67_0_split
I0411 00:00:12.467705 18531 net.cpp:434] fc5_classes_fc5_67_0_split <- fc5_classes
I0411 00:00:12.467710 18531 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_0
I0411 00:00:12.467717 18531 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_1
I0411 00:00:12.467732 18531 net.cpp:408] fc5_classes_fc5_67_0_split -> fc5_classes_fc5_67_0_split_2
I0411 00:00:12.467788 18531 net.cpp:150] Setting up fc5_classes_fc5_67_0_split
I0411 00:00:12.467797 18531 net.cpp:157] Top shape: 1024 67 (68608)
I0411 00:00:12.467799 18531 net.cpp:157] Top shape: 1024 67 (68608)
I0411 00:00:12.467803 18531 net.cpp:157] Top shape: 1024 67 (68608)
I0411 00:00:12.467805 18531 net.cpp:165] Memory required for data: 4107812864
I0411 00:00:12.467808 18531 layer_factory.hpp:77] Creating layer loss
I0411 00:00:12.467816 18531 net.cpp:100] Creating Layer loss
I0411 00:00:12.467820 18531 net.cpp:434] loss <- fc5_classes_fc5_67_0_split_0
I0411 00:00:12.467825 18531 net.cpp:434] loss <- label_data_1_split_0
I0411 00:00:12.467829 18531 net.cpp:408] loss -> loss
I0411 00:00:12.467841 18531 layer_factory.hpp:77] Creating layer loss
I0411 00:00:12.468206 18531 net.cpp:150] Setting up loss
I0411 00:00:12.468219 18531 net.cpp:157] Top shape: (1)
I0411 00:00:12.468230 18531 net.cpp:160]     with loss weight 1
I0411 00:00:12.468240 18531 net.cpp:165] Memory required for data: 4107812868
I0411 00:00:12.468243 18531 layer_factory.hpp:77] Creating layer accuracy_1
I0411 00:00:12.468252 18531 net.cpp:100] Creating Layer accuracy_1
I0411 00:00:12.468255 18531 net.cpp:434] accuracy_1 <- fc5_classes_fc5_67_0_split_1
I0411 00:00:12.468261 18531 net.cpp:434] accuracy_1 <- label_data_1_split_1
I0411 00:00:12.468268 18531 net.cpp:408] accuracy_1 -> accuracy_1
I0411 00:00:12.468282 18531 net.cpp:150] Setting up accuracy_1
I0411 00:00:12.468296 18531 net.cpp:157] Top shape: (1)
I0411 00:00:12.468298 18531 net.cpp:165] Memory required for data: 4107812872
I0411 00:00:12.468302 18531 layer_factory.hpp:77] Creating layer accuracy_5
I0411 00:00:12.468308 18531 net.cpp:100] Creating Layer accuracy_5
I0411 00:00:12.468314 18531 net.cpp:434] accuracy_5 <- fc5_classes_fc5_67_0_split_2
I0411 00:00:12.468318 18531 net.cpp:434] accuracy_5 <- label_data_1_split_2
I0411 00:00:12.468325 18531 net.cpp:408] accuracy_5 -> accuracy_5
I0411 00:00:12.468333 18531 net.cpp:150] Setting up accuracy_5
I0411 00:00:12.468338 18531 net.cpp:157] Top shape: (1)
I0411 00:00:12.468341 18531 net.cpp:165] Memory required for data: 4107812876
I0411 00:00:12.468353 18531 net.cpp:228] accuracy_5 does not need backward computation.
I0411 00:00:12.468356 18531 net.cpp:228] accuracy_1 does not need backward computation.
I0411 00:00:12.468360 18531 net.cpp:226] loss needs backward computation.
I0411 00:00:12.468365 18531 net.cpp:226] fc5_classes_fc5_67_0_split needs backward computation.
I0411 00:00:12.468369 18531 net.cpp:226] fc5_67 needs backward computation.
I0411 00:00:12.468379 18531 net.cpp:226] fc4_postscale needs backward computation.
I0411 00:00:12.468381 18531 net.cpp:226] fc4_sTanH needs backward computation.
I0411 00:00:12.468395 18531 net.cpp:226] fc4_prescale needs backward computation.
I0411 00:00:12.468399 18531 net.cpp:226] fc4_300 needs backward computation.
I0411 00:00:12.468405 18531 net.cpp:226] pool3 needs backward computation.
I0411 00:00:12.468407 18531 net.cpp:226] conv3_postscale needs backward computation.
I0411 00:00:12.468411 18531 net.cpp:226] conv3_sTanH needs backward computation.
I0411 00:00:12.468415 18531 net.cpp:226] conv3_prescale needs backward computation.
I0411 00:00:12.468417 18531 net.cpp:226] conv3 needs backward computation.
I0411 00:00:12.468420 18531 net.cpp:226] pool2 needs backward computation.
I0411 00:00:12.468425 18531 net.cpp:226] conv2_postscale needs backward computation.
I0411 00:00:12.468427 18531 net.cpp:226] conv2_sTanH needs backward computation.
I0411 00:00:12.468431 18531 net.cpp:226] conv2_prescale needs backward computation.
I0411 00:00:12.468435 18531 net.cpp:226] conv2 needs backward computation.
I0411 00:00:12.468437 18531 net.cpp:226] pool1 needs backward computation.
I0411 00:00:12.468446 18531 net.cpp:226] conv1_postscale needs backward computation.
I0411 00:00:12.468448 18531 net.cpp:226] conv1_sTanH needs backward computation.
I0411 00:00:12.468451 18531 net.cpp:226] conv1_prescale needs backward computation.
I0411 00:00:12.468456 18531 net.cpp:226] conv1 needs backward computation.
I0411 00:00:12.468459 18531 net.cpp:228] label_data_1_split does not need backward computation.
I0411 00:00:12.468463 18531 net.cpp:228] data does not need backward computation.
I0411 00:00:12.468466 18531 net.cpp:270] This network produces output accuracy_1
I0411 00:00:12.468471 18531 net.cpp:270] This network produces output accuracy_5
I0411 00:00:12.468480 18531 net.cpp:270] This network produces output loss
I0411 00:00:12.468502 18531 net.cpp:283] Network initialization done.
I0411 00:00:12.468583 18531 solver.cpp:72] Solver scaffolding done.
I0411 00:00:12.469485 18531 caffe.cpp:251] Starting Optimization
I0411 00:00:12.469494 18531 solver.cpp:291] Solving 
I0411 00:00:12.469497 18531 solver.cpp:292] Learning Rate Policy: step
I0411 00:00:12.472674 18531 solver.cpp:349] Iteration 0, Testing net (#0)
I0411 00:00:13.568253 18531 solver.cpp:416]     Test net output #0: accuracy_1 = 0.00817871
I0411 00:00:13.568281 18531 solver.cpp:416]     Test net output #1: accuracy_5 = 0.0500488
I0411 00:00:13.568292 18531 solver.cpp:416]     Test net output #2: loss = 4.54918 (* 1 = 4.54918 loss)
I0411 00:00:13.716012 18531 solver.cpp:240] Iteration 0, loss = 4.57429
I0411 00:00:13.716049 18531 solver.cpp:256]     Train net output #0: loss = 4.57429 (* 1 = 4.57429 loss)
I0411 00:00:13.716063 18531 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0411 00:00:14.090591 18531 solver.cpp:240] Iteration 1, loss = 4.19879
I0411 00:00:14.090626 18531 solver.cpp:256]     Train net output #0: loss = 4.19879 (* 1 = 4.19879 loss)
I0411 00:00:14.090634 18531 sgd_solver.cpp:106] Iteration 1, lr = 0.001
I0411 00:00:14.462152 18531 solver.cpp:240] Iteration 2, loss = 4.46532
I0411 00:00:14.462184 18531 solver.cpp:256]     Train net output #0: loss = 4.46532 (* 1 = 4.46532 loss)
I0411 00:00:14.462193 18531 sgd_solver.cpp:106] Iteration 2, lr = 0.001
I0411 00:00:14.839495 18531 solver.cpp:240] Iteration 3, loss = 5.06407
I0411 00:00:14.839529 18531 solver.cpp:256]     Train net output #0: loss = 5.06407 (* 1 = 5.06407 loss)
I0411 00:00:14.839537 18531 sgd_solver.cpp:106] Iteration 3, lr = 0.001
I0411 00:00:15.211230 18531 solver.cpp:240] Iteration 4, loss = 4.73418
I0411 00:00:15.211269 18531 solver.cpp:256]     Train net output #0: loss = 4.73418 (* 1 = 4.73418 loss)
I0411 00:00:15.211280 18531 sgd_solver.cpp:106] Iteration 4, lr = 0.001
I0411 00:00:15.579478 18531 solver.cpp:240] Iteration 5, loss = 5.03187
I0411 00:00:15.579509 18531 solver.cpp:256]     Train net output #0: loss = 5.03187 (* 1 = 5.03187 loss)
I0411 00:00:15.579519 18531 sgd_solver.cpp:106] Iteration 5, lr = 0.001
I0411 00:00:15.950155 18531 solver.cpp:240] Iteration 6, loss = 5.72569
I0411 00:00:15.950199 18531 solver.cpp:256]     Train net output #0: loss = 5.72569 (* 1 = 5.72569 loss)
I0411 00:00:15.950229 18531 sgd_solver.cpp:106] Iteration 6, lr = 0.001
I0411 00:00:16.317749 18531 solver.cpp:240] Iteration 7, loss = 5.57087
I0411 00:00:16.317790 18531 solver.cpp:256]     Train net output #0: loss = 5.57087 (* 1 = 5.57087 loss)
I0411 00:00:16.317797 18531 sgd_solver.cpp:106] Iteration 7, lr = 0.001
I0411 00:00:16.688681 18531 solver.cpp:240] Iteration 8, loss = 5.57662
I0411 00:00:16.688716 18531 solver.cpp:256]     Train net output #0: loss = 5.57662 (* 1 = 5.57662 loss)
I0411 00:00:16.688724 18531 sgd_solver.cpp:106] Iteration 8, lr = 0.001
I0411 00:00:17.060050 18531 solver.cpp:240] Iteration 9, loss = 5.21935
I0411 00:00:17.060084 18531 solver.cpp:256]     Train net output #0: loss = 5.21935 (* 1 = 5.21935 loss)
I0411 00:00:17.060092 18531 sgd_solver.cpp:106] Iteration 9, lr = 0.001
I0411 00:00:17.429786 18531 solver.cpp:240] Iteration 10, loss = 4.68763
I0411 00:00:17.429821 18531 solver.cpp:256]     Train net output #0: loss = 4.68763 (* 1 = 4.68763 loss)
I0411 00:00:17.429828 18531 sgd_solver.cpp:106] Iteration 10, lr = 0.001
I0411 00:00:17.801196 18531 solver.cpp:240] Iteration 11, loss = 4.68195
I0411 00:00:17.801239 18531 solver.cpp:256]     Train net output #0: loss = 4.68195 (* 1 = 4.68195 loss)
I0411 00:00:17.801245 18531 sgd_solver.cpp:106] Iteration 11, lr = 0.001
I0411 00:00:18.172302 18531 solver.cpp:240] Iteration 12, loss = 4.73091
I0411 00:00:18.172343 18531 solver.cpp:256]     Train net output #0: loss = 4.73091 (* 1 = 4.73091 loss)
I0411 00:00:18.172351 18531 sgd_solver.cpp:106] Iteration 12, lr = 0.001
I0411 00:00:18.546816 18531 solver.cpp:240] Iteration 13, loss = 4.54648
I0411 00:00:18.546859 18531 solver.cpp:256]     Train net output #0: loss = 4.54648 (* 1 = 4.54648 loss)
I0411 00:00:18.546866 18531 sgd_solver.cpp:106] Iteration 13, lr = 0.001
I0411 00:00:18.916851 18531 solver.cpp:240] Iteration 14, loss = 4.55487
I0411 00:00:18.916883 18531 solver.cpp:256]     Train net output #0: loss = 4.55487 (* 1 = 4.55487 loss)
I0411 00:00:18.916893 18531 sgd_solver.cpp:106] Iteration 14, lr = 0.001
I0411 00:00:19.287039 18531 solver.cpp:240] Iteration 15, loss = 4.61384
I0411 00:00:19.287070 18531 solver.cpp:256]     Train net output #0: loss = 4.61384 (* 1 = 4.61384 loss)
I0411 00:00:19.287077 18531 sgd_solver.cpp:106] Iteration 15, lr = 0.001
I0411 00:00:19.660238 18531 solver.cpp:240] Iteration 16, loss = 4.91875
I0411 00:00:19.660269 18531 solver.cpp:256]     Train net output #0: loss = 4.91875 (* 1 = 4.91875 loss)
I0411 00:00:19.660276 18531 sgd_solver.cpp:106] Iteration 16, lr = 0.001
I0411 00:00:20.031889 18531 solver.cpp:240] Iteration 17, loss = 4.96115
I0411 00:00:20.031921 18531 solver.cpp:256]     Train net output #0: loss = 4.96115 (* 1 = 4.96115 loss)
I0411 00:00:20.031929 18531 sgd_solver.cpp:106] Iteration 17, lr = 0.001
I0411 00:00:20.402137 18531 solver.cpp:240] Iteration 18, loss = 5.46146
I0411 00:00:20.402173 18531 solver.cpp:256]     Train net output #0: loss = 5.46146 (* 1 = 5.46146 loss)
I0411 00:00:20.402181 18531 sgd_solver.cpp:106] Iteration 18, lr = 0.001
I0411 00:00:20.772330 18531 solver.cpp:240] Iteration 19, loss = 5.6997
I0411 00:00:20.772378 18531 solver.cpp:256]     Train net output #0: loss = 5.6997 (* 1 = 5.6997 loss)
I0411 00:00:20.772387 18531 sgd_solver.cpp:106] Iteration 19, lr = 0.001
I0411 00:00:21.140931 18531 solver.cpp:240] Iteration 20, loss = 5.37397
I0411 00:00:21.140964 18531 solver.cpp:256]     Train net output #0: loss = 5.37397 (* 1 = 5.37397 loss)
I0411 00:00:21.140972 18531 sgd_solver.cpp:106] Iteration 20, lr = 0.001
I0411 00:00:21.516335 18531 solver.cpp:240] Iteration 21, loss = 5.48302
I0411 00:00:21.516381 18531 solver.cpp:256]     Train net output #0: loss = 5.48302 (* 1 = 5.48302 loss)
I0411 00:00:21.516391 18531 sgd_solver.cpp:106] Iteration 21, lr = 0.001
I0411 00:00:21.888387 18531 solver.cpp:240] Iteration 22, loss = 6.00294
I0411 00:00:21.888419 18531 solver.cpp:256]     Train net output #0: loss = 6.00294 (* 1 = 6.00294 loss)
I0411 00:00:21.888427 18531 sgd_solver.cpp:106] Iteration 22, lr = 0.001
I0411 00:00:22.256552 18531 solver.cpp:240] Iteration 23, loss = 5.57675
I0411 00:00:22.256583 18531 solver.cpp:256]     Train net output #0: loss = 5.57675 (* 1 = 5.57675 loss)
I0411 00:00:22.256592 18531 sgd_solver.cpp:106] Iteration 23, lr = 0.001
I0411 00:00:22.627938 18531 solver.cpp:240] Iteration 24, loss = 5.18953
I0411 00:00:22.627970 18531 solver.cpp:256]     Train net output #0: loss = 5.18953 (* 1 = 5.18953 loss)
I0411 00:00:22.627979 18531 sgd_solver.cpp:106] Iteration 24, lr = 0.001
I0411 00:00:22.628295 18531 solver.cpp:349] Iteration 25, Testing net (#0)
I0411 00:00:23.912026 18531 solver.cpp:416]     Test net output #0: accuracy_1 = 0.0716553
I0411 00:00:23.912055 18531 solver.cpp:416]     Test net output #1: accuracy_5 = 0.102905
I0411 00:00:23.912065 18531 solver.cpp:416]     Test net output #2: loss = 5.40476 (* 1 = 5.40476 loss)
I0411 00:00:24.040282 18531 solver.cpp:240] Iteration 25, loss = 5.16908
I0411 00:00:24.040321 18531 solver.cpp:256]     Train net output #0: loss = 5.16908 (* 1 = 5.16908 loss)
I0411 00:00:24.040329 18531 sgd_solver.cpp:106] Iteration 25, lr = 0.001
I0411 00:00:24.410289 18531 solver.cpp:240] Iteration 26, loss = 5.09857
I0411 00:00:24.410332 18531 solver.cpp:256]     Train net output #0: loss = 5.09857 (* 1 = 5.09857 loss)
I0411 00:00:24.410341 18531 sgd_solver.cpp:106] Iteration 26, lr = 0.001
I0411 00:00:24.781167 18531 solver.cpp:240] Iteration 27, loss = 5.16915
I0411 00:00:24.781199 18531 solver.cpp:256]     Train net output #0: loss = 5.16915 (* 1 = 5.16915 loss)
I0411 00:00:24.781208 18531 sgd_solver.cpp:106] Iteration 27, lr = 0.001
I0411 00:00:25.151448 18531 solver.cpp:240] Iteration 28, loss = 5.05711
I0411 00:00:25.151491 18531 solver.cpp:256]     Train net output #0: loss = 5.05711 (* 1 = 5.05711 loss)
I0411 00:00:25.151499 18531 sgd_solver.cpp:106] Iteration 28, lr = 0.001
I0411 00:00:25.518566 18531 solver.cpp:240] Iteration 29, loss = 4.98552
I0411 00:00:25.518597 18531 solver.cpp:256]     Train net output #0: loss = 4.98552 (* 1 = 4.98552 loss)
I0411 00:00:25.518606 18531 sgd_solver.cpp:106] Iteration 29, lr = 0.001
I0411 00:00:25.889699 18531 solver.cpp:240] Iteration 30, loss = 4.99507
I0411 00:00:25.889732 18531 solver.cpp:256]     Train net output #0: loss = 4.99507 (* 1 = 4.99507 loss)
I0411 00:00:25.889740 18531 sgd_solver.cpp:106] Iteration 30, lr = 0.001
I0411 00:00:26.260638 18531 solver.cpp:240] Iteration 31, loss = 5.11656
I0411 00:00:26.260680 18531 solver.cpp:256]     Train net output #0: loss = 5.11656 (* 1 = 5.11656 loss)
I0411 00:00:26.260687 18531 sgd_solver.cpp:106] Iteration 31, lr = 0.001
I0411 00:00:26.628861 18531 solver.cpp:240] Iteration 32, loss = 5.22186
I0411 00:00:26.628895 18531 solver.cpp:256]     Train net output #0: loss = 5.22186 (* 1 = 5.22186 loss)
I0411 00:00:26.628902 18531 sgd_solver.cpp:106] Iteration 32, lr = 0.001
I0411 00:00:26.998780 18531 solver.cpp:240] Iteration 33, loss = 5.35511
I0411 00:00:26.998826 18531 solver.cpp:256]     Train net output #0: loss = 5.35511 (* 1 = 5.35511 loss)
I0411 00:00:26.998834 18531 sgd_solver.cpp:106] Iteration 33, lr = 0.001
I0411 00:00:27.367761 18531 solver.cpp:240] Iteration 34, loss = 5.43126
I0411 00:00:27.367794 18531 solver.cpp:256]     Train net output #0: loss = 5.43126 (* 1 = 5.43126 loss)
I0411 00:00:27.367802 18531 sgd_solver.cpp:106] Iteration 34, lr = 0.001
I0411 00:00:27.742686 18531 solver.cpp:240] Iteration 35, loss = 5.48078
I0411 00:00:27.742730 18531 solver.cpp:256]     Train net output #0: loss = 5.48078 (* 1 = 5.48078 loss)
I0411 00:00:27.742738 18531 sgd_solver.cpp:106] Iteration 35, lr = 0.001
I0411 00:00:28.113281 18531 solver.cpp:240] Iteration 36, loss = 5.466
I0411 00:00:28.113314 18531 solver.cpp:256]     Train net output #0: loss = 5.466 (* 1 = 5.466 loss)
I0411 00:00:28.113323 18531 sgd_solver.cpp:106] Iteration 36, lr = 0.001
I0411 00:00:28.485998 18531 solver.cpp:240] Iteration 37, loss = 5.8102
I0411 00:00:28.486042 18531 solver.cpp:256]     Train net output #0: loss = 5.8102 (* 1 = 5.8102 loss)
I0411 00:00:28.486083 18531 sgd_solver.cpp:106] Iteration 37, lr = 0.001
I0411 00:00:28.857404 18531 solver.cpp:240] Iteration 38, loss = 6.2593
I0411 00:00:28.857482 18531 solver.cpp:256]     Train net output #0: loss = 6.2593 (* 1 = 6.2593 loss)
I0411 00:00:28.857511 18531 sgd_solver.cpp:106] Iteration 38, lr = 0.001
I0411 00:00:29.233094 18531 solver.cpp:240] Iteration 39, loss = 6.7026
I0411 00:00:29.233137 18531 solver.cpp:256]     Train net output #0: loss = 6.7026 (* 1 = 6.7026 loss)
I0411 00:00:29.233145 18531 sgd_solver.cpp:106] Iteration 39, lr = 0.001
I0411 00:00:29.604045 18531 solver.cpp:240] Iteration 40, loss = 7.62209
I0411 00:00:29.604080 18531 solver.cpp:256]     Train net output #0: loss = 7.62209 (* 1 = 7.62209 loss)
I0411 00:00:29.604089 18531 sgd_solver.cpp:106] Iteration 40, lr = 0.001
I0411 00:00:29.977007 18531 solver.cpp:240] Iteration 41, loss = 9.30024
I0411 00:00:29.977052 18531 solver.cpp:256]     Train net output #0: loss = 9.30024 (* 1 = 9.30024 loss)
I0411 00:00:29.977061 18531 sgd_solver.cpp:106] Iteration 41, lr = 0.001
I0411 00:00:30.348039 18531 solver.cpp:240] Iteration 42, loss = 10.5061
I0411 00:00:30.348075 18531 solver.cpp:256]     Train net output #0: loss = 10.5061 (* 1 = 10.5061 loss)
I0411 00:00:30.348083 18531 sgd_solver.cpp:106] Iteration 42, lr = 0.001
I0411 00:00:30.724665 18531 solver.cpp:240] Iteration 43, loss = 10.4091
I0411 00:00:30.724711 18531 solver.cpp:256]     Train net output #0: loss = 10.4091 (* 1 = 10.4091 loss)
I0411 00:00:30.724720 18531 sgd_solver.cpp:106] Iteration 43, lr = 0.001
I0411 00:00:31.099872 18531 solver.cpp:240] Iteration 44, loss = 11.6053
I0411 00:00:31.099938 18531 solver.cpp:256]     Train net output #0: loss = 11.6053 (* 1 = 11.6053 loss)
I0411 00:00:31.099947 18531 sgd_solver.cpp:106] Iteration 44, lr = 0.001
I0411 00:00:31.471199 18531 solver.cpp:240] Iteration 45, loss = 12.327
I0411 00:00:31.471246 18531 solver.cpp:256]     Train net output #0: loss = 12.327 (* 1 = 12.327 loss)
I0411 00:00:31.471254 18531 sgd_solver.cpp:106] Iteration 45, lr = 0.001
I0411 00:00:31.843011 18531 solver.cpp:240] Iteration 46, loss = 11.8973
I0411 00:00:31.843046 18531 solver.cpp:256]     Train net output #0: loss = 11.8973 (* 1 = 11.8973 loss)
I0411 00:00:31.843055 18531 sgd_solver.cpp:106] Iteration 46, lr = 0.001
I0411 00:00:32.219549 18531 solver.cpp:240] Iteration 47, loss = 9.61381
I0411 00:00:32.219583 18531 solver.cpp:256]     Train net output #0: loss = 9.61381 (* 1 = 9.61381 loss)
I0411 00:00:32.219593 18531 sgd_solver.cpp:106] Iteration 47, lr = 0.001
I0411 00:00:32.593523 18531 solver.cpp:240] Iteration 48, loss = 5.70061
I0411 00:00:32.593555 18531 solver.cpp:256]     Train net output #0: loss = 5.70061 (* 1 = 5.70061 loss)
I0411 00:00:32.593564 18531 sgd_solver.cpp:106] Iteration 48, lr = 0.001
I0411 00:00:32.965224 18531 solver.cpp:240] Iteration 49, loss = 4.23828
I0411 00:00:32.965267 18531 solver.cpp:256]     Train net output #0: loss = 4.23828 (* 1 = 4.23828 loss)
I0411 00:00:32.965276 18531 sgd_solver.cpp:106] Iteration 49, lr = 0.001
I0411 00:00:32.965638 18531 solver.cpp:349] Iteration 50, Testing net (#0)
I0411 00:00:34.255986 18531 solver.cpp:416]     Test net output #0: accuracy_1 = 0.0721436
I0411 00:00:34.256016 18531 solver.cpp:416]     Test net output #1: accuracy_5 = 0.383667
I0411 00:00:34.256026 18531 solver.cpp:416]     Test net output #2: loss = 4.67052 (* 1 = 4.67052 loss)
I0411 00:00:34.384546 18531 solver.cpp:240] Iteration 50, loss = 4.69925
I0411 00:00:34.384579 18531 solver.cpp:256]     Train net output #0: loss = 4.69925 (* 1 = 4.69925 loss)
I0411 00:00:34.384588 18531 sgd_solver.cpp:106] Iteration 50, lr = 0.001
I0411 00:00:34.756283 18531 solver.cpp:240] Iteration 51, loss = 6.1003
I0411 00:00:34.756328 18531 solver.cpp:256]     Train net output #0: loss = 6.1003 (* 1 = 6.1003 loss)
I0411 00:00:34.756336 18531 sgd_solver.cpp:106] Iteration 51, lr = 0.001
I0411 00:00:35.128674 18531 solver.cpp:240] Iteration 52, loss = 6.91364
I0411 00:00:35.128710 18531 solver.cpp:256]     Train net output #0: loss = 6.91364 (* 1 = 6.91364 loss)
I0411 00:00:35.128739 18531 sgd_solver.cpp:106] Iteration 52, lr = 0.001
I0411 00:00:35.500551 18531 solver.cpp:240] Iteration 53, loss = 7.38187
I0411 00:00:35.500593 18531 solver.cpp:256]     Train net output #0: loss = 7.38187 (* 1 = 7.38187 loss)
I0411 00:00:35.500602 18531 sgd_solver.cpp:106] Iteration 53, lr = 0.001
I0411 00:00:35.875156 18531 solver.cpp:240] Iteration 54, loss = 8.65162
I0411 00:00:35.875192 18531 solver.cpp:256]     Train net output #0: loss = 8.65162 (* 1 = 8.65162 loss)
I0411 00:00:35.875201 18531 sgd_solver.cpp:106] Iteration 54, lr = 0.001
I0411 00:00:36.246623 18531 solver.cpp:240] Iteration 55, loss = 10.0526
I0411 00:00:36.246665 18531 solver.cpp:256]     Train net output #0: loss = 10.0526 (* 1 = 10.0526 loss)
I0411 00:00:36.246673 18531 sgd_solver.cpp:106] Iteration 55, lr = 0.001
I0411 00:00:36.618235 18531 solver.cpp:240] Iteration 56, loss = 10.9427
I0411 00:00:36.618270 18531 solver.cpp:256]     Train net output #0: loss = 10.9427 (* 1 = 10.9427 loss)
I0411 00:00:36.618279 18531 sgd_solver.cpp:106] Iteration 56, lr = 0.001
I0411 00:00:36.990689 18531 solver.cpp:240] Iteration 57, loss = 11.8893
I0411 00:00:36.990734 18531 solver.cpp:256]     Train net output #0: loss = 11.8893 (* 1 = 11.8893 loss)
I0411 00:00:36.990743 18531 sgd_solver.cpp:106] Iteration 57, lr = 0.001
I0411 00:00:37.359764 18531 solver.cpp:240] Iteration 58, loss = 13.5106
I0411 00:00:37.359798 18531 solver.cpp:256]     Train net output #0: loss = 13.5106 (* 1 = 13.5106 loss)
I0411 00:00:37.359807 18531 sgd_solver.cpp:106] Iteration 58, lr = 0.001
I0411 00:00:37.738610 18531 solver.cpp:240] Iteration 59, loss = 13.1802
I0411 00:00:37.738642 18531 solver.cpp:256]     Train net output #0: loss = 13.1802 (* 1 = 13.1802 loss)
I0411 00:00:37.738651 18531 sgd_solver.cpp:106] Iteration 59, lr = 0.001
I0411 00:00:38.113878 18531 solver.cpp:240] Iteration 60, loss = 10.6063
I0411 00:00:38.113914 18531 solver.cpp:256]     Train net output #0: loss = 10.6063 (* 1 = 10.6063 loss)
I0411 00:00:38.113921 18531 sgd_solver.cpp:106] Iteration 60, lr = 0.001
I0411 00:00:38.484010 18531 solver.cpp:240] Iteration 61, loss = 9.84509
I0411 00:00:38.484043 18531 solver.cpp:256]     Train net output #0: loss = 9.84509 (* 1 = 9.84509 loss)
I0411 00:00:38.484051 18531 sgd_solver.cpp:106] Iteration 61, lr = 0.001
I0411 00:00:38.857257 18531 solver.cpp:240] Iteration 62, loss = 9.38236
I0411 00:00:38.857291 18531 solver.cpp:256]     Train net output #0: loss = 9.38236 (* 1 = 9.38236 loss)
I0411 00:00:38.857312 18531 sgd_solver.cpp:106] Iteration 62, lr = 0.001
I0411 00:00:39.232511 18531 solver.cpp:240] Iteration 63, loss = 7.26264
I0411 00:00:39.232547 18531 solver.cpp:256]     Train net output #0: loss = 7.26264 (* 1 = 7.26264 loss)
I0411 00:00:39.232554 18531 sgd_solver.cpp:106] Iteration 63, lr = 0.001
I0411 00:00:39.603781 18531 solver.cpp:240] Iteration 64, loss = 7.24231
I0411 00:00:39.603813 18531 solver.cpp:256]     Train net output #0: loss = 7.24231 (* 1 = 7.24231 loss)
I0411 00:00:39.603821 18531 sgd_solver.cpp:106] Iteration 64, lr = 0.001
I0411 00:00:39.976864 18531 solver.cpp:240] Iteration 65, loss = 9.44608
I0411 00:00:39.976896 18531 solver.cpp:256]     Train net output #0: loss = 9.44608 (* 1 = 9.44608 loss)
I0411 00:00:39.976902 18531 sgd_solver.cpp:106] Iteration 65, lr = 0.001
I0411 00:00:40.346680 18531 solver.cpp:240] Iteration 66, loss = 8.32923
I0411 00:00:40.346712 18531 solver.cpp:256]     Train net output #0: loss = 8.32923 (* 1 = 8.32923 loss)
I0411 00:00:40.346720 18531 sgd_solver.cpp:106] Iteration 66, lr = 0.001
I0411 00:00:40.720629 18531 solver.cpp:240] Iteration 67, loss = 7.64223
I0411 00:00:40.720660 18531 solver.cpp:256]     Train net output #0: loss = 7.64223 (* 1 = 7.64223 loss)
I0411 00:00:40.720669 18531 sgd_solver.cpp:106] Iteration 67, lr = 0.001
I0411 00:00:41.090327 18531 solver.cpp:240] Iteration 68, loss = 9.29
I0411 00:00:41.092744 18531 solver.cpp:256]     Train net output #0: loss = 9.29 (* 1 = 9.29 loss)
I0411 00:00:41.092756 18531 sgd_solver.cpp:106] Iteration 68, lr = 0.001
I0411 00:00:41.466135 18531 solver.cpp:240] Iteration 69, loss = 9.09497
I0411 00:00:41.466168 18531 solver.cpp:256]     Train net output #0: loss = 9.09497 (* 1 = 9.09497 loss)
I0411 00:00:41.466177 18531 sgd_solver.cpp:106] Iteration 69, lr = 0.001
I0411 00:00:41.837952 18531 solver.cpp:240] Iteration 70, loss = 10.1731
I0411 00:00:41.837986 18531 solver.cpp:256]     Train net output #0: loss = 10.1731 (* 1 = 10.1731 loss)
I0411 00:00:41.837996 18531 sgd_solver.cpp:106] Iteration 70, lr = 0.001
I0411 00:00:42.211555 18531 solver.cpp:240] Iteration 71, loss = 10.1879
I0411 00:00:42.211591 18531 solver.cpp:256]     Train net output #0: loss = 10.1879 (* 1 = 10.1879 loss)
I0411 00:00:42.211601 18531 sgd_solver.cpp:106] Iteration 71, lr = 0.001
I0411 00:00:42.590312 18531 solver.cpp:240] Iteration 72, loss = 8.57223
I0411 00:00:42.590348 18531 solver.cpp:256]     Train net output #0: loss = 8.57223 (* 1 = 8.57223 loss)
I0411 00:00:42.590356 18531 sgd_solver.cpp:106] Iteration 72, lr = 0.001
I0411 00:00:42.965013 18531 solver.cpp:240] Iteration 73, loss = 9.49057
I0411 00:00:42.965046 18531 solver.cpp:256]     Train net output #0: loss = 9.49057 (* 1 = 9.49057 loss)
I0411 00:00:42.965055 18531 sgd_solver.cpp:106] Iteration 73, lr = 0.001
I0411 00:00:43.335680 18531 solver.cpp:240] Iteration 74, loss = 14.4472
I0411 00:00:43.335712 18531 solver.cpp:256]     Train net output #0: loss = 14.4472 (* 1 = 14.4472 loss)
I0411 00:00:43.335721 18531 sgd_solver.cpp:106] Iteration 74, lr = 0.001
I0411 00:00:43.336063 18531 solver.cpp:349] Iteration 75, Testing net (#0)
I0411 00:00:44.630264 18531 solver.cpp:416]     Test net output #0: accuracy_1 = 0.0598145
I0411 00:00:44.630291 18531 solver.cpp:416]     Test net output #1: accuracy_5 = 0.112427
I0411 00:00:44.630314 18531 solver.cpp:416]     Test net output #2: loss = 14.9914 (* 1 = 14.9914 loss)
I0411 00:00:44.759477 18531 solver.cpp:240] Iteration 75, loss = 15.36
I0411 00:00:44.759510 18531 solver.cpp:256]     Train net output #0: loss = 15.36 (* 1 = 15.36 loss)
I0411 00:00:44.759517 18531 sgd_solver.cpp:106] Iteration 75, lr = 0.001
I0411 00:00:45.134292 18531 solver.cpp:240] Iteration 76, loss = 15.3715
I0411 00:00:45.134327 18531 solver.cpp:256]     Train net output #0: loss = 15.3715 (* 1 = 15.3715 loss)
I0411 00:00:45.134348 18531 sgd_solver.cpp:106] Iteration 76, lr = 0.001
I0411 00:00:45.507165 18531 solver.cpp:240] Iteration 77, loss = 15.9476
I0411 00:00:45.507200 18531 solver.cpp:256]     Train net output #0: loss = 15.9476 (* 1 = 15.9476 loss)
I0411 00:00:45.507208 18531 sgd_solver.cpp:106] Iteration 77, lr = 0.001
I0411 00:00:45.881304 18531 solver.cpp:240] Iteration 78, loss = 16.4936
I0411 00:00:45.881336 18531 solver.cpp:256]     Train net output #0: loss = 16.4936 (* 1 = 16.4936 loss)
I0411 00:00:45.881345 18531 sgd_solver.cpp:106] Iteration 78, lr = 0.001
I0411 00:00:46.253044 18531 solver.cpp:240] Iteration 79, loss = 15.5378
I0411 00:00:46.253078 18531 solver.cpp:256]     Train net output #0: loss = 15.5378 (* 1 = 15.5378 loss)
I0411 00:00:46.253087 18531 sgd_solver.cpp:106] Iteration 79, lr = 0.001
I0411 00:00:46.627436 18531 solver.cpp:240] Iteration 80, loss = 11.6586
I0411 00:00:46.627470 18531 solver.cpp:256]     Train net output #0: loss = 11.6586 (* 1 = 11.6586 loss)
I0411 00:00:46.627478 18531 sgd_solver.cpp:106] Iteration 80, lr = 0.001
I0411 00:00:47.002429 18531 solver.cpp:240] Iteration 81, loss = 11.4264
I0411 00:00:47.002462 18531 solver.cpp:256]     Train net output #0: loss = 11.4264 (* 1 = 11.4264 loss)
I0411 00:00:47.002470 18531 sgd_solver.cpp:106] Iteration 81, lr = 0.001
