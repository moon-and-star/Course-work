I0508 00:31:40.385514 12373 caffe.cpp:217] Using GPUs 1
I0508 00:31:40.603837 12373 caffe.cpp:222] GPU 1: GeForce GTX 1070
I0508 00:31:41.619652 12373 solver.cpp:60] Initializing solver from parameters: 
train_net: "./Prototxt/experiment_15/RTSD/orig/trial_1/train.prototxt"
test_net: "./Prototxt/experiment_15/RTSD/orig/trial_1/test.prototxt"
test_iter: 17
test_interval: 85
base_lr: 1e-05
display: 1
max_iter: 8500
lr_policy: "step"
gamma: 0.7
momentum: 0.9
weight_decay: 0.0005
stepsize: 1700
snapshot: 850
snapshot_prefix: "./snapshots/experiment_15/RTSD/orig/trial_1/snap"
solver_mode: GPU
device_id: 1
train_state {
  level: 0
  stage: ""
}
iter_size: 1
type: "Adam"
I0508 00:31:41.619789 12373 solver.cpp:93] Creating training net from train_net file: ./Prototxt/experiment_15/RTSD/orig/trial_1/train.prototxt
I0508 00:31:41.620256 12373 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.0039215689
    mirror: false
    crop_size: 48
    mean_value: 119
    mean_value: 113
    mean_value: 113
  }
  data_param {
    source: "../local_data/lmdb/RTSD/orig/train/lmdb"
    batch_size: 1024
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 7
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_prescale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv1_sTanH"
  type: "TanH"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_postscale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 150
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_prescale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv2_sTanH"
  type: "TanH"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_postscale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 250
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_prescale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv3_sTanH"
  type: "TanH"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_postscale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc4_300"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc4_300"
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fc4_prescale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "fc4_sTanH"
  type: "TanH"
  bottom: "fc4_300"
  top: "fc4_300"
}
layer {
  name: "fc4_postscale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "fc5_116"
  type: "InnerProduct"
  bottom: "fc4_300"
  top: "fc5_classes"
  inner_product_param {
    num_output: 116
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "softmax"
  type: "Softmax"
  bottom: "fc5_classes"
  top: "softmax"
}
layer {
  name: "loss"
  type: "MultinomialLogisticLoss"
  bottom: "softmax"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy_1"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_1"
  accuracy_param {
    top_k: 1
  }
}
layer {
  name: "accuracy_5"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_5"
  accuracy_param {
    top_k: 5
  }
}
layer {
  name: "silence"
  type: "Silence"
  bottom: "accuracy_1"
  bottom: "accuracy_5"
}
I0508 00:31:41.620416 12373 layer_factory.hpp:77] Creating layer data
I0508 00:31:41.621485 12373 net.cpp:100] Creating Layer data
I0508 00:31:41.621500 12373 net.cpp:408] data -> data
I0508 00:31:41.621522 12373 net.cpp:408] data -> label
I0508 00:31:41.909550 12504 db_lmdb.cpp:35] Opened lmdb ../local_data/lmdb/RTSD/orig/train/lmdb
I0508 00:31:41.994755 12373 data_layer.cpp:41] output data size: 1024,3,48,48
I0508 00:31:42.050626 12373 net.cpp:150] Setting up data
I0508 00:31:42.050657 12373 net.cpp:157] Top shape: 1024 3 48 48 (7077888)
I0508 00:31:42.050662 12373 net.cpp:157] Top shape: 1024 (1024)
I0508 00:31:42.050665 12373 net.cpp:165] Memory required for data: 28315648
I0508 00:31:42.050676 12373 layer_factory.hpp:77] Creating layer label_data_1_split
I0508 00:31:42.050699 12373 net.cpp:100] Creating Layer label_data_1_split
I0508 00:31:42.050707 12373 net.cpp:434] label_data_1_split <- label
I0508 00:31:42.050721 12373 net.cpp:408] label_data_1_split -> label_data_1_split_0
I0508 00:31:42.050734 12373 net.cpp:408] label_data_1_split -> label_data_1_split_1
I0508 00:31:42.050742 12373 net.cpp:408] label_data_1_split -> label_data_1_split_2
I0508 00:31:42.050813 12373 net.cpp:150] Setting up label_data_1_split
I0508 00:31:42.050822 12373 net.cpp:157] Top shape: 1024 (1024)
I0508 00:31:42.050828 12373 net.cpp:157] Top shape: 1024 (1024)
I0508 00:31:42.050832 12373 net.cpp:157] Top shape: 1024 (1024)
I0508 00:31:42.050835 12373 net.cpp:165] Memory required for data: 28327936
I0508 00:31:42.050838 12373 layer_factory.hpp:77] Creating layer conv1
I0508 00:31:42.050855 12373 net.cpp:100] Creating Layer conv1
I0508 00:31:42.050860 12373 net.cpp:434] conv1 <- data
I0508 00:31:42.050868 12373 net.cpp:408] conv1 -> conv1
I0508 00:31:42.059259 12548 blocking_queue.cpp:50] Waiting for data
I0508 00:31:42.414968 12373 net.cpp:150] Setting up conv1
I0508 00:31:42.414999 12373 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0508 00:31:42.415004 12373 net.cpp:165] Memory required for data: 750862336
I0508 00:31:42.415029 12373 layer_factory.hpp:77] Creating layer conv1_prescale
I0508 00:31:42.415042 12373 net.cpp:100] Creating Layer conv1_prescale
I0508 00:31:42.415046 12373 net.cpp:434] conv1_prescale <- conv1
I0508 00:31:42.415055 12373 net.cpp:395] conv1_prescale -> conv1 (in-place)
I0508 00:31:42.415179 12373 net.cpp:150] Setting up conv1_prescale
I0508 00:31:42.415189 12373 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0508 00:31:42.415192 12373 net.cpp:165] Memory required for data: 1473396736
I0508 00:31:42.415200 12373 layer_factory.hpp:77] Creating layer conv1_sTanH
I0508 00:31:42.415210 12373 net.cpp:100] Creating Layer conv1_sTanH
I0508 00:31:42.415215 12373 net.cpp:434] conv1_sTanH <- conv1
I0508 00:31:42.415220 12373 net.cpp:395] conv1_sTanH -> conv1 (in-place)
I0508 00:31:42.415432 12373 net.cpp:150] Setting up conv1_sTanH
I0508 00:31:42.415468 12373 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0508 00:31:42.415474 12373 net.cpp:165] Memory required for data: 2195931136
I0508 00:31:42.415478 12373 layer_factory.hpp:77] Creating layer conv1_postscale
I0508 00:31:42.415485 12373 net.cpp:100] Creating Layer conv1_postscale
I0508 00:31:42.415491 12373 net.cpp:434] conv1_postscale <- conv1
I0508 00:31:42.415498 12373 net.cpp:395] conv1_postscale -> conv1 (in-place)
I0508 00:31:42.415597 12373 net.cpp:150] Setting up conv1_postscale
I0508 00:31:42.415606 12373 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0508 00:31:42.415611 12373 net.cpp:165] Memory required for data: 2918465536
I0508 00:31:42.415616 12373 layer_factory.hpp:77] Creating layer pool1
I0508 00:31:42.415624 12373 net.cpp:100] Creating Layer pool1
I0508 00:31:42.415629 12373 net.cpp:434] pool1 <- conv1
I0508 00:31:42.415634 12373 net.cpp:408] pool1 -> pool1
I0508 00:31:42.415685 12373 net.cpp:150] Setting up pool1
I0508 00:31:42.415693 12373 net.cpp:157] Top shape: 1024 100 21 21 (45158400)
I0508 00:31:42.415698 12373 net.cpp:165] Memory required for data: 3099099136
I0508 00:31:42.415701 12373 layer_factory.hpp:77] Creating layer conv2
I0508 00:31:42.415711 12373 net.cpp:100] Creating Layer conv2
I0508 00:31:42.415716 12373 net.cpp:434] conv2 <- pool1
I0508 00:31:42.415721 12373 net.cpp:408] conv2 -> conv2
I0508 00:31:42.423130 12373 net.cpp:150] Setting up conv2
I0508 00:31:42.423149 12373 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0508 00:31:42.423153 12373 net.cpp:165] Memory required for data: 3298164736
I0508 00:31:42.423166 12373 layer_factory.hpp:77] Creating layer conv2_prescale
I0508 00:31:42.423177 12373 net.cpp:100] Creating Layer conv2_prescale
I0508 00:31:42.423180 12373 net.cpp:434] conv2_prescale <- conv2
I0508 00:31:42.423187 12373 net.cpp:395] conv2_prescale -> conv2 (in-place)
I0508 00:31:42.423300 12373 net.cpp:150] Setting up conv2_prescale
I0508 00:31:42.423310 12373 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0508 00:31:42.423323 12373 net.cpp:165] Memory required for data: 3497230336
I0508 00:31:42.423329 12373 layer_factory.hpp:77] Creating layer conv2_sTanH
I0508 00:31:42.423336 12373 net.cpp:100] Creating Layer conv2_sTanH
I0508 00:31:42.423338 12373 net.cpp:434] conv2_sTanH <- conv2
I0508 00:31:42.423343 12373 net.cpp:395] conv2_sTanH -> conv2 (in-place)
I0508 00:31:42.424249 12373 net.cpp:150] Setting up conv2_sTanH
I0508 00:31:42.424266 12373 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0508 00:31:42.424270 12373 net.cpp:165] Memory required for data: 3696295936
I0508 00:31:42.424275 12373 layer_factory.hpp:77] Creating layer conv2_postscale
I0508 00:31:42.424283 12373 net.cpp:100] Creating Layer conv2_postscale
I0508 00:31:42.424288 12373 net.cpp:434] conv2_postscale <- conv2
I0508 00:31:42.424293 12373 net.cpp:395] conv2_postscale -> conv2 (in-place)
I0508 00:31:42.424394 12373 net.cpp:150] Setting up conv2_postscale
I0508 00:31:42.424403 12373 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0508 00:31:42.424408 12373 net.cpp:165] Memory required for data: 3895361536
I0508 00:31:42.424413 12373 layer_factory.hpp:77] Creating layer pool2
I0508 00:31:42.424422 12373 net.cpp:100] Creating Layer pool2
I0508 00:31:42.424427 12373 net.cpp:434] pool2 <- conv2
I0508 00:31:42.424432 12373 net.cpp:408] pool2 -> pool2
I0508 00:31:42.424476 12373 net.cpp:150] Setting up pool2
I0508 00:31:42.424485 12373 net.cpp:157] Top shape: 1024 150 9 9 (12441600)
I0508 00:31:42.424489 12373 net.cpp:165] Memory required for data: 3945127936
I0508 00:31:42.424494 12373 layer_factory.hpp:77] Creating layer conv3
I0508 00:31:42.424502 12373 net.cpp:100] Creating Layer conv3
I0508 00:31:42.424507 12373 net.cpp:434] conv3 <- pool2
I0508 00:31:42.424512 12373 net.cpp:408] conv3 -> conv3
I0508 00:31:42.431483 12373 net.cpp:150] Setting up conv3
I0508 00:31:42.431501 12373 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0508 00:31:42.431506 12373 net.cpp:165] Memory required for data: 3981991936
I0508 00:31:42.431517 12373 layer_factory.hpp:77] Creating layer conv3_prescale
I0508 00:31:42.431543 12373 net.cpp:100] Creating Layer conv3_prescale
I0508 00:31:42.431550 12373 net.cpp:434] conv3_prescale <- conv3
I0508 00:31:42.431555 12373 net.cpp:395] conv3_prescale -> conv3 (in-place)
I0508 00:31:42.431650 12373 net.cpp:150] Setting up conv3_prescale
I0508 00:31:42.431660 12373 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0508 00:31:42.431664 12373 net.cpp:165] Memory required for data: 4018855936
I0508 00:31:42.431670 12373 layer_factory.hpp:77] Creating layer conv3_sTanH
I0508 00:31:42.431676 12373 net.cpp:100] Creating Layer conv3_sTanH
I0508 00:31:42.431681 12373 net.cpp:434] conv3_sTanH <- conv3
I0508 00:31:42.431686 12373 net.cpp:395] conv3_sTanH -> conv3 (in-place)
I0508 00:31:42.432518 12373 net.cpp:150] Setting up conv3_sTanH
I0508 00:31:42.432534 12373 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0508 00:31:42.432539 12373 net.cpp:165] Memory required for data: 4055719936
I0508 00:31:42.432543 12373 layer_factory.hpp:77] Creating layer conv3_postscale
I0508 00:31:42.432551 12373 net.cpp:100] Creating Layer conv3_postscale
I0508 00:31:42.432556 12373 net.cpp:434] conv3_postscale <- conv3
I0508 00:31:42.432562 12373 net.cpp:395] conv3_postscale -> conv3 (in-place)
I0508 00:31:42.432659 12373 net.cpp:150] Setting up conv3_postscale
I0508 00:31:42.432668 12373 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0508 00:31:42.432672 12373 net.cpp:165] Memory required for data: 4092583936
I0508 00:31:42.432678 12373 layer_factory.hpp:77] Creating layer pool3
I0508 00:31:42.432688 12373 net.cpp:100] Creating Layer pool3
I0508 00:31:42.432693 12373 net.cpp:434] pool3 <- conv3
I0508 00:31:42.432698 12373 net.cpp:408] pool3 -> pool3
I0508 00:31:42.432739 12373 net.cpp:150] Setting up pool3
I0508 00:31:42.432746 12373 net.cpp:157] Top shape: 1024 250 3 3 (2304000)
I0508 00:31:42.432750 12373 net.cpp:165] Memory required for data: 4101799936
I0508 00:31:42.432754 12373 layer_factory.hpp:77] Creating layer fc4_300
I0508 00:31:42.432760 12373 net.cpp:100] Creating Layer fc4_300
I0508 00:31:42.432765 12373 net.cpp:434] fc4_300 <- pool3
I0508 00:31:42.432770 12373 net.cpp:408] fc4_300 -> fc4_300
I0508 00:31:42.441058 12373 net.cpp:150] Setting up fc4_300
I0508 00:31:42.441077 12373 net.cpp:157] Top shape: 1024 300 (307200)
I0508 00:31:42.441082 12373 net.cpp:165] Memory required for data: 4103028736
I0508 00:31:42.441089 12373 layer_factory.hpp:77] Creating layer fc4_prescale
I0508 00:31:42.441099 12373 net.cpp:100] Creating Layer fc4_prescale
I0508 00:31:42.441104 12373 net.cpp:434] fc4_prescale <- fc4_300
I0508 00:31:42.441112 12373 net.cpp:395] fc4_prescale -> fc4_300 (in-place)
I0508 00:31:42.441200 12373 net.cpp:150] Setting up fc4_prescale
I0508 00:31:42.441210 12373 net.cpp:157] Top shape: 1024 300 (307200)
I0508 00:31:42.441215 12373 net.cpp:165] Memory required for data: 4104257536
I0508 00:31:42.441220 12373 layer_factory.hpp:77] Creating layer fc4_sTanH
I0508 00:31:42.441224 12373 net.cpp:100] Creating Layer fc4_sTanH
I0508 00:31:42.441229 12373 net.cpp:434] fc4_sTanH <- fc4_300
I0508 00:31:42.441234 12373 net.cpp:395] fc4_sTanH -> fc4_300 (in-place)
I0508 00:31:42.441431 12373 net.cpp:150] Setting up fc4_sTanH
I0508 00:31:42.441443 12373 net.cpp:157] Top shape: 1024 300 (307200)
I0508 00:31:42.441448 12373 net.cpp:165] Memory required for data: 4105486336
I0508 00:31:42.441452 12373 layer_factory.hpp:77] Creating layer fc4_postscale
I0508 00:31:42.441458 12373 net.cpp:100] Creating Layer fc4_postscale
I0508 00:31:42.441463 12373 net.cpp:434] fc4_postscale <- fc4_300
I0508 00:31:42.441469 12373 net.cpp:395] fc4_postscale -> fc4_300 (in-place)
I0508 00:31:42.441565 12373 net.cpp:150] Setting up fc4_postscale
I0508 00:31:42.441573 12373 net.cpp:157] Top shape: 1024 300 (307200)
I0508 00:31:42.441577 12373 net.cpp:165] Memory required for data: 4106715136
I0508 00:31:42.441582 12373 layer_factory.hpp:77] Creating layer fc5_116
I0508 00:31:42.441591 12373 net.cpp:100] Creating Layer fc5_116
I0508 00:31:42.441596 12373 net.cpp:434] fc5_116 <- fc4_300
I0508 00:31:42.441620 12373 net.cpp:408] fc5_116 -> fc5_classes
I0508 00:31:42.448060 12373 net.cpp:150] Setting up fc5_116
I0508 00:31:42.448078 12373 net.cpp:157] Top shape: 1024 116 (118784)
I0508 00:31:42.448083 12373 net.cpp:165] Memory required for data: 4107190272
I0508 00:31:42.448096 12373 layer_factory.hpp:77] Creating layer fc5_classes_fc5_116_0_split
I0508 00:31:42.448107 12373 net.cpp:100] Creating Layer fc5_classes_fc5_116_0_split
I0508 00:31:42.448112 12373 net.cpp:434] fc5_classes_fc5_116_0_split <- fc5_classes
I0508 00:31:42.448118 12373 net.cpp:408] fc5_classes_fc5_116_0_split -> fc5_classes_fc5_116_0_split_0
I0508 00:31:42.448127 12373 net.cpp:408] fc5_classes_fc5_116_0_split -> fc5_classes_fc5_116_0_split_1
I0508 00:31:42.448135 12373 net.cpp:408] fc5_classes_fc5_116_0_split -> fc5_classes_fc5_116_0_split_2
I0508 00:31:42.448189 12373 net.cpp:150] Setting up fc5_classes_fc5_116_0_split
I0508 00:31:42.448197 12373 net.cpp:157] Top shape: 1024 116 (118784)
I0508 00:31:42.448202 12373 net.cpp:157] Top shape: 1024 116 (118784)
I0508 00:31:42.448206 12373 net.cpp:157] Top shape: 1024 116 (118784)
I0508 00:31:42.448210 12373 net.cpp:165] Memory required for data: 4108615680
I0508 00:31:42.448213 12373 layer_factory.hpp:77] Creating layer softmax
I0508 00:31:42.448220 12373 net.cpp:100] Creating Layer softmax
I0508 00:31:42.448225 12373 net.cpp:434] softmax <- fc5_classes_fc5_116_0_split_0
I0508 00:31:42.448230 12373 net.cpp:408] softmax -> softmax
I0508 00:31:42.448503 12373 net.cpp:150] Setting up softmax
I0508 00:31:42.448515 12373 net.cpp:157] Top shape: 1024 116 (118784)
I0508 00:31:42.448520 12373 net.cpp:165] Memory required for data: 4109090816
I0508 00:31:42.448524 12373 layer_factory.hpp:77] Creating layer loss
I0508 00:31:42.448530 12373 net.cpp:100] Creating Layer loss
I0508 00:31:42.448535 12373 net.cpp:434] loss <- softmax
I0508 00:31:42.448540 12373 net.cpp:434] loss <- label_data_1_split_0
I0508 00:31:42.448547 12373 net.cpp:408] loss -> loss
I0508 00:31:42.448577 12373 net.cpp:150] Setting up loss
I0508 00:31:42.448585 12373 net.cpp:157] Top shape: (1)
I0508 00:31:42.448588 12373 net.cpp:160]     with loss weight 1
I0508 00:31:42.448616 12373 net.cpp:165] Memory required for data: 4109090820
I0508 00:31:42.448619 12373 layer_factory.hpp:77] Creating layer accuracy_1
I0508 00:31:42.448626 12373 net.cpp:100] Creating Layer accuracy_1
I0508 00:31:42.448632 12373 net.cpp:434] accuracy_1 <- fc5_classes_fc5_116_0_split_1
I0508 00:31:42.448637 12373 net.cpp:434] accuracy_1 <- label_data_1_split_1
I0508 00:31:42.448643 12373 net.cpp:408] accuracy_1 -> accuracy_1
I0508 00:31:42.448654 12373 net.cpp:150] Setting up accuracy_1
I0508 00:31:42.448660 12373 net.cpp:157] Top shape: (1)
I0508 00:31:42.448663 12373 net.cpp:165] Memory required for data: 4109090824
I0508 00:31:42.448667 12373 layer_factory.hpp:77] Creating layer accuracy_5
I0508 00:31:42.448673 12373 net.cpp:100] Creating Layer accuracy_5
I0508 00:31:42.448678 12373 net.cpp:434] accuracy_5 <- fc5_classes_fc5_116_0_split_2
I0508 00:31:42.448681 12373 net.cpp:434] accuracy_5 <- label_data_1_split_2
I0508 00:31:42.448686 12373 net.cpp:408] accuracy_5 -> accuracy_5
I0508 00:31:42.448693 12373 net.cpp:150] Setting up accuracy_5
I0508 00:31:42.448698 12373 net.cpp:157] Top shape: (1)
I0508 00:31:42.448701 12373 net.cpp:165] Memory required for data: 4109090828
I0508 00:31:42.448704 12373 layer_factory.hpp:77] Creating layer silence
I0508 00:31:42.448709 12373 net.cpp:100] Creating Layer silence
I0508 00:31:42.448714 12373 net.cpp:434] silence <- accuracy_1
I0508 00:31:42.448717 12373 net.cpp:434] silence <- accuracy_5
I0508 00:31:42.448724 12373 net.cpp:150] Setting up silence
I0508 00:31:42.448726 12373 net.cpp:165] Memory required for data: 4109090828
I0508 00:31:42.448729 12373 net.cpp:228] silence does not need backward computation.
I0508 00:31:42.448737 12373 net.cpp:228] accuracy_5 does not need backward computation.
I0508 00:31:42.448741 12373 net.cpp:228] accuracy_1 does not need backward computation.
I0508 00:31:42.448745 12373 net.cpp:226] loss needs backward computation.
I0508 00:31:42.448767 12373 net.cpp:226] softmax needs backward computation.
I0508 00:31:42.448773 12373 net.cpp:226] fc5_classes_fc5_116_0_split needs backward computation.
I0508 00:31:42.448777 12373 net.cpp:226] fc5_116 needs backward computation.
I0508 00:31:42.448781 12373 net.cpp:226] fc4_postscale needs backward computation.
I0508 00:31:42.448783 12373 net.cpp:226] fc4_sTanH needs backward computation.
I0508 00:31:42.448786 12373 net.cpp:226] fc4_prescale needs backward computation.
I0508 00:31:42.448791 12373 net.cpp:226] fc4_300 needs backward computation.
I0508 00:31:42.448793 12373 net.cpp:226] pool3 needs backward computation.
I0508 00:31:42.448797 12373 net.cpp:226] conv3_postscale needs backward computation.
I0508 00:31:42.448801 12373 net.cpp:226] conv3_sTanH needs backward computation.
I0508 00:31:42.448803 12373 net.cpp:226] conv3_prescale needs backward computation.
I0508 00:31:42.448807 12373 net.cpp:226] conv3 needs backward computation.
I0508 00:31:42.448810 12373 net.cpp:226] pool2 needs backward computation.
I0508 00:31:42.448814 12373 net.cpp:226] conv2_postscale needs backward computation.
I0508 00:31:42.448817 12373 net.cpp:226] conv2_sTanH needs backward computation.
I0508 00:31:42.448820 12373 net.cpp:226] conv2_prescale needs backward computation.
I0508 00:31:42.448823 12373 net.cpp:226] conv2 needs backward computation.
I0508 00:31:42.448827 12373 net.cpp:226] pool1 needs backward computation.
I0508 00:31:42.448830 12373 net.cpp:226] conv1_postscale needs backward computation.
I0508 00:31:42.448834 12373 net.cpp:226] conv1_sTanH needs backward computation.
I0508 00:31:42.448837 12373 net.cpp:226] conv1_prescale needs backward computation.
I0508 00:31:42.448840 12373 net.cpp:226] conv1 needs backward computation.
I0508 00:31:42.448845 12373 net.cpp:228] label_data_1_split does not need backward computation.
I0508 00:31:42.448851 12373 net.cpp:228] data does not need backward computation.
I0508 00:31:42.448854 12373 net.cpp:270] This network produces output loss
I0508 00:31:42.448875 12373 net.cpp:283] Network initialization done.
I0508 00:31:42.449157 12373 solver.cpp:193] Creating test net (#0) specified by test_net file: ./Prototxt/experiment_15/RTSD/orig/trial_1/test.prototxt
I0508 00:31:42.449338 12373 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.0039215689
    mirror: false
    crop_size: 48
    mean_value: 121
    mean_value: 117
    mean_value: 120
  }
  data_param {
    source: "../local_data/lmdb/RTSD/orig/test/lmdb"
    batch_size: 1024
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 7
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_prescale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv1_sTanH"
  type: "TanH"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_postscale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 150
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_prescale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv2_sTanH"
  type: "TanH"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_postscale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 250
    pad: 0
    kernel_size: 4
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_prescale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "conv3_sTanH"
  type: "TanH"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_postscale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc4_300"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc4_300"
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fc4_prescale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 0.6666
    }
    bias_term: false
  }
}
layer {
  name: "fc4_sTanH"
  type: "TanH"
  bottom: "fc4_300"
  top: "fc4_300"
}
layer {
  name: "fc4_postscale"
  type: "Scale"
  bottom: "fc4_300"
  top: "fc4_300"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1.7159
    }
    bias_term: false
  }
}
layer {
  name: "fc5_116"
  type: "InnerProduct"
  bottom: "fc4_300"
  top: "fc5_classes"
  inner_product_param {
    num_output: 116
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "softmax"
  type: "Softmax"
  bottom: "fc5_classes"
  top: "softmax"
}
layer {
  name: "loss"
  type: "MultinomialLogisticLoss"
  bottom: "softmax"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy_1"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_1"
  accuracy_param {
    top_k: 1
  }
}
layer {
  name: "accuracy_5"
  type: "Accuracy"
  bottom: "fc5_classes"
  bottom: "label"
  top: "accuracy_5"
  accuracy_param {
    top_k: 5
  }
}
I0508 00:31:42.449450 12373 layer_factory.hpp:77] Creating layer data
I0508 00:31:42.454011 12373 net.cpp:100] Creating Layer data
I0508 00:31:42.454026 12373 net.cpp:408] data -> data
I0508 00:31:42.454035 12373 net.cpp:408] data -> label
I0508 00:31:42.598881 12599 db_lmdb.cpp:35] Opened lmdb ../local_data/lmdb/RTSD/orig/test/lmdb
I0508 00:31:42.758126 12373 data_layer.cpp:41] output data size: 1024,3,48,48
I0508 00:31:42.835630 12373 net.cpp:150] Setting up data
I0508 00:31:42.835666 12373 net.cpp:157] Top shape: 1024 3 48 48 (7077888)
I0508 00:31:42.835675 12373 net.cpp:157] Top shape: 1024 (1024)
I0508 00:31:42.835680 12373 net.cpp:165] Memory required for data: 28315648
I0508 00:31:42.835690 12373 layer_factory.hpp:77] Creating layer label_data_1_split
I0508 00:31:42.835707 12373 net.cpp:100] Creating Layer label_data_1_split
I0508 00:31:42.835717 12373 net.cpp:434] label_data_1_split <- label
I0508 00:31:42.835731 12373 net.cpp:408] label_data_1_split -> label_data_1_split_0
I0508 00:31:42.835750 12373 net.cpp:408] label_data_1_split -> label_data_1_split_1
I0508 00:31:42.835763 12373 net.cpp:408] label_data_1_split -> label_data_1_split_2
I0508 00:31:42.835866 12373 net.cpp:150] Setting up label_data_1_split
I0508 00:31:42.835894 12373 net.cpp:157] Top shape: 1024 (1024)
I0508 00:31:42.835904 12373 net.cpp:157] Top shape: 1024 (1024)
I0508 00:31:42.835913 12373 net.cpp:157] Top shape: 1024 (1024)
I0508 00:31:42.835942 12373 net.cpp:165] Memory required for data: 28327936
I0508 00:31:42.835952 12373 layer_factory.hpp:77] Creating layer conv1
I0508 00:31:42.835974 12373 net.cpp:100] Creating Layer conv1
I0508 00:31:42.835983 12373 net.cpp:434] conv1 <- data
I0508 00:31:42.835994 12373 net.cpp:408] conv1 -> conv1
I0508 00:31:42.840087 12373 net.cpp:150] Setting up conv1
I0508 00:31:42.840112 12373 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0508 00:31:42.840121 12373 net.cpp:165] Memory required for data: 750862336
I0508 00:31:42.840142 12373 layer_factory.hpp:77] Creating layer conv1_prescale
I0508 00:31:42.840157 12373 net.cpp:100] Creating Layer conv1_prescale
I0508 00:31:42.840162 12373 net.cpp:434] conv1_prescale <- conv1
I0508 00:31:42.840168 12373 net.cpp:395] conv1_prescale -> conv1 (in-place)
I0508 00:31:42.840333 12373 net.cpp:150] Setting up conv1_prescale
I0508 00:31:42.840351 12373 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0508 00:31:42.840359 12373 net.cpp:165] Memory required for data: 1473396736
I0508 00:31:42.840373 12373 layer_factory.hpp:77] Creating layer conv1_sTanH
I0508 00:31:42.840386 12373 net.cpp:100] Creating Layer conv1_sTanH
I0508 00:31:42.840394 12373 net.cpp:434] conv1_sTanH <- conv1
I0508 00:31:42.840404 12373 net.cpp:395] conv1_sTanH -> conv1 (in-place)
I0508 00:31:42.840739 12373 net.cpp:150] Setting up conv1_sTanH
I0508 00:31:42.840759 12373 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0508 00:31:42.840767 12373 net.cpp:165] Memory required for data: 2195931136
I0508 00:31:42.840775 12373 layer_factory.hpp:77] Creating layer conv1_postscale
I0508 00:31:42.840790 12373 net.cpp:100] Creating Layer conv1_postscale
I0508 00:31:42.840798 12373 net.cpp:434] conv1_postscale <- conv1
I0508 00:31:42.840809 12373 net.cpp:395] conv1_postscale -> conv1 (in-place)
I0508 00:31:42.843065 12373 net.cpp:150] Setting up conv1_postscale
I0508 00:31:42.843088 12373 net.cpp:157] Top shape: 1024 100 42 42 (180633600)
I0508 00:31:42.843096 12373 net.cpp:165] Memory required for data: 2918465536
I0508 00:31:42.843106 12373 layer_factory.hpp:77] Creating layer pool1
I0508 00:31:42.843119 12373 net.cpp:100] Creating Layer pool1
I0508 00:31:42.843127 12373 net.cpp:434] pool1 <- conv1
I0508 00:31:42.843142 12373 net.cpp:408] pool1 -> pool1
I0508 00:31:42.843202 12373 net.cpp:150] Setting up pool1
I0508 00:31:42.843216 12373 net.cpp:157] Top shape: 1024 100 21 21 (45158400)
I0508 00:31:42.843224 12373 net.cpp:165] Memory required for data: 3099099136
I0508 00:31:42.843230 12373 layer_factory.hpp:77] Creating layer conv2
I0508 00:31:42.843245 12373 net.cpp:100] Creating Layer conv2
I0508 00:31:42.843252 12373 net.cpp:434] conv2 <- pool1
I0508 00:31:42.843261 12373 net.cpp:408] conv2 -> conv2
I0508 00:31:42.852408 12373 net.cpp:150] Setting up conv2
I0508 00:31:42.852447 12373 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0508 00:31:42.852452 12373 net.cpp:165] Memory required for data: 3298164736
I0508 00:31:42.852465 12373 layer_factory.hpp:77] Creating layer conv2_prescale
I0508 00:31:42.852478 12373 net.cpp:100] Creating Layer conv2_prescale
I0508 00:31:42.852486 12373 net.cpp:434] conv2_prescale <- conv2
I0508 00:31:42.852494 12373 net.cpp:395] conv2_prescale -> conv2 (in-place)
I0508 00:31:42.852605 12373 net.cpp:150] Setting up conv2_prescale
I0508 00:31:42.852615 12373 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0508 00:31:42.852619 12373 net.cpp:165] Memory required for data: 3497230336
I0508 00:31:42.852624 12373 layer_factory.hpp:77] Creating layer conv2_sTanH
I0508 00:31:42.852632 12373 net.cpp:100] Creating Layer conv2_sTanH
I0508 00:31:42.852635 12373 net.cpp:434] conv2_sTanH <- conv2
I0508 00:31:42.852640 12373 net.cpp:395] conv2_sTanH -> conv2 (in-place)
I0508 00:31:42.853688 12373 net.cpp:150] Setting up conv2_sTanH
I0508 00:31:42.853704 12373 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0508 00:31:42.853708 12373 net.cpp:165] Memory required for data: 3696295936
I0508 00:31:42.853713 12373 layer_factory.hpp:77] Creating layer conv2_postscale
I0508 00:31:42.853744 12373 net.cpp:100] Creating Layer conv2_postscale
I0508 00:31:42.853749 12373 net.cpp:434] conv2_postscale <- conv2
I0508 00:31:42.853756 12373 net.cpp:395] conv2_postscale -> conv2 (in-place)
I0508 00:31:42.853860 12373 net.cpp:150] Setting up conv2_postscale
I0508 00:31:42.853869 12373 net.cpp:157] Top shape: 1024 150 18 18 (49766400)
I0508 00:31:42.853874 12373 net.cpp:165] Memory required for data: 3895361536
I0508 00:31:42.853879 12373 layer_factory.hpp:77] Creating layer pool2
I0508 00:31:42.853888 12373 net.cpp:100] Creating Layer pool2
I0508 00:31:42.853893 12373 net.cpp:434] pool2 <- conv2
I0508 00:31:42.853899 12373 net.cpp:408] pool2 -> pool2
I0508 00:31:42.853943 12373 net.cpp:150] Setting up pool2
I0508 00:31:42.853952 12373 net.cpp:157] Top shape: 1024 150 9 9 (12441600)
I0508 00:31:42.853956 12373 net.cpp:165] Memory required for data: 3945127936
I0508 00:31:42.853960 12373 layer_factory.hpp:77] Creating layer conv3
I0508 00:31:42.853968 12373 net.cpp:100] Creating Layer conv3
I0508 00:31:42.853973 12373 net.cpp:434] conv3 <- pool2
I0508 00:31:42.853979 12373 net.cpp:408] conv3 -> conv3
I0508 00:31:42.861003 12373 net.cpp:150] Setting up conv3
I0508 00:31:42.861028 12373 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0508 00:31:42.861034 12373 net.cpp:165] Memory required for data: 3981991936
I0508 00:31:42.861050 12373 layer_factory.hpp:77] Creating layer conv3_prescale
I0508 00:31:42.861062 12373 net.cpp:100] Creating Layer conv3_prescale
I0508 00:31:42.861069 12373 net.cpp:434] conv3_prescale <- conv3
I0508 00:31:42.861080 12373 net.cpp:395] conv3_prescale -> conv3 (in-place)
I0508 00:31:42.861212 12373 net.cpp:150] Setting up conv3_prescale
I0508 00:31:42.861223 12373 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0508 00:31:42.861230 12373 net.cpp:165] Memory required for data: 4018855936
I0508 00:31:42.861240 12373 layer_factory.hpp:77] Creating layer conv3_sTanH
I0508 00:31:42.861249 12373 net.cpp:100] Creating Layer conv3_sTanH
I0508 00:31:42.861256 12373 net.cpp:434] conv3_sTanH <- conv3
I0508 00:31:42.861265 12373 net.cpp:395] conv3_sTanH -> conv3 (in-place)
I0508 00:31:42.866724 12373 net.cpp:150] Setting up conv3_sTanH
I0508 00:31:42.866744 12373 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0508 00:31:42.866747 12373 net.cpp:165] Memory required for data: 4055719936
I0508 00:31:42.866751 12373 layer_factory.hpp:77] Creating layer conv3_postscale
I0508 00:31:42.866761 12373 net.cpp:100] Creating Layer conv3_postscale
I0508 00:31:42.866765 12373 net.cpp:434] conv3_postscale <- conv3
I0508 00:31:42.866770 12373 net.cpp:395] conv3_postscale -> conv3 (in-place)
I0508 00:31:42.866873 12373 net.cpp:150] Setting up conv3_postscale
I0508 00:31:42.866883 12373 net.cpp:157] Top shape: 1024 250 6 6 (9216000)
I0508 00:31:42.866888 12373 net.cpp:165] Memory required for data: 4092583936
I0508 00:31:42.866894 12373 layer_factory.hpp:77] Creating layer pool3
I0508 00:31:42.866904 12373 net.cpp:100] Creating Layer pool3
I0508 00:31:42.866909 12373 net.cpp:434] pool3 <- conv3
I0508 00:31:42.866915 12373 net.cpp:408] pool3 -> pool3
I0508 00:31:42.866958 12373 net.cpp:150] Setting up pool3
I0508 00:31:42.866966 12373 net.cpp:157] Top shape: 1024 250 3 3 (2304000)
I0508 00:31:42.866971 12373 net.cpp:165] Memory required for data: 4101799936
I0508 00:31:42.866974 12373 layer_factory.hpp:77] Creating layer fc4_300
I0508 00:31:42.866981 12373 net.cpp:100] Creating Layer fc4_300
I0508 00:31:42.866986 12373 net.cpp:434] fc4_300 <- pool3
I0508 00:31:42.866991 12373 net.cpp:408] fc4_300 -> fc4_300
I0508 00:31:42.877535 12373 net.cpp:150] Setting up fc4_300
I0508 00:31:42.877559 12373 net.cpp:157] Top shape: 1024 300 (307200)
I0508 00:31:42.877563 12373 net.cpp:165] Memory required for data: 4103028736
I0508 00:31:42.877571 12373 layer_factory.hpp:77] Creating layer fc4_prescale
I0508 00:31:42.877581 12373 net.cpp:100] Creating Layer fc4_prescale
I0508 00:31:42.877588 12373 net.cpp:434] fc4_prescale <- fc4_300
I0508 00:31:42.877593 12373 net.cpp:395] fc4_prescale -> fc4_300 (in-place)
I0508 00:31:42.877691 12373 net.cpp:150] Setting up fc4_prescale
I0508 00:31:42.877719 12373 net.cpp:157] Top shape: 1024 300 (307200)
I0508 00:31:42.877724 12373 net.cpp:165] Memory required for data: 4104257536
I0508 00:31:42.877730 12373 layer_factory.hpp:77] Creating layer fc4_sTanH
I0508 00:31:42.877738 12373 net.cpp:100] Creating Layer fc4_sTanH
I0508 00:31:42.877743 12373 net.cpp:434] fc4_sTanH <- fc4_300
I0508 00:31:42.877748 12373 net.cpp:395] fc4_sTanH -> fc4_300 (in-place)
I0508 00:31:42.877961 12373 net.cpp:150] Setting up fc4_sTanH
I0508 00:31:42.877974 12373 net.cpp:157] Top shape: 1024 300 (307200)
I0508 00:31:42.877977 12373 net.cpp:165] Memory required for data: 4105486336
I0508 00:31:42.877981 12373 layer_factory.hpp:77] Creating layer fc4_postscale
I0508 00:31:42.877987 12373 net.cpp:100] Creating Layer fc4_postscale
I0508 00:31:42.877991 12373 net.cpp:434] fc4_postscale <- fc4_300
I0508 00:31:42.877997 12373 net.cpp:395] fc4_postscale -> fc4_300 (in-place)
I0508 00:31:42.878113 12373 net.cpp:150] Setting up fc4_postscale
I0508 00:31:42.878121 12373 net.cpp:157] Top shape: 1024 300 (307200)
I0508 00:31:42.878126 12373 net.cpp:165] Memory required for data: 4106715136
I0508 00:31:42.878131 12373 layer_factory.hpp:77] Creating layer fc5_116
I0508 00:31:42.878140 12373 net.cpp:100] Creating Layer fc5_116
I0508 00:31:42.878144 12373 net.cpp:434] fc5_116 <- fc4_300
I0508 00:31:42.878149 12373 net.cpp:408] fc5_116 -> fc5_classes
I0508 00:31:42.878495 12373 net.cpp:150] Setting up fc5_116
I0508 00:31:42.878505 12373 net.cpp:157] Top shape: 1024 116 (118784)
I0508 00:31:42.878510 12373 net.cpp:165] Memory required for data: 4107190272
I0508 00:31:42.878521 12373 layer_factory.hpp:77] Creating layer fc5_classes_fc5_116_0_split
I0508 00:31:42.878530 12373 net.cpp:100] Creating Layer fc5_classes_fc5_116_0_split
I0508 00:31:42.878535 12373 net.cpp:434] fc5_classes_fc5_116_0_split <- fc5_classes
I0508 00:31:42.878540 12373 net.cpp:408] fc5_classes_fc5_116_0_split -> fc5_classes_fc5_116_0_split_0
I0508 00:31:42.878547 12373 net.cpp:408] fc5_classes_fc5_116_0_split -> fc5_classes_fc5_116_0_split_1
I0508 00:31:42.878556 12373 net.cpp:408] fc5_classes_fc5_116_0_split -> fc5_classes_fc5_116_0_split_2
I0508 00:31:42.878607 12373 net.cpp:150] Setting up fc5_classes_fc5_116_0_split
I0508 00:31:42.878614 12373 net.cpp:157] Top shape: 1024 116 (118784)
I0508 00:31:42.878619 12373 net.cpp:157] Top shape: 1024 116 (118784)
I0508 00:31:42.878624 12373 net.cpp:157] Top shape: 1024 116 (118784)
I0508 00:31:42.878628 12373 net.cpp:165] Memory required for data: 4108615680
I0508 00:31:42.878630 12373 layer_factory.hpp:77] Creating layer softmax
I0508 00:31:42.878638 12373 net.cpp:100] Creating Layer softmax
I0508 00:31:42.878641 12373 net.cpp:434] softmax <- fc5_classes_fc5_116_0_split_0
I0508 00:31:42.878646 12373 net.cpp:408] softmax -> softmax
I0508 00:31:42.878888 12373 net.cpp:150] Setting up softmax
I0508 00:31:42.878901 12373 net.cpp:157] Top shape: 1024 116 (118784)
I0508 00:31:42.878906 12373 net.cpp:165] Memory required for data: 4109090816
I0508 00:31:42.878909 12373 layer_factory.hpp:77] Creating layer loss
I0508 00:31:42.878916 12373 net.cpp:100] Creating Layer loss
I0508 00:31:42.878921 12373 net.cpp:434] loss <- softmax
I0508 00:31:42.878926 12373 net.cpp:434] loss <- label_data_1_split_0
I0508 00:31:42.878931 12373 net.cpp:408] loss -> loss
I0508 00:31:42.878958 12373 net.cpp:150] Setting up loss
I0508 00:31:42.878965 12373 net.cpp:157] Top shape: (1)
I0508 00:31:42.878968 12373 net.cpp:160]     with loss weight 1
I0508 00:31:42.878980 12373 net.cpp:165] Memory required for data: 4109090820
I0508 00:31:42.878984 12373 layer_factory.hpp:77] Creating layer accuracy_1
I0508 00:31:42.878990 12373 net.cpp:100] Creating Layer accuracy_1
I0508 00:31:42.878995 12373 net.cpp:434] accuracy_1 <- fc5_classes_fc5_116_0_split_1
I0508 00:31:42.878999 12373 net.cpp:434] accuracy_1 <- label_data_1_split_1
I0508 00:31:42.879005 12373 net.cpp:408] accuracy_1 -> accuracy_1
I0508 00:31:42.879014 12373 net.cpp:150] Setting up accuracy_1
I0508 00:31:42.879019 12373 net.cpp:157] Top shape: (1)
I0508 00:31:42.879036 12373 net.cpp:165] Memory required for data: 4109090824
I0508 00:31:42.879040 12373 layer_factory.hpp:77] Creating layer accuracy_5
I0508 00:31:42.879046 12373 net.cpp:100] Creating Layer accuracy_5
I0508 00:31:42.879050 12373 net.cpp:434] accuracy_5 <- fc5_classes_fc5_116_0_split_2
I0508 00:31:42.879055 12373 net.cpp:434] accuracy_5 <- label_data_1_split_2
I0508 00:31:42.879060 12373 net.cpp:408] accuracy_5 -> accuracy_5
I0508 00:31:42.879068 12373 net.cpp:150] Setting up accuracy_5
I0508 00:31:42.879075 12373 net.cpp:157] Top shape: (1)
I0508 00:31:42.879076 12373 net.cpp:165] Memory required for data: 4109090828
I0508 00:31:42.879081 12373 net.cpp:228] accuracy_5 does not need backward computation.
I0508 00:31:42.879084 12373 net.cpp:228] accuracy_1 does not need backward computation.
I0508 00:31:42.879088 12373 net.cpp:226] loss needs backward computation.
I0508 00:31:42.879092 12373 net.cpp:226] softmax needs backward computation.
I0508 00:31:42.879097 12373 net.cpp:226] fc5_classes_fc5_116_0_split needs backward computation.
I0508 00:31:42.879101 12373 net.cpp:226] fc5_116 needs backward computation.
I0508 00:31:42.879103 12373 net.cpp:226] fc4_postscale needs backward computation.
I0508 00:31:42.879107 12373 net.cpp:226] fc4_sTanH needs backward computation.
I0508 00:31:42.879109 12373 net.cpp:226] fc4_prescale needs backward computation.
I0508 00:31:42.879112 12373 net.cpp:226] fc4_300 needs backward computation.
I0508 00:31:42.879117 12373 net.cpp:226] pool3 needs backward computation.
I0508 00:31:42.879119 12373 net.cpp:226] conv3_postscale needs backward computation.
I0508 00:31:42.879122 12373 net.cpp:226] conv3_sTanH needs backward computation.
I0508 00:31:42.879125 12373 net.cpp:226] conv3_prescale needs backward computation.
I0508 00:31:42.879129 12373 net.cpp:226] conv3 needs backward computation.
I0508 00:31:42.879133 12373 net.cpp:226] pool2 needs backward computation.
I0508 00:31:42.879137 12373 net.cpp:226] conv2_postscale needs backward computation.
I0508 00:31:42.879139 12373 net.cpp:226] conv2_sTanH needs backward computation.
I0508 00:31:42.879143 12373 net.cpp:226] conv2_prescale needs backward computation.
I0508 00:31:42.879146 12373 net.cpp:226] conv2 needs backward computation.
I0508 00:31:42.879149 12373 net.cpp:226] pool1 needs backward computation.
I0508 00:31:42.879153 12373 net.cpp:226] conv1_postscale needs backward computation.
I0508 00:31:42.879156 12373 net.cpp:226] conv1_sTanH needs backward computation.
I0508 00:31:42.879160 12373 net.cpp:226] conv1_prescale needs backward computation.
I0508 00:31:42.879163 12373 net.cpp:226] conv1 needs backward computation.
I0508 00:31:42.879168 12373 net.cpp:228] label_data_1_split does not need backward computation.
I0508 00:31:42.879171 12373 net.cpp:228] data does not need backward computation.
I0508 00:31:42.879175 12373 net.cpp:270] This network produces output accuracy_1
I0508 00:31:42.879179 12373 net.cpp:270] This network produces output accuracy_5
I0508 00:31:42.879182 12373 net.cpp:270] This network produces output loss
I0508 00:31:42.879201 12373 net.cpp:283] Network initialization done.
I0508 00:31:42.879287 12373 solver.cpp:72] Solver scaffolding done.
I0508 00:31:42.880164 12373 caffe.cpp:251] Starting Optimization
I0508 00:31:42.880177 12373 solver.cpp:291] Solving 
I0508 00:31:42.880180 12373 solver.cpp:292] Learning Rate Policy: step
I0508 00:31:42.883894 12373 solver.cpp:349] Iteration 0, Testing net (#0)
I0508 00:31:42.885607 12373 net.cpp:693] Ignoring source layer silence
I0508 00:31:42.885633 12373 blocking_queue.cpp:50] Data layer prefetch queue empty
I0508 00:31:46.317806 12373 solver.cpp:416]     Test net output #0: accuracy_1 = 0.013557
I0508 00:31:46.317842 12373 solver.cpp:416]     Test net output #1: accuracy_5 = 0.065085
I0508 00:31:46.317852 12373 solver.cpp:416]     Test net output #2: loss = 4.72682 (* 1 = 4.72682 loss)
I0508 00:31:46.493223 12373 solver.cpp:240] Iteration 0, loss = 4.76184
I0508 00:31:46.493261 12373 solver.cpp:256]     Train net output #0: loss = 4.76184 (* 1 = 4.76184 loss)
I0508 00:31:46.493316 12373 sgd_solver.cpp:106] Iteration 0, lr = 1e-05
I0508 00:31:46.901609 12373 solver.cpp:240] Iteration 1, loss = 4.73301
I0508 00:31:46.901657 12373 solver.cpp:256]     Train net output #0: loss = 4.73301 (* 1 = 4.73301 loss)
I0508 00:31:46.901671 12373 sgd_solver.cpp:106] Iteration 1, lr = 1e-05
I0508 00:31:47.310370 12373 solver.cpp:240] Iteration 2, loss = 4.68433
I0508 00:31:47.310405 12373 solver.cpp:256]     Train net output #0: loss = 4.68433 (* 1 = 4.68433 loss)
I0508 00:31:47.310415 12373 sgd_solver.cpp:106] Iteration 2, lr = 1e-05
I0508 00:31:47.719400 12373 solver.cpp:240] Iteration 3, loss = 4.64814
I0508 00:31:47.719436 12373 solver.cpp:256]     Train net output #0: loss = 4.64814 (* 1 = 4.64814 loss)
I0508 00:31:47.719444 12373 sgd_solver.cpp:106] Iteration 3, lr = 1e-05
I0508 00:31:48.128690 12373 solver.cpp:240] Iteration 4, loss = 4.60631
I0508 00:31:48.128727 12373 solver.cpp:256]     Train net output #0: loss = 4.60631 (* 1 = 4.60631 loss)
I0508 00:31:48.128736 12373 sgd_solver.cpp:106] Iteration 4, lr = 1e-05
I0508 00:31:48.537753 12373 solver.cpp:240] Iteration 5, loss = 4.55289
I0508 00:31:48.537791 12373 solver.cpp:256]     Train net output #0: loss = 4.55289 (* 1 = 4.55289 loss)
I0508 00:31:48.537799 12373 sgd_solver.cpp:106] Iteration 5, lr = 1e-05
I0508 00:31:48.947006 12373 solver.cpp:240] Iteration 6, loss = 4.5308
I0508 00:31:48.947042 12373 solver.cpp:256]     Train net output #0: loss = 4.5308 (* 1 = 4.5308 loss)
I0508 00:31:48.947051 12373 sgd_solver.cpp:106] Iteration 6, lr = 1e-05
I0508 00:31:49.356408 12373 solver.cpp:240] Iteration 7, loss = 4.47039
I0508 00:31:49.356446 12373 solver.cpp:256]     Train net output #0: loss = 4.47039 (* 1 = 4.47039 loss)
I0508 00:31:49.356454 12373 sgd_solver.cpp:106] Iteration 7, lr = 1e-05
I0508 00:31:49.765748 12373 solver.cpp:240] Iteration 8, loss = 4.4669
I0508 00:31:49.765786 12373 solver.cpp:256]     Train net output #0: loss = 4.4669 (* 1 = 4.4669 loss)
I0508 00:31:49.765794 12373 sgd_solver.cpp:106] Iteration 8, lr = 1e-05
I0508 00:31:50.175189 12373 solver.cpp:240] Iteration 9, loss = 4.3991
I0508 00:31:50.175230 12373 solver.cpp:256]     Train net output #0: loss = 4.3991 (* 1 = 4.3991 loss)
I0508 00:31:50.175238 12373 sgd_solver.cpp:106] Iteration 9, lr = 1e-05
I0508 00:31:50.584270 12373 solver.cpp:240] Iteration 10, loss = 4.3591
I0508 00:31:50.584306 12373 solver.cpp:256]     Train net output #0: loss = 4.3591 (* 1 = 4.3591 loss)
I0508 00:31:50.584314 12373 sgd_solver.cpp:106] Iteration 10, lr = 1e-05
I0508 00:31:50.992414 12373 solver.cpp:240] Iteration 11, loss = 4.34773
I0508 00:31:50.992451 12373 solver.cpp:256]     Train net output #0: loss = 4.34773 (* 1 = 4.34773 loss)
I0508 00:31:50.992460 12373 sgd_solver.cpp:106] Iteration 11, lr = 1e-05
I0508 00:31:51.401612 12373 solver.cpp:240] Iteration 12, loss = 4.29695
I0508 00:31:51.401649 12373 solver.cpp:256]     Train net output #0: loss = 4.29695 (* 1 = 4.29695 loss)
I0508 00:31:51.401657 12373 sgd_solver.cpp:106] Iteration 12, lr = 1e-05
I0508 00:31:51.810636 12373 solver.cpp:240] Iteration 13, loss = 4.29395
I0508 00:31:51.810679 12373 solver.cpp:256]     Train net output #0: loss = 4.29395 (* 1 = 4.29395 loss)
I0508 00:31:51.810688 12373 sgd_solver.cpp:106] Iteration 13, lr = 1e-05
I0508 00:31:52.219736 12373 solver.cpp:240] Iteration 14, loss = 4.19568
I0508 00:31:52.219772 12373 solver.cpp:256]     Train net output #0: loss = 4.19568 (* 1 = 4.19568 loss)
I0508 00:31:52.219781 12373 sgd_solver.cpp:106] Iteration 14, lr = 1e-05
I0508 00:31:52.628537 12373 solver.cpp:240] Iteration 15, loss = 4.20038
I0508 00:31:52.628576 12373 solver.cpp:256]     Train net output #0: loss = 4.20038 (* 1 = 4.20038 loss)
I0508 00:31:52.628584 12373 sgd_solver.cpp:106] Iteration 15, lr = 1e-05
I0508 00:31:53.037993 12373 solver.cpp:240] Iteration 16, loss = 4.13674
I0508 00:31:53.038028 12373 solver.cpp:256]     Train net output #0: loss = 4.13674 (* 1 = 4.13674 loss)
I0508 00:31:53.038038 12373 sgd_solver.cpp:106] Iteration 16, lr = 1e-05
I0508 00:31:53.447183 12373 solver.cpp:240] Iteration 17, loss = 4.11823
I0508 00:31:53.447222 12373 solver.cpp:256]     Train net output #0: loss = 4.11823 (* 1 = 4.11823 loss)
I0508 00:31:53.447230 12373 sgd_solver.cpp:106] Iteration 17, lr = 1e-05
I0508 00:31:53.856627 12373 solver.cpp:240] Iteration 18, loss = 4.10128
I0508 00:31:53.856665 12373 solver.cpp:256]     Train net output #0: loss = 4.10128 (* 1 = 4.10128 loss)
I0508 00:31:53.856673 12373 sgd_solver.cpp:106] Iteration 18, lr = 1e-05
I0508 00:31:54.266330 12373 solver.cpp:240] Iteration 19, loss = 3.9963
I0508 00:31:54.266371 12373 solver.cpp:256]     Train net output #0: loss = 3.9963 (* 1 = 3.9963 loss)
I0508 00:31:54.266378 12373 sgd_solver.cpp:106] Iteration 19, lr = 1e-05
I0508 00:31:54.678300 12373 solver.cpp:240] Iteration 20, loss = 4.01927
I0508 00:31:54.678354 12373 solver.cpp:256]     Train net output #0: loss = 4.01927 (* 1 = 4.01927 loss)
I0508 00:31:54.678366 12373 sgd_solver.cpp:106] Iteration 20, lr = 1e-05
I0508 00:31:55.087121 12373 solver.cpp:240] Iteration 21, loss = 3.92568
I0508 00:31:55.087160 12373 solver.cpp:256]     Train net output #0: loss = 3.92568 (* 1 = 3.92568 loss)
I0508 00:31:55.087169 12373 sgd_solver.cpp:106] Iteration 21, lr = 1e-05
I0508 00:31:55.496526 12373 solver.cpp:240] Iteration 22, loss = 3.9204
I0508 00:31:55.496563 12373 solver.cpp:256]     Train net output #0: loss = 3.9204 (* 1 = 3.9204 loss)
I0508 00:31:55.496572 12373 sgd_solver.cpp:106] Iteration 22, lr = 1e-05
I0508 00:31:55.906090 12373 solver.cpp:240] Iteration 23, loss = 3.95475
I0508 00:31:55.906124 12373 solver.cpp:256]     Train net output #0: loss = 3.95475 (* 1 = 3.95475 loss)
I0508 00:31:55.906132 12373 sgd_solver.cpp:106] Iteration 23, lr = 1e-05
I0508 00:31:56.315403 12373 solver.cpp:240] Iteration 24, loss = 3.90639
I0508 00:31:56.315440 12373 solver.cpp:256]     Train net output #0: loss = 3.90639 (* 1 = 3.90639 loss)
I0508 00:31:56.315448 12373 sgd_solver.cpp:106] Iteration 24, lr = 1e-05
I0508 00:31:56.724740 12373 solver.cpp:240] Iteration 25, loss = 3.8761
I0508 00:31:56.724777 12373 solver.cpp:256]     Train net output #0: loss = 3.8761 (* 1 = 3.8761 loss)
I0508 00:31:56.724786 12373 sgd_solver.cpp:106] Iteration 25, lr = 1e-05
I0508 00:31:57.133671 12373 solver.cpp:240] Iteration 26, loss = 3.80353
I0508 00:31:57.133710 12373 solver.cpp:256]     Train net output #0: loss = 3.80353 (* 1 = 3.80353 loss)
I0508 00:31:57.133718 12373 sgd_solver.cpp:106] Iteration 26, lr = 1e-05
I0508 00:31:57.543329 12373 solver.cpp:240] Iteration 27, loss = 3.76428
I0508 00:31:57.543366 12373 solver.cpp:256]     Train net output #0: loss = 3.76428 (* 1 = 3.76428 loss)
I0508 00:31:57.543375 12373 sgd_solver.cpp:106] Iteration 27, lr = 1e-05
I0508 00:31:57.952468 12373 solver.cpp:240] Iteration 28, loss = 3.74302
I0508 00:31:57.952507 12373 solver.cpp:256]     Train net output #0: loss = 3.74302 (* 1 = 3.74302 loss)
I0508 00:31:57.952515 12373 sgd_solver.cpp:106] Iteration 28, lr = 1e-05
I0508 00:31:58.361640 12373 solver.cpp:240] Iteration 29, loss = 3.61121
I0508 00:31:58.361676 12373 solver.cpp:256]     Train net output #0: loss = 3.61121 (* 1 = 3.61121 loss)
I0508 00:31:58.361685 12373 sgd_solver.cpp:106] Iteration 29, lr = 1e-05
I0508 00:31:58.771152 12373 solver.cpp:240] Iteration 30, loss = 3.6436
I0508 00:31:58.771193 12373 solver.cpp:256]     Train net output #0: loss = 3.6436 (* 1 = 3.6436 loss)
I0508 00:31:58.771200 12373 sgd_solver.cpp:106] Iteration 30, lr = 1e-05
I0508 00:31:59.180420 12373 solver.cpp:240] Iteration 31, loss = 3.6473
I0508 00:31:59.180459 12373 solver.cpp:256]     Train net output #0: loss = 3.6473 (* 1 = 3.6473 loss)
I0508 00:31:59.180465 12373 sgd_solver.cpp:106] Iteration 31, lr = 1e-05
I0508 00:31:59.590227 12373 solver.cpp:240] Iteration 32, loss = 3.72215
I0508 00:31:59.590265 12373 solver.cpp:256]     Train net output #0: loss = 3.72215 (* 1 = 3.72215 loss)
I0508 00:31:59.590273 12373 sgd_solver.cpp:106] Iteration 32, lr = 1e-05
I0508 00:31:59.999445 12373 solver.cpp:240] Iteration 33, loss = 3.54258
I0508 00:31:59.999485 12373 solver.cpp:256]     Train net output #0: loss = 3.54258 (* 1 = 3.54258 loss)
I0508 00:31:59.999521 12373 sgd_solver.cpp:106] Iteration 33, lr = 1e-05
I0508 00:32:00.408946 12373 solver.cpp:240] Iteration 34, loss = 3.6415
I0508 00:32:00.408982 12373 solver.cpp:256]     Train net output #0: loss = 3.6415 (* 1 = 3.6415 loss)
I0508 00:32:00.408991 12373 sgd_solver.cpp:106] Iteration 34, lr = 1e-05
I0508 00:32:00.817540 12373 solver.cpp:240] Iteration 35, loss = 3.57169
I0508 00:32:00.817579 12373 solver.cpp:256]     Train net output #0: loss = 3.57169 (* 1 = 3.57169 loss)
I0508 00:32:00.817587 12373 sgd_solver.cpp:106] Iteration 35, lr = 1e-05
I0508 00:32:01.228356 12373 solver.cpp:240] Iteration 36, loss = 3.62425
I0508 00:32:01.228394 12373 solver.cpp:256]     Train net output #0: loss = 3.62425 (* 1 = 3.62425 loss)
I0508 00:32:01.228402 12373 sgd_solver.cpp:106] Iteration 36, lr = 1e-05
I0508 00:32:01.637744 12373 solver.cpp:240] Iteration 37, loss = 3.47315
I0508 00:32:01.637783 12373 solver.cpp:256]     Train net output #0: loss = 3.47315 (* 1 = 3.47315 loss)
I0508 00:32:01.637790 12373 sgd_solver.cpp:106] Iteration 37, lr = 1e-05
